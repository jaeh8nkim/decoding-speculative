{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e086839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def analyze_db(db_file: str, table_name: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Connects to the given SQLite database,\n",
    "    analyzes the 'trace' column for empty values,\n",
    "    and prints summary stats plus non-empty row indices.\n",
    "    \"\"\"\n",
    "    # Load table\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT trace FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Metrics\n",
    "    total_rows       = len(df)\n",
    "    null_rows        = df['trace'].isna().sum()\n",
    "    empty_str_rows   = (df['trace'] == '').sum()\n",
    "    whitespace_rows  = df['trace'].str.strip().eq('').sum()\n",
    "    empty_rows       = null_rows + whitespace_rows  # counts '' and all-whitespace\n",
    "    non_empty_mask   = df['trace'].notna() & (df['trace'].str.strip() != '')\n",
    "    non_empty_rows   = non_empty_mask.sum()\n",
    "    non_empty_indices = df[non_empty_mask].index.tolist()\n",
    "\n",
    "    # Output\n",
    "    print(f\"Total rows:        {total_rows}\")\n",
    "    print(f\"Null rows:         {null_rows}\")\n",
    "    print(f\"Empty rows:        {empty_rows}\")\n",
    "    print(f\"Non-empty rows:    {non_empty_rows}\\n\")\n",
    "    print(\"Non-empty row indices:\")\n",
    "    print(non_empty_indices)\n",
    "\n",
    "def merge_db(\n",
    "    db1_file: str,\n",
    "    db2_file: str,\n",
    "    table_name: str = 'dataset',\n",
    "    out_db_file: str = 'merged.db'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Merge two SQLite DBs by keeping 'question' and 'answer' from DB1\n",
    "    and merging 'trace' by choosing the shorter-token one on conflicts.\n",
    "    \"\"\"\n",
    "    # tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"Qwen/Qwen3-0.6B\", trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # load full df from DB1\n",
    "    conn1 = sqlite3.connect(db1_file)\n",
    "    df1 = pd.read_sql(f\"SELECT * FROM {table_name}\", conn1)\n",
    "    conn1.close()\n",
    "    \n",
    "    # load only trace from DB2\n",
    "    conn2 = sqlite3.connect(db2_file)\n",
    "    df2 = pd.read_sql(f\"SELECT trace FROM {table_name}\", conn2)\n",
    "    conn2.close()\n",
    "    \n",
    "    assert len(df1) == len(df2), \"Row counts differ\"\n",
    "    \n",
    "    t1 = df1['trace'].fillna('').astype(str)\n",
    "    t2 = df2['trace'].fillna('').astype(str)\n",
    "    \n",
    "    merged_traces = []\n",
    "    conflicts = []\n",
    "    \n",
    "    for i, (a, b) in enumerate(zip(t1, t2)):\n",
    "        empty1 = not a.strip()\n",
    "        empty2 = not b.strip()\n",
    "        if empty1 and not empty2:\n",
    "            choice = b\n",
    "        elif empty2 and not empty1:\n",
    "            choice = a\n",
    "        elif a == b:\n",
    "            choice = a\n",
    "        else:\n",
    "            # conflict: pick shorter in tokens\n",
    "            l1 = len(tokenizer(a).input_ids)\n",
    "            l2 = len(tokenizer(b).input_ids)\n",
    "            choice = a if l1 <= l2 else b\n",
    "            conflicts.append(i)\n",
    "        merged_traces.append(choice)\n",
    "    \n",
    "    # assign merged traces back to df1\n",
    "    df1['trace'] = merged_traces\n",
    "    \n",
    "    # write merged DB\n",
    "    conn_out = sqlite3.connect(out_db_file)\n",
    "    df1.to_sql(table_name, conn_out, index=False, if_exists='replace')\n",
    "    conn_out.close()\n",
    "    \n",
    "    # report\n",
    "    print(f\"Total rows:      {len(df1)}\")\n",
    "    print(f\"Conflicts found: {len(conflicts)}\")\n",
    "    print(\"Conflict indices:\", conflicts)\n",
    "\n",
    "def show_trace(db_file: str, idx: int, table_name: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Load the 'trace' column from the given SQLite database,\n",
    "    print the trace at row `idx` with separators, and its token count\n",
    "    using the Qwen/Qwen3-0.6B tokenizer.\n",
    "    \"\"\"\n",
    "    # load\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT trace FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    # bounds check\n",
    "    if idx < 0 or idx >= len(df):\n",
    "        raise IndexError(f\"Index {idx} out of range (0–{len(df)-1})\")\n",
    "\n",
    "    # get trace\n",
    "    trace = df.at[idx, 'trace'] or \"\"\n",
    "\n",
    "    sep = \"-\" * 80\n",
    "\n",
    "    # output with separators\n",
    "    print(f\"Trace[{idx}]:\")\n",
    "    print(sep)\n",
    "    print(trace)\n",
    "    print(sep)\n",
    "\n",
    "    # tokenize & count\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"Qwen/Qwen3-0.6B\", trust_remote_code=True\n",
    "    )\n",
    "    tokens = tokenizer(trace).input_ids\n",
    "    print(f\"Token count: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10b2a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE_1      = 'dataset_4qwen3_250630a_1.db'\n",
    "DB_FILE_2      = 'dataset_4qwen3_250630a_2.db'\n",
    "DB_FILE_MERGED = 'dataset_4qwen3_250630a_merged.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f12fab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:        1817\n",
      "Null rows:         0\n",
      "Empty rows:        1733\n",
      "Non-empty rows:    84\n",
      "\n",
      "Non-empty row indices:\n",
      "[0, 2, 5, 35, 41, 50, 54, 59, 60, 61, 67, 71, 72, 73, 77, 78, 81, 84, 91, 98, 100, 102, 104, 111, 113, 118, 120, 131, 137, 140, 145, 159, 165, 171, 181, 185, 188, 194, 197, 202, 204, 211, 212, 213, 233, 237, 241, 248, 258, 273, 279, 280, 303, 305, 321, 325, 331, 337, 363, 365, 369, 372, 379, 385, 390, 395, 402, 408, 411, 418, 428, 429, 430, 431, 432, 443, 448, 463, 464, 474, 484, 496, 498, 502]\n"
     ]
    }
   ],
   "source": [
    "analyze_db(DB_FILE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2f6b092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:        1817\n",
      "Null rows:         0\n",
      "Empty rows:        1744\n",
      "Non-empty rows:    73\n",
      "\n",
      "Non-empty row indices:\n",
      "[506, 509, 514, 528, 531, 534, 536, 537, 553, 560, 562, 572, 578, 582, 585, 586, 588, 594, 598, 602, 607, 612, 615, 618, 621, 627, 639, 646, 650, 653, 663, 669, 673, 678, 679, 684, 686, 689, 696, 718, 722, 723, 734, 735, 748, 750, 752, 758, 770, 776, 785, 789, 790, 794, 806, 807, 811, 818, 821, 824, 825, 828, 830, 833, 841, 867, 871, 875, 877, 891, 926, 932, 951]\n"
     ]
    }
   ],
   "source": [
    "analyze_db(DB_FILE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0301f826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:      1817\n",
      "Conflicts found: 0\n",
      "Conflict indices: []\n"
     ]
    }
   ],
   "source": [
    "merge_db(\n",
    "    DB_FILE_1,\n",
    "    DB_FILE_2,\n",
    "    out_db_file=DB_FILE_MERGED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d915134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:        1817\n",
      "Null rows:         0\n",
      "Empty rows:        1660\n",
      "Non-empty rows:    157\n",
      "\n",
      "Non-empty row indices:\n",
      "[0, 2, 5, 35, 41, 50, 54, 59, 60, 61, 67, 71, 72, 73, 77, 78, 81, 84, 91, 98, 100, 102, 104, 111, 113, 118, 120, 131, 137, 140, 145, 159, 165, 171, 181, 185, 188, 194, 197, 202, 204, 211, 212, 213, 233, 237, 241, 248, 258, 273, 279, 280, 303, 305, 321, 325, 331, 337, 363, 365, 369, 372, 379, 385, 390, 395, 402, 408, 411, 418, 428, 429, 430, 431, 432, 443, 448, 463, 464, 474, 484, 496, 498, 502, 506, 509, 514, 528, 531, 534, 536, 537, 553, 560, 562, 572, 578, 582, 585, 586, 588, 594, 598, 602, 607, 612, 615, 618, 621, 627, 639, 646, 650, 653, 663, 669, 673, 678, 679, 684, 686, 689, 696, 718, 722, 723, 734, 735, 748, 750, 752, 758, 770, 776, 785, 789, 790, 794, 806, 807, 811, 818, 821, 824, 825, 828, 830, 833, 841, 867, 871, 875, 877, 891, 926, 932, 951]\n"
     ]
    }
   ],
   "source": [
    "analyze_db(DB_FILE_MERGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21f064cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace[448]:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Okay, let's tackle this crossword puzzle clue: \"Adjust section of Gatt unethically (6)\". Hmm, first, I need to break down the clue. The answer is a 6-letter word. The clue mentions \"adjust\" which might be a verb meaning to modify or change something. Then there's \"section of Gatt\" – maybe \"section\" here refers to a part of a word, like a substring. \"Unethically\" is an adjective, but it might be part of the wordplay. Wait, the word \"unethically\" could be a part of the word \"unethical\", but that's a 9-letter word. Wait, perhaps \"unethical\" isn't the right path. Maybe it's part of a wordplay where \"unethically\" modifies another part. Let me think again. \n",
      "\n",
      "The structure might be something like a word meaning \"adjust\" combined with a section of the word \"GATT\", but maybe not directly. Wait, \"GATT\" is an acronym, maybe the letters are used as parts of the word. Let me think of synonyms for \"adjust\". Words like \"modify\", \"alter\", \"revise\", \"optimize\". The answer could be a combination of letters. Let's see, \"section of GATT\" – if \"section\" means a part, maybe \"att\" is a part of \"GATT\", and then combine it with another part. But how does \"unethically\" fit in? Maybe \"unethically\" is part of the word, like \"unethical\" but with some letters missing. Wait, maybe \"uneth\" as a part. Let me check: the answer is 6 letters. Let's try to think of a 6-letter word that means \"adjust\" and uses some part of \"GATT\" and \"unethically\".\n",
      "\n",
      "Alternatively, maybe \"unethically\" is an adverb modifying the verb \"adjust\". So the word would be \"unethically\" + some letters. But that seems unlikely since the answer is 6 letters. Maybe \"adjust\" is the main word, and \"section of Gatt\" is a part, and \"unethically\" modifies it. For example, \"unethically\" could mean to change something in a unethical way, but that doesn't seem to fit. Wait, maybe \"adjust\" is a synonym, like \"alter\", and \"section of Gatt\" is a part. Let me think of \"alter\" and see. \"Alter\" is 5 letters. Then maybe \"alter\" plus something. Wait, but we need 6 letters. Alternatively, \"attune\" – that's 6 letters. \"Attune\" means to adjust or bring into harmony. But how does \"section of Gatt\" relate to \"attune\"? If \"attune\" is part of it, maybe \"att\" from Gatt and \"une\" from \"unethically\"? Let's see: \"att\" plus \"une\" is \"attune\", but where does \"section\" come in? Maybe \"attune\" is the answer, and the wordplay is that \"section of GATT\" is \"att\" (the first three letters), and \"unethically\" is \"une\" (the first two letters of the word \"unethical\"). That could work. Let's check: \"attune\" is 6 letters, and it means adjust. So \"adjust section of Gatt unethically\" could be \"attune\". But let me confirm. \"Section of GATT\" – Gatt is an acronym, maybe \"section\" is a substring. So \"GATT\" would be \"G\", \"A\", \"T\", \"T\". If \"section\" is a part of that, maybe \"ATT\" (the 3rd and 4th letters). Then \"unethically\" might mean to take the first letters? Hmm, but \"unethically\" doesn't directly fit into a 6-letter word here. Wait, perhaps the answer is \"uneth\" + \"tically\"? But that doesn't form a word. Maybe \"attune\" with \"uneth\" and \"ically\" but that doesn't fit. \n",
      "\n",
      "Alternatively, \"unethically\" might be part of the answer as a word. Let me check: \"unethically\" has 9 letters, which is too long. But if we take the first letters of \"unethically\", that would be \"u\", but we need 6 letters. Alternatively, maybe \"unethically\" modifies \"att\", but that's a stretch. \n",
      "\n",
      "Wait, another angle. The word \"unethically\" has the word \"unethical\" which is a 9-letter word. But maybe the clue is using part of that. Maybe \"unethical\" has a 5-letter part, but I need a 6-letter answer. Alternatively, \"unethically\" could mean \"in an unethical manner\", but how would that help with the wordplay? \n",
      "\n",
      "Alternatively, the word \"unethically\" might have some hidden letters. Let me check. The word \"unethically\" has letters U-N-E-T-H-I-C-A-L-L-Y. If I take some letters from here, maybe. Wait, but I need 6 letters. Let me think again. The answer is a 6-letter word meaning \"adjust\", and the clue mentions \"section of Gatt\" and \"unethically\". \n",
      "\n",
      "Maybe \"section of Gatt\" refers to a part of the word \"GATT\" and \"unethically\" refers to the letters in \"unethical\". Let me think of the word \"uneth\" plus \"ically\", but that's \"unethically\" again. Wait, \"uneth\" is \"uneth\" and then add something. Hmm. Maybe \"uneth\" + \"ic\" + \"ally\"? But that's not 6 letters either. \n",
      "\n",
      "Alternatively, \"unethically\" could mean \"unethic\" + \"ally\". But \"unethic\" is part of \"unethical\". Wait, \"unethic\" is 7 letters. No. Alternatively, maybe \"unethical\" is split into \"uneth\" and \"ical\", and then \"ically\". But that seems convoluted. \n",
      "\n",
      "Wait, let's try another approach. The word \"adjust\" could mean to alter or revise. The answer is 6 letters. Let's think of 6-letter words related to adjusting. \"Revise\" is 6 letters, but that might be too direct. \"Revise\" is to adjust or correct. But \"section of GATT\" – GATT is an acronym, maybe \"revis\" as in \"revis\" (but GATT is G, A, T, T). \"Revis\" is part of that. But \"unethically\" is an adverb. Maybe the answer is \"uneth\" + \"ically\"? But \"unethically\" is the adverb, so maybe \"uneth\" (uneth) plus \"ically\"? But that's \"unethically\", again. \n",
      "\n",
      "Wait, maybe \"adjust\" is the main word, and \"section of Gatt\" is a part. Let's take the letters from Gatt. \"GATT\" has letters G, A, T, T. If \"section\" refers to a part, maybe \"att\" is the section. Then \"unethically\" might have something to do with the letters. Let me see: if the answer is \"attune\", then \"unethically\" isn't directly part of it. But \"attune\" does mean to adjust, and \"section of GATT\" is \"att\", and perhaps the \"unethically\" is just a red herring? Or maybe \"uneth\" is part of it. Wait, \"uneth\" is part of \"unethically\", but how? Maybe \"uneth\" (from \"unethically\") plus \"tically\" (from \"unethically\") but that's not 6 letters. \n",
      "\n",
      "Alternatively, \"unethically\" is a synonym for \"unethically\", and the word is \"unethically\" but with letters rearranged. But that's probably not. \n",
      "\n",
      "Wait, maybe the word is \"attune\" as I thought before, and \"unethically\" refers to the fact that it's an unethical way of adjusting? But that seems a bit forced. Alternatively, \"unethically\" could mean that the adjustment is done in an unethical manner, but how does that tie into the wordplay? \n",
      "\n",
      "Alternatively, \"adjust\" is part of the word, and \"unethically\" is modifying it, but I don't see it. \n",
      "\n",
      "Wait, let me think again. The answer is 6 letters. Let's brainstorm 6-letter words related to adjusting: revise, alter, adjust, optimize, modify, rectify. \"Revise\" is 6 letters. But how does \"section of GATT\" fit in? \"Revise\" can be split as \"re\" + \"vise\". Maybe \"re\" is a part, but not directly. Alternatively, \"section of GATT\" is \"re\" and \"vise\" but that doesn't fit. \n",
      "\n",
      "Another angle: \"section of Gatt\" could be \"att\", which is the first three letters of GATT. Then \"unethically\" might relate to \"att\". Maybe \"uneth\" is part of it. If I take \"uneth\" and \"att\", but that doesn't make sense. Wait, \"uneth\" plus \"att\" would be \"unethatt\", which isn't a word. \n",
      "\n",
      "Alternatively, \"unethical\" is 9 letters, so maybe \"uneth\" (5 letters) and \"ic\" (2 letters) and \"ally\" (4 letters) but that's too long. \n",
      "\n",
      "Wait, maybe \"unethically\" is the adverb, and the main word is \"attune\" with \"uneth\" and \"ically\" but not quite. \n",
      "\n",
      "Alternatively, think of \"attune\" as the answer, where \"att\" is from GATT, and \"une\" is from \"unethically\". So \"attune\" is 6 letters. \"Att\" (the first three letters of \"GATT\") + \"une\" (from \"unethically\"). That fits. So the answer is \"attune\". But \"unethically\" is the adverb here? Wait, \"attune\" can be a verb meaning to adjust. The clue says \"adjust section of Gatt unethically\". So \"adjust\" is the action, \"section of Gatt\" is the part being adjusted, and \"unethically\" modifies how it's adjusted. So \"attune\" fits as the answer. Let me confirm. \"Section of Gatt\" would be \"att\" (the three letters from GATT), and \"unethically\" modifies it as an unethical way of adjusting. But \"unethically\" is the adverb here, so perhaps that's part of the wordplay. Alternatively, \"uneth\" (the first four letters of \"unethically\") plus \"tically\" (the last part), but that doesn't form a word. \n",
      "\n",
      "Alternatively, \"unethically\" is a part of the word \"unethical\", so maybe \"uneth\" is part of it, but then where does \"att\" come in? Maybe \"att\" (from GATT) and \"uneth\" (from unethically) make \"attuneth\", but that's not a word. \n",
      "\n",
      "Wait, maybe the word is \"attune\" as before, and the clue is \"adjust section of Gatt unethically\", meaning the section of Gatt (att) is adjusted in an unethically way. So \"unethically\" modifies \"att\" to make \"attune\". But that seems a bit indirect. Alternatively, \"unethically\" might be part of the answer, and \"att\" is from GATT. So combining \"uneth\" (part of \"unethically\") and \"att\", making \"unethatt\", but that's not right. \n",
      "\n",
      "Alternatively, maybe \"unethically\" is part of the word, like \"uneth\" and \"ically\". Let me check \"unethically\" spelled out: U-N-E-T-H-I-C-A-L-L-Y. If the answer is a 6-letter word, then maybe taking the first letters U-N-E-T-H-I-C... but that's too many. Wait, maybe \"unethic\" (but that's 7 letters) and then \"ally\". No. \n",
      "\n",
      "Wait, maybe the word is \"unethic\" but it's 7 letters. Hmm. \n",
      "\n",
      "Alternatively, \"unethical\" is split into \"uneth\" (5 letters) and \"ical\" (4 letters), but adding them would be 9. Not helpful. \n",
      "\n",
      "I think I'm going in circles here. Let's try another approach. The clue says \"adjust section of Gatt unethically\". The answer is a 6-letter word. The key elements are \"adjust\", \"section of Gatt\", and \"unethically\". Maybe \"adjust\" is part of the word, and \"section of Gatt\" is a part. Let's take \"att\" from Gatt, then \"unethically\" could be part of the word. But how? \"Attunethically\"? That doesn't make sense. \n",
      "\n",
      "Alternatively, the word is \"attune\", which is 6 letters, meaning adjust. The \"section of GATT\" is \"att\", and \"unethically\" is an adverb modifying \"attune\" to make \"unethically\". So the answer is \"attune\", with the clue indicating that it's adjusted in an unethically manner. But \"unethically\" is the adverb here. Maybe \"uneth\" is part of the word, and \"tically\" is from \"unethically\", but that doesn't fit. \n",
      "\n",
      "Alternatively, \"unethically\" is part of the word, like \"uneth\" and \"ically\". Let me try combining \"uneth\" (from \"unethically\") and \"att\" (from Gatt). So \"unethatt\" but that's not a word. \n",
      "\n",
      "Wait, maybe \"unethical\" is part of the word. Let's take the letters from \"unethical\" and combine them with \"att\" from Gatt. For example, \"att\" + \"uneth\" = \"attuneth\", but that's too long. Alternatively, take \"uneth\" from \"unethical\" and \"att\" from Gatt to make \"unethatt\" which is not valid. \n",
      "\n",
      "I'm really stuck. Let me check if there's another angle. \"Adjust\" is the main action. The answer is 6 letters. Let's consider \"optimize\". Optimize is 8 letters. No. \"Adjust\" itself is 7 letters. But maybe a synonym is needed. \n",
      "\n",
      "Alternatively, \"modify\" is 6 letters. But \"modify\" can be adjusted. \"Section of Gatt\" might be \"mody\" (from modify), but that doesn't make sense. \n",
      "\n",
      "Wait, \"modify\" has \"mod\" and \"ify\". If the section of Gatt is \"mod\" and \"ify\" comes from \"unethically\"? Not sure. \n",
      "\n",
      "Alternatively, \"adjust\" could be \"re\" + \"vise\". But where does \"section of Gatt\" come in? \n",
      "\n",
      "Wait, maybe the answer is \"attune\" and the wordplay is \"section of Gatt\" is \"att\" (first three letters) and \"unethically\" is \"uneth\", making \"attuneth\", but that's not a word. \n",
      "\n",
      "Alternatively, \"uneth\" (from \"unethically\") and \"att\" (from Gatt) make \"unethatt\", which isn't valid. \n",
      "\n",
      "This is challenging. Let's try to look for a 6-letter word that combines \"att\" (from Gatt) and \"uneth\". If we take \"uneth\" plus \"att\", that doesn't work. Wait, \"uneth\" plus \"att\" would be \"unethatt\", which isn't a word. But perhaps the word is \"unethic\", but that's 7 letters. \n",
      "\n",
      "Wait, what about \"unethic\" minus one letter? If I remove an \"i\" from \"unethical\", but that doesn't help. \n",
      "\n",
      "Alternatively, maybe \"unethic\" + \"al\" but that's 9 letters. \n",
      "\n",
      "Wait, let me think again. If the answer is \"attune\", then the breakdown would be \"att\" from GATT (section of Gatt) and \"une\" from \"unethically\". So \"attune\" would be the answer, with \"unethically\" acting as an adverb, but it seems the clue is saying \"adjust section of Gatt unethically\", which would mean \"attune\" with \"unethically\" modifying the adjustment. But perhaps \"unethically\" is just part of the word. \n",
      "\n",
      "Alternatively, \"unethically\" could be \"uneth\" (the first four letters) + \"ically\" (the last four letters). If I take the first four letters of \"unethically\" and combine them with \"att\" from Gatt, but that gives \"unethatt\", which isn't a word. \n",
      "\n",
      "Alternatively, maybe the word is \"uneth\" + \"tically\". \"Uneth\" (from \"unethically\") and \"tically\" (the end of \"unethically\"), but that would be \"uneth\" and \"tically\", which doesn't make a word. \n",
      "\n",
      "I'm really struggling here. Let me try to think of synonyms for \"adjust\" again. \"Revise\" is a synonym. Let's consider \"revise\". How does \"section of Gatt\" come into play? \"Revise\" is R-E-V-I-S-E, but \"section of Gatt\" is G-A-T-T. Maybe \"re\" is part of GATT. So \"revise\" could be \"re\" from GATT (as in G, A, T, T) and then \"vis\" from \"vis\" in \"revise\". But not sure. \n",
      "\n",
      "Another angle: The word \"unethically\" might contain the letters \"uneth\" which are part of the answer. If the answer is a 6-letter word, and \"uneth\" is a part of it, then the word would have to be \"unethic\" but that's 7 letters. So not. \n",
      "\n",
      "Wait, maybe the word is \"unethic\" with a letter removed. But that's 7 letters. \n",
      "\n",
      "Alternatively, \"unethically\" is the word, but with some letters removed. If we remove the \"ally\" part, but how? \n",
      "\n",
      "I'm going around in circles. Let me consider that \"unethically\" might be the word, but I need to fit the letters into 6. Wait, \"uneth\" (5 letters) + \"ic\" (2 letters) + \"ally\" (4 letters) = 11 letters. No. \n",
      "\n",
      "Alternatively, maybe \"unethic\" (7 letters) but that's too long. \n",
      "\n",
      "Wait, maybe \"uneth\" (4 letters) + \"ic\" (2) + \"ally\" (4) = 10, but no. \n",
      "\n",
      "Alternatively, \"unethically\" is split into \"uneth\" (5) and \"ically\" (5). Not 6. \n",
      "\n",
      "This is really tricky. Let me try to think of the answer as \"attune\" again, with \"uneth\" from \"unethically\" and \"att\" from \"GATT\". So \"att\" is part of GATT, and \"uneth\" is part of \"unethically\". Then combining them gives \"attuneth\", which isn't a word. But if we remove the \"h\" from \"uneth\" to make \"uneth\" and \"att\" to form \"unethatt\", nope. \n",
      "\n",
      "Alternatively, maybe the answer is \"unethic\" (7 letters) but that's too long. \n",
      "\n",
      "Wait, perhaps \"unethic\" is part of the word, and \"att\" is another part. Like \"unethic\" + \"att\"? But that would be too long. \n",
      "\n",
      "Alternatively, maybe \"uneth\" and \"att\" make \"unethatt\", but not a word. \n",
      "\n",
      "I think I need to give up and check the answer. Wait, the answer is probably \"attune\". Let me confirm:\n",
      "\n",
      "\"Adjust\" = \"attune\". \"Section of Gatt\" refers to the first three letters of GATT, which is \"att\". \"Unethically\" refers to \"une\" from \"unethically\" (the first four letters). But \"uneth\" + \"att\" = \"unethatt\", which isn't a word. Hmm, maybe the answer is not \"attune\" because of this. \n",
      "\n",
      "Wait, maybe \"attune\" is correct, and \"unethically\" is an adverb that modifies the action. But how does \"unethically\" fit into the wordplay? Maybe \"uneth\" and \"tically\" from \"unethically\", but I don't see it. \n",
      "\n",
      "Alternatively, \"unethically\" is a synonym for \"unethically\". So if I take \"uneth\" (from \"unethically\") and \"att\" (from Gatt), maybe it's \"unethatt\" but not a word. \n",
      "\n",
      "I think I'm stuck. Let me check if \"attune\" is a possible answer. If the clue is \"adjust section of Gatt unethically\", and \"attune\" is the answer, then \"section of Gatt\" would be \"att\", and \"unethically\" is just the way it's done, but perhaps the wordplay is \"uneth\" (from \"unethically\") + \"att\" (from Gatt) = \"unethatt\" but that's not a word. Maybe it's a different approach. \n",
      "\n",
      "Alternatively, maybe \"attune\" is the answer, and the clue is saying that the section of GATT (which is \"att\") is adjusted in an unethically way, but \"unethically\" isn't part of the word. \n",
      "\n",
      "Alternatively, the clue might be \"Adjust (section of Gatt) unethically (6)\". So the answer is a word that means adjust, with \"section of Gatt\" being \"att\" and \"unethically\" being part of the answer. Maybe the answer is \"uneth\" + \"att\", but that doesn't make a word. \n",
      "\n",
      "I'm really struggling here. Let me think of possible 6-letter words that relate to \"adjust\" and \"unethically\". If the answer is \"unethic\", but that's 7 letters. Wait, \"unethic\" is a variant spelling of \"unethical\", but it's 7 letters. No. \n",
      "\n",
      "Wait, maybe \"uneth\" (4 letters) + \"ic\" (2) + \"ally\" (4) = 10. Not helpful. \n",
      "\n",
      "Alternatively, maybe \"unethically\" is \"uneth\" plus \"ically\". If the answer is \"uneth\" plus \"ic\" plus \"ally\", but that's 11 letters. No. \n",
      "\n",
      "This is getting frustrating. Let's try another approach. The word \"unethically\" has the letters \"U-N-E-T-H-I-C-A-L-L-Y\". Maybe we can combine some of these letters. The answer is 6 letters. Let's see: U-N-E-T-H-I-C. Wait, but that's 7 letters. Not helpful. \n",
      "\n",
      "Alternatively, maybe \"unethically\" can be split into \"uneth\" and \"ically\", but \"uneth\" + \"ically\" = \"unethically\". No. \n",
      "\n",
      "Alternatively, \"uneth\" and \"ic\" and \"ally\" – again, too long. \n",
      "\n",
      "Wait, maybe \"unethically\" is the adverb, and \"attune\" is the main word. But \"unethically\" is a 9-letter word, which is too long. So perhaps the answer is \"attune\", and the wordplay is that \"section of Gatt\" is \"att\", and \"uneth\" is a part of \"unethically\" (the first four letters). So \"attuneth\", but that's not a word. \n",
      "\n",
      "Alternatively, maybe \"att\" (from Gatt) + \"uneth\" (from unethically) = \"attuneth\" which isn't valid. \n",
      "\n",
      "I think I've tried everything. Maybe the answer is \"attune\". The clue is a bit tricky, but \"attune\" fits as \"adjust\", \"section of Gatt\" is \"att\", and \"unethically\" is just part of the wordplay indicating that it's an unethical adjustment, though not part of the actual answer. \n",
      "\n",
      "Alternatively, maybe the answer is \"attune\" with \"unethic\" (part of \"unethically\") and \"ally\" (part of the word), but that doesn't fit. \n",
      "\n",
      "Wait, \"attune\" is 6 letters, and \"unethically\" is an adverb. The word \"adjust\" is \"attune\", and the clue says \"adjust section of Gatt unethically\". So maybe \"unethic\" is part of the word, but I can't figure out how. \n",
      "\n",
      "At this point, I'm going to have to guess that the answer is \"attune\", even though I'm not entirely confident. Let me just check one more time. \n",
      "\n",
      "\"Attune\" is 6 letters. \"Section of Gatt\" could be \"att\" from GATT (since \"att\" is part of \"GATT\"), and \"uneth\" from \"unethically\". If you combine \"att\" and \"uneth\", that gives \"attuneth\", but that's not a word. However, \"attune\" is a word that means adjust. Maybe the clue is a bit misleading, but \"attune\" seems like the most plausible answer given the wordplay. \n",
      "\n",
      "Therefore, the answer is \\boxed{attune}.\n",
      "</think>\n",
      "\n",
      "The clue \"Adjust section of Gatt unethically (6)\" requires a 6-letter word. The wordplay involves \"section of Gatt\" referring to the first part \"att\" from the acronym GATT. \"Unethically\" indicates an unethical adjustment, so the answer is \"attune\" (6 letters), meaning to adjust, where \"att\" comes from GATT and \"une\" is derived from \"unethically\" (the first two letters). \n",
      "\n",
      "\\boxed{attune}<|im_end|>\n",
      "--------------------------------------------------------------------------------\n",
      "Token count: 5641\n"
     ]
    }
   ],
   "source": [
    "show_trace(DB_FILE_MERGED, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd552834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fffd_trace_indices(db_file: str, table: str = 'dataset') -> list[int]:\n",
    "    \"\"\"\n",
    "    Identify indices of rows in the 'trace' column that contain the Unicode replacement character � (U+FFFD).\n",
    "\n",
    "    Args:\n",
    "        db_file: Path to the SQLite database file.\n",
    "        table: Name of the table (default: 'dataset').\n",
    "\n",
    "    Returns:\n",
    "        List of row indices with traces containing � (U+FFFD).\n",
    "    \"\"\"\n",
    "    import sqlite3\n",
    "    import pandas as pd\n",
    "\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT trace FROM {table}\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    fffd_indices = [\n",
    "        idx for idx, trace in enumerate(df['trace']) \n",
    "        if trace is not None and '�' in str(trace)\n",
    "    ]\n",
    "\n",
    "    return fffd_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7a75aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "[2, 18, 36, 50, 59, 61, 71, 72, 76, 77, 78, 81, 91, 100, 102, 113, 120, 121, 131, 140, 144, 145, 159, 181, 185, 188, 194, 195, 197, 204, 211, 212, 273, 280, 305, 321, 325, 337, 359, 361, 369, 372, 379, 385, 394, 402, 418, 423, 428, 429, 430, 467, 474, 480, 484, 496, 502, 506, 511, 514, 522, 527, 531, 534, 536, 560, 562, 572, 578, 582, 585, 589, 598, 602, 607, 615, 618, 622, 627, 631, 636, 669, 673, 684, 686, 696, 720, 722, 723, 731, 735, 736, 739, 750, 752, 776, 778, 785, 793, 806, 811, 821, 824, 825, 830, 841, 855, 867, 875, 891, 926, 932]\n"
     ]
    }
   ],
   "source": [
    "DB_FILE = 'dataset_4qwen3_250622a_merged.db'\n",
    "\n",
    "contaminated_trace_indices = find_fffd_trace_indices(DB_FILE)\n",
    "\n",
    "print(len(contaminated_trace_indices))\n",
    "print(contaminated_trace_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90485acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "DB_FILE = 'dataset_4qwen3_250630a_merged.db'\n",
    "\n",
    "contaminated_trace_indices = find_fffd_trace_indices(DB_FILE)\n",
    "\n",
    "print(len(contaminated_trace_indices))\n",
    "print(contaminated_trace_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def print_index_groups(indices: list[int], n: int) -> None:\n",
    "    \"\"\"\n",
    "    Partition `indices` into `n` groups (last group may be shorter)\n",
    "    and print each group as a comma-separated string with no spaces.\n",
    "    \"\"\"\n",
    "    m = len(indices)                          # total indices\n",
    "    s = math.ceil(m / n)                      # group size  s = ⌈ m ⁄ n ⌉\n",
    "    \n",
    "    for k in range(n):\n",
    "        start, end = k * s, (k + 1) * s       # slice bounds\n",
    "        group = indices[start:end]\n",
    "        if group:                             # ignore empty tail groups\n",
    "            print(\",\".join(map(str, group)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e82980f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,18,36,50,59,61,71,72,76,77,78,81,91,100,102,113,120,121,131,140,144,145,159,181,185,188,194,195\n",
      "197,204,211,212,273,280,305,321,325,337,359,361,369,372,379,385,394,402,418,423,428,429,430,467,474,480,484,496\n",
      "502,506,511,514,522,527,531,534,536,560,562,572,578,582,585,589,598,602,607,615,618,622,627,631,636,669,673,684\n",
      "686,696,720,722,723,731,735,736,750,752,776,778,785,793,806,811,821,824,825,830,841,855,867,875,891,926,932\n"
     ]
    }
   ],
   "source": [
    "print_index_groups(contaminated_trace_indices, n=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def clear_fffd_traces(db_file: str,\n",
    "                      row_indices: list[int],\n",
    "                      table: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Set trace = '' for all rows in the table whose pandas index is in `row_indices`.\n",
    "    Assumes the table’s implicit ROWID order matches the DataFrame index.\n",
    "    \"\"\"\n",
    "    if not row_indices:\n",
    "        return\n",
    "\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    for idx in row_indices:\n",
    "        cur.execute(f\"UPDATE {table} SET trace = '' WHERE rowid = ?\", (idx + 1,))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18498e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"dataset_4qwen3.db\"\n",
    "\n",
    "clear_fffd_traces(DB_FILE, contaminated_trace_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d46cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from typing import Sequence\n",
    "\n",
    "def clear_trace_and_teacher(db_path: str,\n",
    "                            row_indices: Sequence[int],\n",
    "                            table: str = \"dataset\") -> None:\n",
    "    \"\"\"\n",
    "    Blank out both the `trace` **and** `teacher` columns for every row whose\n",
    "    *DataFrame* index appears in `row_indices`.\n",
    "\n",
    "    Assumption  \n",
    "    \\( \\text{ROWID}_{\\text{SQLite}} = \\text{pandas index} + 1 \\)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db_path : str\n",
    "        Path to the SQLite database file.\n",
    "    row_indices : Sequence[int]\n",
    "        Zero-based pandas indices to wipe.\n",
    "    table : str, optional\n",
    "        Target table name (default ``'dataset'``).\n",
    "    \"\"\"\n",
    "    if not row_indices:            # ∅ → no work\n",
    "        return\n",
    "\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Convert pandas indices → SQLite rowid (1-based)\n",
    "        params = [(idx + 1,) for idx in row_indices]\n",
    "\n",
    "        # Execute once per row via executemany:\n",
    "        cur.executemany(\n",
    "            f\"UPDATE {table} SET trace = '', teacher = '' WHERE rowid = ?\",\n",
    "            params\n",
    "        )\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78947f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"dataset_4qwen3.db\"\n",
    "\n",
    "clear_trace_and_teacher(DB_FILE, [35,70,103,137,365,807])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3b5e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def add_teacher_column(db_file: str, table_name: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Add a 'teacher' column to the database. For rows where 'trace' is not empty,\n",
    "    set 'teacher' to 's1.1-7B'. For empty trace rows, set 'teacher' to empty string.\n",
    "    \"\"\"\n",
    "    # Load the full table\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Create teacher column if it doesn't already exist\n",
    "    if 'teacher' not in df.columns:\n",
    "        df['teacher'] = ''\n",
    "    \n",
    "    # Check if trace is not empty/whitespace\n",
    "    non_empty_mask = df['trace'].str.strip() != ''\n",
    "    \n",
    "    # Set 'teacher' to 's1.1-7B' where trace is not empty\n",
    "    df.loc[non_empty_mask, 'teacher'] = 's1.1-7B'\n",
    "    \n",
    "    # Write back to database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df.to_sql(table_name, conn, index=False, if_exists='replace')\n",
    "    conn.close()\n",
    "    \n",
    "    # Report results\n",
    "    total_rows = len(df)\n",
    "    teacher_assigned = non_empty_mask.sum()\n",
    "    \n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Rows with teacher assigned: {teacher_assigned}\")\n",
    "    print(f\"Rows with empty teacher: {total_rows - teacher_assigned}\")\n",
    "\n",
    "def analyze_db_with_teacher(db_file: str, table_name: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of the database including trace and teacher columns.\n",
    "    Shows statistics for all teacher types and their distribution.\n",
    "    \"\"\"\n",
    "    # Load table\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Check if teacher column exists\n",
    "    if 'teacher' not in df.columns:\n",
    "        print(\"No 'teacher' column found in the database.\")\n",
    "        return\n",
    "    \n",
    "    # Trace metrics - no entries are null, only empty or non-empty\n",
    "    trace_empty_rows = (df['trace'].str.strip() == '').sum()\n",
    "    trace_non_empty = df['trace'].str.strip() != ''\n",
    "    \n",
    "    # Teacher metrics - no entries are null, only empty or non-empty\n",
    "    teacher_empty_rows = (df['teacher'].str.strip() == '').sum()\n",
    "    teacher_non_empty = df['teacher'].str.strip() != ''\n",
    "    \n",
    "    # Get unique teacher types (excluding empty)\n",
    "    unique_teachers = df[teacher_non_empty]['teacher'].unique()\n",
    "    teacher_counts = df['teacher'].value_counts(dropna=False)\n",
    "    \n",
    "    print(\"Trace column:\")\n",
    "    print(f\"    Empty rows: {trace_empty_rows}\")\n",
    "    print(f\"    Non-empty rows: {trace_non_empty.sum()}\")\n",
    "    \n",
    "    print(f\"\\nTeacher column:\")\n",
    "    print(f\"    Empty rows: {teacher_empty_rows}\")\n",
    "    print(f\"    Non-empty rows: {teacher_non_empty.sum()}\")\n",
    "    print(f\"    Unique teacher types: {len(unique_teachers)}\")\n",
    "    \n",
    "    # Show counts for each teacher type\n",
    "    for teacher_type in sorted(unique_teachers):\n",
    "        count = teacher_counts.get(teacher_type, 0)\n",
    "        print(f\"    '{teacher_type}': {count}\")\n",
    "    \n",
    "    # Consistency checks\n",
    "    trace_teacher_mismatch = trace_non_empty & ~teacher_non_empty\n",
    "    teacher_trace_mismatch = teacher_non_empty & ~trace_non_empty\n",
    "    \n",
    "    print(f\"\\nConsistency checks:\")\n",
    "    print(f\"    Rows with non-empty trace but empty teacher: {trace_teacher_mismatch.sum()}\")\n",
    "    print(f\"    Rows with non-empty teacher but empty trace: {teacher_trace_mismatch.sum()}\")\n",
    "\n",
    "def update_teacher_column(db_file: str, teacher_model: str, table_name: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Update the 'teacher' column for rows where trace is non-empty but teacher is empty.\n",
    "    Sets teacher to the specified teacher_model for these rows.\n",
    "    \n",
    "    Args:\n",
    "        db_file: Path to the SQLite database file\n",
    "        teacher_model: The teacher model string to assign (e.g., 's1.1-14B')\n",
    "        table_name: Name of the table in the database\n",
    "    \"\"\"\n",
    "    # Load the full table\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Check if teacher column exists\n",
    "    if 'teacher' not in df.columns:\n",
    "        print(\"Error: 'teacher' column not found in the database.\")\n",
    "        print(\"Please run add_teacher_column() first.\")\n",
    "        return\n",
    "    \n",
    "    # Find rows where trace is non-empty but teacher is empty\n",
    "    trace_non_empty = df['trace'].str.strip() != ''\n",
    "    teacher_empty = df['teacher'].str.strip() == ''\n",
    "    \n",
    "    # Mask for rows to update: trace non-empty AND teacher empty\n",
    "    update_mask = trace_non_empty & teacher_empty\n",
    "    \n",
    "    # Count rows before update for reporting\n",
    "    rows_to_update = update_mask.sum()\n",
    "    \n",
    "    if rows_to_update == 0:\n",
    "        print(\"No rows found where trace is non-empty and teacher is empty.\")\n",
    "        return\n",
    "    \n",
    "    # Update teacher column for matching rows\n",
    "    df.loc[update_mask, 'teacher'] = teacher_model\n",
    "    \n",
    "    # Write back to database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df.to_sql(table_name, conn, index=False, if_exists='replace')\n",
    "    conn.close()\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"Updated {rows_to_update} rows with teacher model: '{teacher_model}'\")\n",
    "    \n",
    "    # Show indices of updated rows\n",
    "    updated_indices = df[update_mask].index.tolist()\n",
    "    print(f\"Updated row indices: {updated_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0bc753cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1817\n",
      "Rows with teacher assigned: 157\n",
      "Rows with empty teacher: 1660\n"
     ]
    }
   ],
   "source": [
    "DB_FILE = 'dataset_4qwen3_250630a_merged.db'\n",
    "\n",
    "# Add teacher column to your database\n",
    "add_teacher_column(DB_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e56fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = 'dataset_4qwen3_250624a_merged.db'\n",
    "\n",
    "# Update teacher column for rows with non-empty trace but empty teacher\n",
    "update_teacher_column(DB_FILE, 's1.1-7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1997d820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace column:\n",
      "    Empty rows: 1660\n",
      "    Non-empty rows: 157\n",
      "\n",
      "Teacher column:\n",
      "    Empty rows: 1660\n",
      "    Non-empty rows: 157\n",
      "    Unique teacher types: 1\n",
      "    's1.1-7B': 157\n",
      "\n",
      "Consistency checks:\n",
      "    Rows with non-empty trace but empty teacher: 0\n",
      "    Rows with non-empty teacher but empty trace: 0\n"
     ]
    }
   ],
   "source": [
    "DB_FILE = 'dataset_4qwen3_250630a_merged.db'\n",
    "\n",
    "# Analyze the results\n",
    "analyze_db_with_teacher(DB_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import vllm\n",
    "print(vllm.__version__)\n",
    "\n",
    "# 2.6.0+cu124\n",
    "# 0.8.5.post1\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b27137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "DB_FILE = 'dataset_4qwen3.db'\n",
    "\n",
    "# Read the SQLite database back into a DataFrame\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "df = pd.read_sql('SELECT * FROM dataset', conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "null_count = df['trace'].isnull().sum()\n",
    "empty_string_count = (df['trace'] == '').sum()\n",
    "whitespace_only_count = df['trace'].str.strip().eq('').sum() if df['trace'].dtype == 'object' else 0\n",
    "total_empty = df['trace'].isnull().sum() + (df['trace'].str.strip() == '').sum()\n",
    "\n",
    "print(f\"Null/NaN values: {null_count}\")\n",
    "print(f\"Empty strings: {empty_string_count}\")\n",
    "print(f\"Whitespace-only strings: {whitespace_only_count}\")\n",
    "print(f\"Total empty rows: {total_empty}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Probability Analyzer - Check reasoning trace token probabilities with small model\n",
    "#\n",
    "# Analyzes an existing reasoning trace to check the probability of each token\n",
    "# according to a small model. Marks tokens with probability below threshold with asterisk.\n",
    "\n",
    "# --------------------------- imports ---------------------------------------\n",
    "import os, html, uuid, asyncio, contextlib, nest_asyncio, logging\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "from vllm import TokensPrompt\n",
    "from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "from vllm.sampling_params import SamplingParams, RequestOutputKind\n",
    "\n",
    "nest_asyncio.apply()\n",
    "torch.set_grad_enabled(False)\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "# --------------------------- configuration ---------------------------------\n",
    "SMALL_GPU_INDEX = \"0\"\n",
    "SMALL_MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "SMALL_TEMPERATURE = 0.7\n",
    "MAX_SEQ_LEN = 8192\n",
    "PROB_THRESHOLD = 0.01\n",
    "\n",
    "# ---------------- utility: temporarily set visible GPUs --------------------\n",
    "@contextlib.contextmanager\n",
    "def visible_gpus(devices: str):\n",
    "    original = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "    print(f\"\\nCUDA_VISIBLE_DEVICES = {devices}\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = original\n",
    "\n",
    "# --------------------------- engine setup ----------------------------------\n",
    "async def setup_small_engine():\n",
    "    global small_engine, small_tokenizer, small_vocab_size\n",
    "    \n",
    "    small_checkpoint = snapshot_download(SMALL_MODEL_NAME)\n",
    "\n",
    "    with visible_gpus(SMALL_GPU_INDEX):\n",
    "        print(\"torch sees\", torch.cuda.device_count(), \"GPU(s)\")              \n",
    "        small_engine = AsyncLLMEngine.from_engine_args(\n",
    "            AsyncEngineArgs(model=small_checkpoint, \n",
    "                            tensor_parallel_size=1,\n",
    "                            max_model_len=MAX_SEQ_LEN, \n",
    "                            gpu_memory_utilization=0.20,\n",
    "                            dtype=\"bfloat16\"),\n",
    "            start_engine_loop=True)\n",
    "        \n",
    "        small_tokenizer = await small_engine.get_tokenizer()\n",
    "\n",
    "    # Get model config using async method\n",
    "    small_model_config = await small_engine.get_model_config()\n",
    "    small_vocab_size = small_model_config.get_vocab_size()\n",
    "    \n",
    "    print(f\"Small vocab size: {small_vocab_size}\")\n",
    "\n",
    "# --------------------------- sampling params -------------------------------\n",
    "small_sampling_params = SamplingParams(\n",
    "    max_tokens=1,\n",
    "    temperature=SMALL_TEMPERATURE,\n",
    "    top_p=0.95, \n",
    "    logprobs=20,  # vLLM's max allowed logprob size\n",
    "    output_kind=RequestOutputKind.DELTA,\n",
    ")\n",
    "\n",
    "# -------------------------- helper functions -------------------------------\n",
    "def html_heatmap(token_ids, probabilities, tokenizer):\n",
    "    \"\"\"Create heatmap visualization of token probabilities\"\"\"\n",
    "    \n",
    "    def colour(probability):\n",
    "        if probability < PROB_THRESHOLD:\n",
    "            return \"rgb(255,0,0)\"  # Red for below threshold\n",
    "        else:\n",
    "            return \"rgb(0,0,0)\"    # Black for above threshold\n",
    "    \n",
    "    spans = []\n",
    "    \n",
    "    # Find token groups that form complete characters\n",
    "    token_groups = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(token_ids):\n",
    "        # Start with current token\n",
    "        group_start = i\n",
    "        group_end = i + 1\n",
    "        \n",
    "        # Expand the group until we have a valid UTF-8 sequence\n",
    "        while group_end <= len(token_ids):\n",
    "            # Try decoding the current group\n",
    "            group_text = tokenizer.decode(token_ids[group_start:group_end])\n",
    "            \n",
    "            if '\\ufffd' not in group_text:\n",
    "                # Valid decode, but check if we should include more tokens\n",
    "                if group_end < len(token_ids):\n",
    "                    # Check if adding the next token changes the decode\n",
    "                    extended_text = tokenizer.decode(token_ids[group_start:group_end+1])\n",
    "                    current_plus_next = group_text + tokenizer.decode([token_ids[group_end]])\n",
    "                    \n",
    "                    if extended_text != current_plus_next or '\\ufffd' in current_plus_next:\n",
    "                        # Next token is part of this character, continue\n",
    "                        group_end += 1\n",
    "                        continue\n",
    "                \n",
    "                # We have a complete group\n",
    "                break\n",
    "            else:\n",
    "                # Invalid decode, need more tokens\n",
    "                group_end += 1\n",
    "                if group_end > len(token_ids):\n",
    "                    # Reached end with incomplete sequence\n",
    "                    group_end = len(token_ids)\n",
    "                    break\n",
    "        \n",
    "        # Store the group\n",
    "        token_groups.append((group_start, group_end))\n",
    "        i = group_end\n",
    "    \n",
    "    # Now render each group\n",
    "    for group_start, group_end in token_groups:\n",
    "        # Decode the group\n",
    "        text = tokenizer.decode(token_ids[group_start:group_end])\n",
    "        \n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        escaped = html.escape(text).replace(\" \", \"&nbsp;\")\n",
    "        \n",
    "        # Check if any token in this group has low probability\n",
    "        any_low_prob = False\n",
    "        min_prob = 1.0\n",
    "        \n",
    "        for token_idx in range(group_start, group_end):\n",
    "            if token_idx < len(probabilities):\n",
    "                prob = probabilities[token_idx]\n",
    "                min_prob = min(min_prob, prob)\n",
    "                any_low_prob = any_low_prob or (prob < PROB_THRESHOLD)\n",
    "        \n",
    "        style = f\"color:{colour(min_prob)};\"\n",
    "        if any_low_prob:\n",
    "            style += \" text-decoration:underline;\"\n",
    "        spans.append(f\"<span style='{style}'>{escaped}</span>\")\n",
    "    \n",
    "    return HTML(\"<pre style='white-space:pre-wrap; line-height:1.45; \"\n",
    "                \"font-family:inherit; background:#fff; padding:8px; \"\n",
    "                \"border:1px solid #ddd;'>\" + \"\".join(spans) + \"</pre>\")\n",
    "\n",
    "# ------------------------- core analysis loop ------------------------------\n",
    "async def one_step_analyze(context_ids):\n",
    "    \"\"\"Get probability distribution for next token given context\"\"\"\n",
    "    tokens_prompt = TokensPrompt(prompt_token_ids=context_ids)\n",
    "    generator = small_engine.generate(tokens_prompt, small_sampling_params, request_id=str(uuid.uuid4()))\n",
    "    return (await anext(generator)).outputs[0]\n",
    "\n",
    "async def analyze_trace(prompt_part: str, trace_part: str):\n",
    "    \"\"\"Analyze each token in the trace part for its probability\"\"\"\n",
    "    \n",
    "    # Tokenize the prompt and trace separately\n",
    "    prompt_token_ids = small_tokenizer.encode(prompt_part)\n",
    "    trace_token_ids = small_tokenizer.encode(trace_part)\n",
    "    \n",
    "    print(f\"Prompt tokens: {len(prompt_token_ids)}\")\n",
    "    print(f\"Trace tokens: {len(trace_token_ids)}\")\n",
    "    print(f\"Analyzing {len(trace_token_ids)} trace tokens...\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Step\\tProb\\tTok_ID\\tTok_Txt\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    probabilities = []\n",
    "    records = []\n",
    "    \n",
    "    # For each token in the trace, check its probability\n",
    "    for step_index in range(len(trace_token_ids)):\n",
    "        # Context is: prompt + trace tokens up to this position\n",
    "        context_ids = prompt_token_ids + trace_token_ids[:step_index]\n",
    "        \n",
    "        # The token we're analyzing\n",
    "        actual_token_id = trace_token_ids[step_index]\n",
    "        actual_token_text = small_tokenizer.decode([actual_token_id])\n",
    "        \n",
    "        # Get model's probability distribution for next token\n",
    "        output = await one_step_analyze(context_ids)\n",
    "        \n",
    "        # Extract probabilities from model output\n",
    "        logprobs_dict = output.logprobs[0]\n",
    "        \n",
    "        # Get probability of the actual token that was used\n",
    "        if actual_token_id in logprobs_dict:\n",
    "            actual_prob = torch.exp(torch.tensor(logprobs_dict[actual_token_id].logprob)).item()\n",
    "        else:\n",
    "            actual_prob = 0.0  # Token not in top predictions\n",
    "        \n",
    "        probabilities.append(actual_prob)\n",
    "        \n",
    "        # Check if probability is below threshold\n",
    "        low_prob = actual_prob < PROB_THRESHOLD\n",
    "        \n",
    "        record = {\n",
    "            'step': step_index + 1,\n",
    "            'token_id': actual_token_id,\n",
    "            'token_text': actual_token_text,\n",
    "            'probability': actual_prob,\n",
    "            'low_prob': low_prob\n",
    "        }\n",
    "        records.append(record)\n",
    "        \n",
    "        print(f\"{step_index + 1:4d}{'*' if low_prob else ' '}\\t\"\n",
    "              f\"{actual_prob:.4f}\\t\"\n",
    "              f\"{actual_token_id}\\t'{actual_token_text}'\",\n",
    "              flush=True)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Display the heatmap for trace tokens only\n",
    "    display(html_heatmap(trace_token_ids, probabilities, small_tokenizer))\n",
    "    \n",
    "    # Print only tokens below probability threshold\n",
    "    low_prob_records = [record for record in records if record['low_prob']]\n",
    "    if low_prob_records:\n",
    "        print(f\"\\nTokens below {PROB_THRESHOLD} probability threshold:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Step\\tProb\\tTok_ID\\tTok_Txt\")\n",
    "        print(\"-\" * 80)\n",
    "        for record in low_prob_records:\n",
    "            print(f\"{record['step']:4d}*\\t\"\n",
    "                  f\"{record['probability']:.4f}\\t\"\n",
    "                  f\"{record['token_id']}\\t'{record['token_text']}'\")\n",
    "        print(\"-\" * 80)\n",
    "    else:\n",
    "        print(f\"\\nNo tokens below {PROB_THRESHOLD} probability threshold found.\")\n",
    "    \n",
    "    # Statistics\n",
    "    low_prob_count = sum(record['low_prob'] for record in records)\n",
    "    print(f\"\\nLow probability tokens (< {PROB_THRESHOLD}): {low_prob_count}/{len(records)} \"\n",
    "          f\"({low_prob_count/len(records)*100:.2f}%)\")\n",
    "    \n",
    "    return records\n",
    "\n",
    "# ---------------------- high-level convenience -----------------------------\n",
    "async def run_analysis(prompt_part: str, trace_part: str):\n",
    "    \"\"\"Main function to run the analysis\"\"\"\n",
    "    return await analyze_trace(prompt_part, trace_part)\n",
    "\n",
    "# ------------------------ fire up the engine ------------------------------\n",
    "await setup_small_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- example usage ---------------------------------\n",
    "idx = 0  # Set the dataframe index you want to analyze\n",
    "\n",
    "# Get question and trace from dataframe\n",
    "question = df['question'].iloc[idx]\n",
    "trace = df['trace'].iloc[idx]\n",
    "\n",
    "# Create the prompt part (ends with \"<think>\")\n",
    "prompt_part = f\"\"\"A conversation between User and Assistant. The User asks a question, and the Assistant responds in two clearly defined sections: 1. Reasoning Process - A step-by-step, logical exploration and analysis of the problem, enclosed within <think> and </think> tags. 2. Answer - A direct and concise response based on the reasoning process, with the final answer enclosed within \\\\boxed{{}}. For example, \n",
    "<think>\n",
    "reasoning process here\n",
    "</think>\n",
    "answer here\n",
    "\\\\boxed{{final answer here}}\n",
    "\n",
    "Now, continue the actual conversation below.\n",
    "User: {question}\n",
    "Assistant:\n",
    "<think>\"\"\"\n",
    "\n",
    "# The trace part is just the trace\n",
    "trace_part = trace\n",
    "\n",
    "# Run the analysis\n",
    "records = await run_analysis(prompt_part, trace_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6daf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def non_empty_trace_indices(db_path: str, table: str = \"dataset\", col: str = \"trace\") -> list[int]:\n",
    "    \"\"\"\n",
    "    Return a list of row indices (SQLite `rowid`s) whose **trace** column\n",
    "    is neither NULL nor an empty / whitespace-only string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db_path : str\n",
    "        Path to the SQLite database file.\n",
    "    table   : str, optional\n",
    "        Table name holding the data (default ``\"dataset\"``).\n",
    "    col     : str, optional\n",
    "        Column holding the trace text (default ``\"trace\"``).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[int]\n",
    "        SQLite rowids that satisfy the non-empty condition.\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        # Pull only rowid + trace to keep it light\n",
    "        df = pd.read_sql(f\"SELECT rowid AS idx, {col} FROM {table}\", conn)\n",
    "\n",
    "    mask = df[col].notna() & df[col].astype(str).str.strip().ne(\"\")\n",
    "    idx = df.loc[mask, \"idx\"]\n",
    "\n",
    "    return (idx - 1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ffca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"dataset_4qwen3.db\"\n",
    "\n",
    "indices = non_empty_trace_indices(DB_FILE)\n",
    "print(len(indices))\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af700086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- batch analysis wrapper -----------------------\n",
    "async def analyze_batch_low_prob_only(indices: list):\n",
    "    \"\"\"Analyze multiple traces and only print tokens below probability threshold\"\"\"\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        print(f\"Entry {idx}\")\n",
    "        \n",
    "        # Get question and trace from dataframe\n",
    "        question = df['question'].iloc[idx]\n",
    "        trace = df['trace'].iloc[idx]\n",
    "        \n",
    "        # Create the prompt part\n",
    "        prompt_part = f\"\"\"A conversation between User and Assistant. The User asks a question, and the Assistant responds in two clearly defined sections: 1. Reasoning Process - A step-by-step, logical exploration and analysis of the problem, enclosed within <think> and </think> tags. 2. Answer - A direct and concise response based on the reasoning process, with the final answer enclosed within \\\\boxed{{}}. For example, \n",
    "<think>\n",
    "reasoning process here\n",
    "</think>\n",
    "answer here\n",
    "\\\\boxed{{final answer here}}\n",
    "\n",
    "Now, continue the actual conversation below.\n",
    "User: {question}\n",
    "Assistant:\n",
    "<think>\"\"\"\n",
    "        \n",
    "        # The trace part is just the trace\n",
    "        trace_part = trace\n",
    "        \n",
    "        # Tokenize the prompt and trace separately\n",
    "        prompt_token_ids = small_tokenizer.encode(prompt_part)\n",
    "        trace_token_ids = small_tokenizer.encode(trace_part)\n",
    "        \n",
    "        probabilities = []\n",
    "        low_prob_records = []\n",
    "        \n",
    "        # For each token in the trace, check its probability\n",
    "        for step_index in range(len(trace_token_ids)):\n",
    "            # Context is: prompt + trace tokens up to this position\n",
    "            context_ids = prompt_token_ids + trace_token_ids[:step_index]\n",
    "            \n",
    "            # The token we're analyzing\n",
    "            actual_token_id = trace_token_ids[step_index]\n",
    "            actual_token_text = small_tokenizer.decode([actual_token_id])\n",
    "            \n",
    "            # Get model's probability distribution for next token\n",
    "            output = await one_step_analyze(context_ids)\n",
    "            \n",
    "            # Extract probabilities from model output\n",
    "            logprobs_dict = output.logprobs[0]\n",
    "            \n",
    "            # Get probability of the actual token that was used\n",
    "            if actual_token_id in logprobs_dict:\n",
    "                actual_prob = torch.exp(torch.tensor(logprobs_dict[actual_token_id].logprob)).item()\n",
    "            else:\n",
    "                actual_prob = 0.0  # Token not in top predictions\n",
    "            \n",
    "            probabilities.append(actual_prob)\n",
    "            \n",
    "            # Check if probability is below threshold\n",
    "            low_prob = actual_prob < PROB_THRESHOLD\n",
    "            \n",
    "            if low_prob:\n",
    "                record = {\n",
    "                    'step': step_index + 1,\n",
    "                    'token_id': actual_token_id,\n",
    "                    'token_text': actual_token_text,\n",
    "                    'probability': actual_prob,\n",
    "                    'low_prob': low_prob\n",
    "                }\n",
    "                low_prob_records.append(record)\n",
    "        \n",
    "        # Print only tokens below probability threshold\n",
    "        if low_prob_records:\n",
    "            print(\"\\nStep\\tProb\\tTok_ID\\tTok_Txt\")\n",
    "            for record in low_prob_records:\n",
    "                print(f\"{record['step']:4d}*\\t\"\n",
    "                      f\"{record['probability']:.4f}\\t\"\n",
    "                      f\"{record['token_id']}\\t'{record['token_text']}'\")\n",
    "            # Statistics\n",
    "            total_tokens = len(trace_token_ids)\n",
    "            low_prob_count = len(low_prob_records)\n",
    "            print(f\"\\nLow probability tokens (< {PROB_THRESHOLD}): {low_prob_count}/{total_tokens} \"\n",
    "                f\"({low_prob_count/total_tokens*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"\\nNo tokens below {PROB_THRESHOLD} probability threshold found.\")\n",
    "        \n",
    "# --------------------------- batch usage example --------------------------\n",
    "await analyze_batch_low_prob_only(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ae9673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def convert_db_to_df(db_file: str, table_name: str = 'dataset') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts SQLite database to DataFrame with specific text formatting.\n",
    "    \n",
    "    Args:\n",
    "        db_file: Path to the SQLite database file\n",
    "        table_name: Name of the table to read from (default: 'dataset')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 'question', 'answer', 'trace', and 'text' columns,\n",
    "        filtered to only include rows with non-empty traces.\n",
    "    \"\"\"\n",
    "    # Load data from database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Keep only the required columns (question, answer, trace)\n",
    "    required_cols = ['question', 'answer', 'trace']\n",
    "    df = df[required_cols]\n",
    "    \n",
    "    # Filter to keep only rows with non-empty trace\n",
    "    # Handle both null and empty/whitespace-only strings\n",
    "    non_empty_mask = df['trace'].notna() & (df['trace'].str.strip() != '')\n",
    "    df = df[non_empty_mask].reset_index(drop=True)\n",
    "    \n",
    "    # Define the system prompt\n",
    "    system_prompt = (\n",
    "        f\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\n\"\n",
    "        f\"You must respond to every query in the following manner:\\n\"\n",
    "        f\"First, provide a step-by-step logical exploration of the problem.\\n\"\n",
    "        f\"Then, provide a clear and direct response based on your reasoning, with the final answer enclosed in \\\\boxed{{}}.\"\n",
    "    )\n",
    "    \n",
    "    # Create the text column\n",
    "    def create_text(row):\n",
    "        question = row['question']\n",
    "        trace = row['trace']\n",
    "        \n",
    "        input_text = (\n",
    "            f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{question}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n<think>\"\n",
    "        )\n",
    "        \n",
    "        return input_text + trace\n",
    "    \n",
    "    df['text'] = df.apply(create_text, axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a9b99678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting DataFrame shape: (157, 4)\n",
      "Columns: ['question', 'answer', 'trace', 'text']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>trace</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Given a rational number, write it as a fractio...</td>\n",
       "      <td>128</td>\n",
       "      <td>\\nOkay, let's tackle this problem step by step...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Find the remainder when $9 \\times 99 \\times 99...</td>\n",
       "      <td>109</td>\n",
       "      <td>\\nOkay, let me try to figure out how to solve ...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One base of a trapezoid is $100$ units longer ...</td>\n",
       "      <td>181</td>\n",
       "      <td>\\nOkay, so I need to solve this problem about ...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let $P(x)$ be a polynomial with integer coeffi...</td>\n",
       "      <td>418</td>\n",
       "      <td>\\nOkay, let me try to figure out this problem ...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many positive integers less than 10,000 ha...</td>\n",
       "      <td>927</td>\n",
       "      <td>\\nOkay, let's see. The question is: How many p...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer  \\\n",
       "0  Given a rational number, write it as a fractio...    128   \n",
       "1  Find the remainder when $9 \\times 99 \\times 99...    109   \n",
       "2  One base of a trapezoid is $100$ units longer ...    181   \n",
       "3  Let $P(x)$ be a polynomial with integer coeffi...    418   \n",
       "4  How many positive integers less than 10,000 ha...    927   \n",
       "\n",
       "                                               trace  \\\n",
       "0  \\nOkay, let's tackle this problem step by step...   \n",
       "1  \\nOkay, let me try to figure out how to solve ...   \n",
       "2  \\nOkay, so I need to solve this problem about ...   \n",
       "3  \\nOkay, let me try to figure out this problem ...   \n",
       "4  \\nOkay, let's see. The question is: How many p...   \n",
       "\n",
       "                                                text  \n",
       "0  <|im_start|>system\\nYou are Qwen, created by A...  \n",
       "1  <|im_start|>system\\nYou are Qwen, created by A...  \n",
       "2  <|im_start|>system\\nYou are Qwen, created by A...  \n",
       "3  <|im_start|>system\\nYou are Qwen, created by A...  \n",
       "4  <|im_start|>system\\nYou are Qwen, created by A...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_FILE = \"dataset_4qwen3_250630a_merged.db\"\n",
    "\n",
    "df = convert_db_to_df(DB_FILE)\n",
    "\n",
    "print(f\"Resulting DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "05e12dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "You must respond to every query in the following manner:\n",
      "First, provide a step-by-step logical exploration of the problem.\n",
      "Then, provide a clear and direct response based on your reasoning, with the final answer enclosed in \\boxed{}.<|im_end|>\n",
      "<|im_start|>user\n",
      "Given a rational number, write it as a fraction in lowest terms and calculate the product of the resulting numerator and denominator. For how many rational numbers between 0 and 1 will $20_{}^{}!$ be the resulting product?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, let's tackle this problem step by step. The question is: Given a rational number between 0 and 1, write it as a fraction in lowest terms, and then calculate the product of the numerator and the denominator. We need to find how many such rational numbers will result in the product being equal to 20! (20 factorial). \n",
      "\n",
      "First, let me make sure I understand the problem. A rational number between 0 and 1 can be expressed as a fraction a/b where a and b are positive integers, with a < b, and the fraction is in its lowest terms. Then, we compute a * b, and we want to know how many such fractions have a * b = 20! \n",
      "\n",
      "So, essentially, we need to find all pairs (a, b) such that:\n",
      "\n",
      "1. 0 < a/b < 1\n",
      "2. a and b are coprime (since it's in lowest terms)\n",
      "3. a * b = 20!\n",
      "\n",
      "And then count these pairs. \n",
      "\n",
      "Let me break this down. Since a/b is between 0 and 1, it's equivalent to saying that a < b. Also, since the product a*b is 20!, that means that a and b are factors of 20! and they are coprime, and a < b.\n",
      "\n",
      "So, the problem reduces to finding all coprime pairs (a, b) where a * b = 20! and a < b. Then, the number of such pairs will be the answer.\n",
      "\n",
      "But since a and b are coprime, their greatest common divisor is 1. Therefore, we need to find all divisors d of 20! such that d < sqrt(20!) and d and 20!/d are coprime. Wait, actually, if a and b are coprime and a*b = 20!, then a and b must be coprime divisors of 20! such that their product is 20!. But since they are divisors, a must divide 20! and b must divide 20!, but they must also be co-prime.\n",
      "\n",
      "But if a and b are divisors of 20! and coprime, then a and b must be coprime divisors, so that a * b = 20!. But since 20! has a prime factorization, which we might need to consider.\n",
      "\n",
      "Wait, 20! is 20 × 19 × 18 × ... × 1. Let me recall that 20! = 2^18 × 3^8 × 5^4 × 7^2 × 11^1 × 13^1 × 17^1 × 19^1. \n",
      "\n",
      "So, the prime factors of 20! are all primes less than or equal to 20.\n",
      "\n",
      "So, if a and b are coprime divisors of 20!, then they must each consist of a subset of the prime factors such that their product is 20!. However, since they are coprime, they must not share any prime factors. Therefore, the prime factors of a and b must be disjoint. \n",
      "\n",
      "Therefore, to find all such pairs (a, b), we need to partition all the prime factors of 20! into two sets such that the first set is a and the second set is b. However, since a and b must be divisors, they must each have some subset of the prime factors. But since 20! is a product of primes, each prime factor must be assigned entirely to a or entirely to b, but since they have to be coprime, they cannot be split. Wait, actually, each prime factor can only be in one of the two sets. So, each prime in 20! has to be assigned either to a or to b, but not both. Hence, the number of ways to divide the prime factors into two sets (one for a and one for b) is 2^k, where k is the number of distinct prime factors of 20!. Since 20! has 8 distinct prime factors: 2,3,5,7,11,13,17,19. So, k=8.\n",
      "\n",
      "Therefore, the number of possible partitions is 2^8 = 256. However, this counts each pair (a, b) twice, except when a = b. However, since a and b have to be coprime and a < b, then a cannot equal b. Hence, each unordered pair {a, b} is counted twice, except when a = b, but since they must be coprime and a < b, there are no such cases where a = b. Therefore, total number of ordered pairs (a, b) such that a*b = 20! and a < b would be 256/2 = 128. But wait, that can't be right.\n",
      "\n",
      "Wait, let me think again. If each prime factor is assigned to either a or b, then the total number of such assignments is 2^8, since there are 8 distinct primes. Each assignment defines a unique a and b where a is the product of primes assigned to a and b is the product of primes assigned to b. Since the product a*b is the product of all primes in 20!, which is 20!.\n",
      "\n",
      "However, each unordered pair {a, b} is counted exactly once in these 256 assignments, but in this problem, we need ordered pairs where a < b, right? So, actually, each ordered pair (a, b) with a ≠ b is counted twice, once as (a, b) and once as (b, a). Therefore, the number of ordered pairs is 256, but we need only those where a < b. Therefore, the number of such pairs is 256 / 2 = 128. But hold on, 256 divided by 2 is 128. But is that correct?\n",
      "\n",
      "Wait, let's think. Suppose the assignments of primes to a or b are 2^8 = 256. Each assignment corresponds to a unique ordered pair (a, b). Since a and b are coprime and their product is 20!, each assignment defines an ordered pair where a is the product of primes in one subset and b is the product of primes in the other. But each unordered pair {a, b} is counted exactly once in these 256 assignments. Therefore, if we have 256 ordered pairs, then half of them have a < b and the other half have a > b. However, when a and b are distinct, which they are since they are coprime and product is 20!. Therefore, the number of ordered pairs with a < b is 256 / 2 = 128.\n",
      "\n",
      "But let's test with a smaller number. For example, take 2! = 2. Then 2! = 2. The divisors of 2 are 1 and 2. The coprime pairs (a, b) with a * b = 2 and a < b would be (1, 2). Since 1 and 2 are coprime. So, there's 1 such pair, which would correspond to 2^1 = 2 possible assignments (each prime in 2! (which is just 2) can be assigned to a or b, so 2 assignments, leading to 1 ordered pair (a < b). So, 2^1 / 2 = 1. That works.\n",
      "\n",
      "Another example: 6! = 720. Let's see. Prime factors of 6! are 2,3,5. So, 3 primes. Then, number of assignments is 2^3=8. The ordered pairs (a, b) would be 8, with half (a < b). So 4 such pairs. Let's list them. If the primes are 2,3,5. Assign each to a or b.\n",
      "\n",
      "Possible assignments:\n",
      "\n",
      "1. a=1, b=6!\n",
      "2. a=2, b=3*5=15\n",
      "3. a=3, b=2*5=10\n",
      "4. a=5, b=2*3=6\n",
      "5. a=2*3=6, b=5\n",
      "6. a=2*5=10, b=3\n",
      "7. a=3*5=15, b=2\n",
      "8. a=2*3*5=30, b=1\n",
      "\n",
      "Then the ordered pairs where a < b are:\n",
      "\n",
      "(1, 6!), (2,15), (3,10), (5,6)\n",
      "\n",
      "So 4 ordered pairs. Which is 2^3 / 2 = 4. So that works. So applying this logic, for 20!, which has 8 distinct primes, the number of ordered pairs (a,b) with a < b and a*b=20! is 2^8 / 2 = 128.\n",
      "\n",
      "Therefore, the answer should be 128. Therefore, the final answer is \\boxed{128}.\n",
      "\n",
      "But wait, hold on. Let me check if this is correct. Let's think again.\n",
      "\n",
      "Each divisor a of 20! can be written as the product of primes in some subset of the prime factors. Similarly, b would be the product of the remaining primes. But since a and b must be coprime, each prime must be in either a or b, not both. Therefore, the number of such coprime pairs (a, b) where a*b=20! is equal to the number of ways to split the 8 distinct prime factors into two disjoint sets, which is 2^8. However, each unordered pair {a, b} is counted once. But since a and b are required to satisfy a < b, the number is half of 2^8. Hence, 128.\n",
      "\n",
      "But let me verify once again with the example of 6! (prime factors 2,3,5). 2^3=8 possibilities, and 2^3 / 2 =4 ordered pairs where a < b. Which is correct. Hence, this method is correct.\n",
      "\n",
      "Therefore, the answer should be 20! has 8 distinct prime factors, so the number is 2^8 /2 = 128.\n",
      "\n",
      "But wait, the problem says \"write it as a fraction in lowest terms\". So, if we have a/b, where a and b are coprime, then a and b are just divisors of 20! which are coprime. But in our problem, we need a and b to satisfy a * b = 20!, which is equivalent to a and b being divisors of 20! such that they are coprime.\n",
      "\n",
      "But is that the same as splitting the prime factors into two groups? Yes, because if a and b are coprime divisors of 20!, then each prime factor must be in one of them. Thus, the number of such pairs is equal to the number of ways to divide the prime factors into two groups. Which is 2^8. But since we need ordered pairs where a < b, then half of them.\n",
      "\n",
      "But maybe I need to confirm that a and b are uniquely determined by such a partition.\n",
      "\n",
      "Suppose we have a partition of the prime factors into two sets: one for a and one for b. Then, a is the product of primes in set 1, and b is the product of primes in set 2. Since these sets are disjoint and cover all primes in 20!, a and b are divisors of 20! with a * b = 20!. Also, because they are products of disjoint primes, they are coprime.\n",
      "\n",
      "Therefore, the total number of such pairs (a, b) is equal to the number of such partitions, which is 2^8 = 256, but since ordered pairs with a < b would be half of that, 128. Therefore, the answer is 128.\n",
      "\n",
      "But wait, is there any other way to get such pairs? Because for example, if we consider fractions where the numerator and denominator are not just divisors of 20! but any positive integers, but in lowest terms. For example, 1/20! is in lowest terms, and the product is 1, which is not 20!. So, the problem specifically states that the product is 20!, so the numerator and denominator in the fraction must multiply to 20! Therefore, a and b are divisors of 20! such that a*b = 20! and coprime.\n",
      "\n",
      "Therefore, our reasoning is correct. Therefore, the answer is 128.\n",
      "\n",
      "**Final Answer**\n",
      "\\boxed{128}\n",
      "</think>\n",
      "\n",
      "To solve the problem, we need to find how many rational numbers between 0 and 1, when written in lowest terms as a fraction $ \\frac{a}{b} $, result in the product $ a \\times b = 20! $.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Steps and Reasoning:\n",
      "\n",
      "1. **Representation of Rational Numbers**:\n",
      "   - Any rational number between 0 and 1 can be written as $ \\frac{a}{b} $ where $ a $ and $ b $ are coprime positive integers and $ a < b $.\n",
      "   - Since the fraction must be in lowest terms, $ \\gcd(a, b) = 1 $.\n",
      "\n",
      "2. **Product of Numerator and Denominator**:\n",
      "   - We need $ a \\times b = 20! $.\n",
      "\n",
      "3. **Prime Factorization of 20!**:\n",
      "   - The prime factorization of $ 20! $ is $ 2^{18} \\times 3^8 \\times 5^4 \\times 7^2 \\times 11 \\times 13 \\times 17 \\times 19 $, which includes 8 distinct prime factors.\n",
      "\n",
      "4. **Coprime Divisors**:\n",
      "   - For $ a $ and $ b $ to be coprime, each prime factor must belong entirely to either $ a $ or $ b $.\n",
      "   - This means that $ a $ and $ b $ must be formed by assigning each prime factor to either $ a $ or $ b $, ensuring they do not share any primes.\n",
      "\n",
      "5. **Counting Coprime Pairs**:\n",
      "   - Each prime factor can be assigned to either $ a $ or $ b $, resulting in $ 2^8 $ possible assignments (since there are 8 distinct primes).\n",
      "   - However, each pair $ (a, b) $ is counted twice (once as $ (a, b) $ and once as $ (b, a) $), except when $ a = b $, which is not possible since $ a < b $.\n",
      "\n",
      "6. **Final Calculation**:\n",
      "   - The number of ordered pairs $ (a, b) $ where $ a < b $ is $ \\frac{2^8}{2} = 128 $.\n",
      "\n",
      "Thus, the final answer is:\n",
      "\n",
      "$$\n",
      "\\boxed{128}\n",
      "$$<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c37237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_upload_dataset(df, dataset_remote_path):\n",
    "    \"\"\"\n",
    "    Create a HuggingFace dataset from DataFrame and upload it.\n",
    "    Replicates the structure of simplescaling/s1K-1.1_tokenized.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'question', 'answer', 'text' columns\n",
    "        dataset_remote_path: HuggingFace dataset path to upload to (e.g., \"jaeh8nkim/s1K_for_Qwen3-0.6B\")\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict: The created dataset, or None if upload failed\n",
    "    \"\"\"\n",
    "    from datasets import Dataset, DatasetDict\n",
    "    from huggingface_hub import create_repo, login\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load environment and login\n",
    "    load_dotenv()\n",
    "    login(token=os.getenv(\"HF_WRITE_TOKEN\"))\n",
    "    \n",
    "    print(f\"🔄 Creating dataset from DataFrame with {len(df)} samples...\")\n",
    "    \n",
    "    # Validate DataFrame structure\n",
    "    required_columns = ['question', 'answer', 'text']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"DataFrame missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Create dataset from DataFrame, keeping all three columns like the original s1K structure\n",
    "    train_data = {\n",
    "        'question': df['question'].tolist(),\n",
    "        'answer': df['answer'].tolist(), \n",
    "        'text': df['text'].tolist()\n",
    "    }\n",
    "    train_dataset = Dataset.from_dict(train_data)\n",
    "    \n",
    "    # Create DatasetDict with train split only (matching s1K-1.1_tokenized structure)\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset\n",
    "    })\n",
    "    \n",
    "    # Print dataset info before upload\n",
    "    print(f\"📊 Dataset structure:\")\n",
    "    print(f\"   Splits: {list(dataset_dict.keys())}\")\n",
    "    print(f\"   Train samples: {len(train_dataset)}\")\n",
    "    print(f\"   Columns: {train_dataset.column_names}\")\n",
    "    if len(train_dataset) > 0:\n",
    "        example_text = train_dataset[0]['text']\n",
    "        example_question = train_dataset[0]['question']\n",
    "        print(f\"   Example question length: {len(example_question)} characters\")\n",
    "        print(f\"   Example text length: {len(example_text)} characters\")\n",
    "        print(f\"   First 200 chars of question: {example_question[:200]}...\")\n",
    "        print(f\"   First 200 chars of text: {example_text[:200]}...\")\n",
    "    \n",
    "    # Create repository on HuggingFace\n",
    "    try:\n",
    "        create_repo(repo_id=dataset_remote_path, repo_type=\"dataset\", private=False, exist_ok=True)\n",
    "        print(f\"✅ Repository ready: https://huggingface.co/datasets/{dataset_remote_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Repository setup issue: {e}\")\n",
    "        # Continue anyway, might already exist\n",
    "    \n",
    "    # Upload dataset (this will create a .parquet file automatically)\n",
    "    try:\n",
    "        print(f\"📤 Uploading dataset to {dataset_remote_path}...\")\n",
    "        dataset_dict.push_to_hub(\n",
    "            dataset_remote_path,\n",
    "            commit_message=\"Upload tokenized dataset for Qwen3-0.6B training\"\n",
    "        )\n",
    "        print(f\"✅ Dataset uploaded successfully as .parquet file!\")\n",
    "        print(f\"🔗 View at: https://huggingface.co/datasets/{dataset_remote_path}\")\n",
    "        \n",
    "        # Verify upload by trying to load it back\n",
    "        print(\"🔍 Verifying upload...\")\n",
    "        from datasets import load_dataset\n",
    "        verification_dataset = load_dataset(dataset_remote_path)\n",
    "        print(f\"✅ Verification successful: {len(verification_dataset['train'])} samples loaded\")\n",
    "        print(f\"✅ Verified columns: {verification_dataset['train'].column_names}\")\n",
    "        \n",
    "        return dataset_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Upload failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_REMOTE_PATH = \"jaeh8nkim/s1K_for_Qwen3-0.6B\"\n",
    "dataset = create_and_upload_dataset(df, DATASET_REMOTE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c9ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
