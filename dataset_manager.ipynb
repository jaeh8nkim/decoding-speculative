{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e086839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def analyze_db(db_file: str, table_name: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Connects to the given SQLite database,\n",
    "    analyzes the 'trace' column for empty values,\n",
    "    and prints summary stats plus non-empty row indices.\n",
    "    \"\"\"\n",
    "    # Load table\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT trace FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Metrics\n",
    "    total_rows       = len(df)\n",
    "    null_rows        = df['trace'].isna().sum()\n",
    "    empty_str_rows   = (df['trace'] == '').sum()\n",
    "    whitespace_rows  = df['trace'].str.strip().eq('').sum()\n",
    "    empty_rows       = null_rows + whitespace_rows  # counts '' and all-whitespace\n",
    "    non_empty_mask   = df['trace'].notna() & (df['trace'].str.strip() != '')\n",
    "    non_empty_rows   = non_empty_mask.sum()\n",
    "    non_empty_indices = df[non_empty_mask].index.tolist()\n",
    "\n",
    "    # Output\n",
    "    print(f\"Total rows:        {total_rows}\")\n",
    "    print(f\"Null rows:         {null_rows}\")\n",
    "    print(f\"Empty rows:        {empty_rows}\")\n",
    "    print(f\"Non-empty rows:    {non_empty_rows}\\n\")\n",
    "    print(\"Non-empty row indices:\")\n",
    "    print(non_empty_indices)\n",
    "\n",
    "def merge_db(\n",
    "    db1_file: str,\n",
    "    db2_file: str,\n",
    "    table_name: str = 'dataset',\n",
    "    out_db_file: str = 'merged.db'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Merge two SQLite DBs by keeping 'question' and 'answer' from DB1\n",
    "    and merging 'trace' by choosing the shorter-token one on conflicts.\n",
    "    \"\"\"\n",
    "    # tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"Qwen/Qwen3-0.6B\", trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # load full df from DB1\n",
    "    conn1 = sqlite3.connect(db1_file)\n",
    "    df1 = pd.read_sql(f\"SELECT * FROM {table_name}\", conn1)\n",
    "    conn1.close()\n",
    "    \n",
    "    # load only trace from DB2\n",
    "    conn2 = sqlite3.connect(db2_file)\n",
    "    df2 = pd.read_sql(f\"SELECT trace FROM {table_name}\", conn2)\n",
    "    conn2.close()\n",
    "    \n",
    "    assert len(df1) == len(df2), \"Row counts differ\"\n",
    "    \n",
    "    t1 = df1['trace'].fillna('').astype(str)\n",
    "    t2 = df2['trace'].fillna('').astype(str)\n",
    "    \n",
    "    merged_traces = []\n",
    "    conflicts = []\n",
    "    \n",
    "    for i, (a, b) in enumerate(zip(t1, t2)):\n",
    "        empty1 = not a.strip()\n",
    "        empty2 = not b.strip()\n",
    "        if empty1 and not empty2:\n",
    "            choice = b\n",
    "        elif empty2 and not empty1:\n",
    "            choice = a\n",
    "        elif a == b:\n",
    "            choice = a\n",
    "        else:\n",
    "            # conflict: pick shorter in tokens\n",
    "            l1 = len(tokenizer(a).input_ids)\n",
    "            l2 = len(tokenizer(b).input_ids)\n",
    "            choice = a if l1 <= l2 else b\n",
    "            conflicts.append(i)\n",
    "        merged_traces.append(choice)\n",
    "    \n",
    "    # assign merged traces back to df1\n",
    "    df1['trace'] = merged_traces\n",
    "    \n",
    "    # write merged DB\n",
    "    conn_out = sqlite3.connect(out_db_file)\n",
    "    df1.to_sql(table_name, conn_out, index=False, if_exists='replace')\n",
    "    conn_out.close()\n",
    "    \n",
    "    # report\n",
    "    print(f\"Total rows:      {len(df1)}\")\n",
    "    print(f\"Conflicts found: {len(conflicts)}\")\n",
    "    print(\"Conflict indices:\", conflicts)\n",
    "\n",
    "def show_trace(db_file: str, idx: int, table_name: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Load the 'trace' column from the given SQLite database,\n",
    "    print the trace at row `idx` with separators, and its token count\n",
    "    using the Qwen/Qwen3-0.6B tokenizer.\n",
    "    \"\"\"\n",
    "    # load\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT trace FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    # bounds check\n",
    "    if idx < 0 or idx >= len(df):\n",
    "        raise IndexError(f\"Index {idx} out of range (0–{len(df)-1})\")\n",
    "\n",
    "    # get trace\n",
    "    trace = df.at[idx, 'trace'] or \"\"\n",
    "\n",
    "    sep = \"-\" * 80\n",
    "\n",
    "    # output with separators\n",
    "    print(f\"Trace[{idx}]:\")\n",
    "    print(sep)\n",
    "    print(trace)\n",
    "    print(sep)\n",
    "\n",
    "    # tokenize & count\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"Qwen/Qwen3-0.6B\", trust_remote_code=True\n",
    "    )\n",
    "    tokens = tokenizer(trace).input_ids\n",
    "    print(f\"Token count: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10b2a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE_1      = 'dataset_4qwen3_250624a_1.db'\n",
    "DB_FILE_2      = 'dataset_4qwen3_250624a_2.db'\n",
    "DB_FILE_MERGED = 'dataset_4qwen3_250624a_merged.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f12fab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:        1817\n",
      "Null rows:         0\n",
      "Empty rows:        1681\n",
      "Non-empty rows:    136\n",
      "\n",
      "Non-empty row indices:\n",
      "[0, 2, 5, 35, 41, 45, 50, 54, 59, 60, 61, 67, 70, 71, 72, 73, 77, 78, 81, 84, 91, 98, 100, 102, 103, 104, 111, 113, 118, 120, 131, 137, 140, 145, 159, 165, 171, 178, 185, 188, 194, 195, 197, 202, 204, 211, 212, 213, 233, 237, 241, 248, 258, 279, 280, 297, 303, 305, 321, 325, 331, 337, 359, 361, 363, 365, 369, 372, 379, 385, 390, 394, 395, 402, 408, 411, 418, 428, 429, 430, 431, 432, 443, 448, 463, 464, 474, 484, 488, 498, 509, 528, 537, 552, 553, 573, 586, 587, 588, 594, 601, 612, 621, 639, 646, 650, 653, 663, 665, 678, 679, 689, 697, 718, 734, 741, 748, 758, 764, 770, 789, 790, 794, 798, 807, 818, 819, 828, 831, 833, 843, 871, 877, 914, 918, 951]\n"
     ]
    }
   ],
   "source": [
    "analyze_db(DB_FILE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2f6b092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:        1817\n",
      "Null rows:         0\n",
      "Empty rows:        1681\n",
      "Non-empty rows:    136\n",
      "\n",
      "Non-empty row indices:\n",
      "[0, 5, 35, 41, 45, 54, 60, 67, 70, 73, 84, 98, 103, 104, 111, 118, 137, 165, 171, 178, 202, 213, 233, 237, 241, 248, 258, 279, 297, 303, 331, 363, 365, 390, 395, 408, 411, 431, 432, 443, 448, 463, 464, 488, 498, 502, 506, 509, 511, 514, 528, 531, 534, 536, 537, 552, 553, 560, 562, 573, 578, 582, 586, 587, 588, 594, 598, 601, 602, 607, 612, 615, 618, 621, 627, 631, 636, 639, 646, 650, 653, 663, 665, 669, 673, 678, 679, 684, 686, 689, 696, 697, 718, 720, 722, 723, 734, 735, 739, 741, 748, 750, 752, 758, 764, 770, 776, 778, 785, 789, 790, 794, 798, 806, 807, 811, 818, 819, 821, 824, 825, 828, 830, 831, 833, 843, 855, 867, 871, 877, 891, 914, 918, 926, 932, 951]\n"
     ]
    }
   ],
   "source": [
    "analyze_db(DB_FILE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0301f826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:      1817\n",
      "Conflicts found: 0\n",
      "Conflict indices: []\n"
     ]
    }
   ],
   "source": [
    "merge_db(\n",
    "    DB_FILE_1,\n",
    "    DB_FILE_2,\n",
    "    out_db_file=DB_FILE_MERGED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d915134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:        1817\n",
      "Null rows:         0\n",
      "Empty rows:        1636\n",
      "Non-empty rows:    181\n",
      "\n",
      "Non-empty row indices:\n",
      "[0, 2, 5, 35, 41, 45, 50, 54, 59, 60, 61, 67, 70, 71, 72, 73, 77, 78, 81, 84, 91, 98, 100, 102, 103, 104, 111, 113, 118, 120, 131, 137, 140, 145, 159, 165, 171, 178, 185, 188, 194, 195, 197, 202, 204, 211, 212, 213, 233, 237, 241, 248, 258, 279, 280, 297, 303, 305, 321, 325, 331, 337, 359, 361, 363, 365, 369, 372, 379, 385, 390, 394, 395, 402, 408, 411, 418, 428, 429, 430, 431, 432, 443, 448, 463, 464, 474, 484, 488, 498, 502, 506, 509, 511, 514, 528, 531, 534, 536, 537, 552, 553, 560, 562, 573, 578, 582, 586, 587, 588, 594, 598, 601, 602, 607, 612, 615, 618, 621, 627, 631, 636, 639, 646, 650, 653, 663, 665, 669, 673, 678, 679, 684, 686, 689, 696, 697, 718, 720, 722, 723, 734, 735, 739, 741, 748, 750, 752, 758, 764, 770, 776, 778, 785, 789, 790, 794, 798, 806, 807, 811, 818, 819, 821, 824, 825, 828, 830, 831, 833, 843, 855, 867, 871, 877, 891, 914, 918, 926, 932, 951]\n"
     ]
    }
   ],
   "source": [
    "analyze_db(DB_FILE_MERGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21f064cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace[448]:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Okay, let me try to work through this crossword puzzle clue. The clue is \"Adjust section of Gatt unethically\" with 6 letters. Wait, \"Adjust\" is 6 letters? Let me count: A-D-J-U-S-T. Yeah, that's six letters. So the answer is \"Adjust\" with 6 letters. But the clue mentions \"section of Gatt unethically.\" Hmm, maybe I need to break down the clue further. \n",
      "\n",
      "First, \"section\" could mean a part or a block. \"Adjust\" is the word we need, but how does \"section of Gatt\" come into play? Maybe \"section\" is a component of the word \"Adjust\". Let me think about possible words. \"Section\" in a crossword might refer to a part of a word, like a substring. So if I take \"Adjust\" and find a section that matches \"section of Gatt\" when combined. Wait, maybe \"section\" here is a part of the word \"Adjust\" or maybe a word formed by combining parts. \n",
      "\n",
      "Alternatively, maybe \"section of Gatt\" is a word that when adjusted would give the answer. Wait, the clue is \"Adjust section of Gatt unethically.\" So \"unethically\" might be a typo or part of the word. Wait, \"unethically\" is a word, but how does that fit? Maybe \"unethically\" is supposed to be part of the answer, but perhaps it's a play on words. Let me think. \"Adjust\" has 6 letters, and the clue mentions \"section of Gatt unethically\". Maybe \"section of Gatt\" is a word that when adjusted (i.e., changed) gives \"Adjust\". Alternatively, maybe \"unethically\" is part of a wordplay, such as an anagram or hidden word.\n",
      "\n",
      "Alternatively, perhaps \"section of Gatt\" is \"GATT\" itself, but that's four letters. Wait, \"Gatt\" is part of the word, and \"unethically\" might be a word to combine with it. Maybe \"GATT\" plus something else. But the answer needs to be 6 letters. Let's think of the word \"adjust\". Maybe \"adjust\" can be broken down as \"a-j-d-u-s-t\". If \"section of Gatt unethically\" refers to a part of \"Gatt\" that is adjusted, perhaps \"att\" from \"Gatt\" is adjusted? But then how does \"unethically\" fit in? Alternatively, maybe \"unethically\" is \"unethic\" + \"ally\", but that seems unlikely. \n",
      "\n",
      "Wait, perhaps \"unethically\" is a typo, and the actual word is \"unethic\"? But the clue says \"unethically\", so maybe it's part of the wordplay. Let's consider that \"unethically\" is an anagram or part of a word. Alternatively, maybe \"section\" refers to a part of the word \"GATT\", which is \"GATT\", and when you add something else. Wait, \"adjust\" is a 6-letter word. Let me think of common 6-letter words that relate to \"adjust\" and \"GATT\". \n",
      "\n",
      "Another angle: \"unethically\" might be \"unethic\" plus \"ally\". \"Unethic\" is not a common word, but \"unethically\" is. Maybe \"section of Gatt\" is \"GATT\" and \"unethically\" is \"unethic\" + \"ally\". But that doesn't seem to fit. Alternatively, \"unethically\" could be a synonym for \"unethically\", but that doesn't help. Maybe the answer is a combination of \"section of Gatt\" and \"unethically\", but I need to figure out how. \n",
      "\n",
      "Alternatively, perhaps \"adjust\" is the answer, and \"section of Gatt\" is a hint. Let me check if \"section of Gatt\" is \"GATT\" itself. \"GATT\" is four letters, but the answer needs to be 6 letters. Hmm. Wait, maybe the \"section of Gatt\" is \"GAT\" or \"ATT\"? If I take \"GATT\" and \"unethically\", maybe \"GATT\" plus \"unethically\"? That's a lot of letters. Wait, the answer is 6 letters. Maybe the answer is \"GATT\" plus something else, but that's not making sense. \n",
      "\n",
      "Alternatively, maybe \"adjust\" is the answer, and the clue is trying to hint at a part of \"GATT\" being adjusted. Let's think of \"GAT\" (from GATT) and adjust it. \"GAT\" is 3 letters. If I add something to make a 6-letter word. For example, \"GATT\" plus \"unethically\"? No, that's too long. Alternatively, maybe \"adjust\" is the answer, and \"GATT\" is part of the word. Let's see: \"GATT\" can be part of the word \"adjust\", but \"adjust\" is a 6-letter word. If I take \"GATT\" and add \"unethically\" which is not helpful. \n",
      "\n",
      "Alternatively, maybe \"section of Gatt\" is a part of the word \"GATT\". Let me think of the letters in \"GATT\": G, A, T, T. The word \"adjust\" has A, D, J, U, S, T. Maybe the letters from GATT are used in \"adjust\"? Let's check: A is there, T is there. But how about the other letters? D, J, U, S are not in GATT. So perhaps \"section of Gatt\" is part of \"GATT\", but not sure. Maybe \"section\" in the clue means a part of the word \"GATT\", which when combined with \"unethically\" gives the 6-letter word \"adjust\". \n",
      "\n",
      "Alternatively, \"adjust\" itself is the answer, and the clue is a hint. The \"section of Gatt\" is a part of \"GATT\" that's being adjusted. Let's think of \"GATT\" as G, A, T, T. If I adjust the T's to D, J, U, S, but that seems forced. Alternatively, \"adjust\" can be formed by taking some letters from \"GATT\" plus others. For example, \"GATT\" plus \"unethically\" would be too long. Wait, but \"unethically\" is 9 letters. No, the answer is 6 letters. \n",
      "\n",
      "Alternatively, \"adjust\" is the answer, and \"section of Gatt\" refers to a part of \"GATT\" that is part of \"adjust\". Let's look at \"adjust\": A, D, J, U, S, T. Maybe \"section of Gatt\" is \"ATT\", and adjusting it (maybe replacing T with something)? But \"ATT\" is three letters. Hmm. Maybe \"adjust\" is formed by \"A\" from GATT, and then some other letters. But that's not helpful. \n",
      "\n",
      "Alternatively, perhaps the answer is \"adjust\" and the clue is just a riddle where \"section of Gatt\" is a trick, and the answer is the word \"adjust\" because \"adjust\" is a 6-letter word that fits. Maybe the \"section of Gatt\" part is not needed, and the answer is just the word that matches the letters, which is \"adjust\". \n",
      "\n",
      "Alternatively, maybe \"Gatt unethically\" is a play on words. Let's think: \"unethically\" could be a typo or an anagram. For example, if \"unethically\" is an anagram of \"GATT\" with some letters, but it's 9 letters, which doesn't help. Alternatively, maybe \"unethically\" is a part of the word, like \"unethic\" + \"ally\", but again, not helpful.\n",
      "\n",
      "Wait, the clue is \"Adjust section of Gatt unethically\". If \"adjust\" is the answer, perhaps \"section of Gatt\" is a word that when added or combined with \"adjust\" gives the answer. But since the answer is 6 letters and \"adjust\" is 6 letters, maybe \"section\" is just a part of the word \"adjust\". Let me check the word \"adjust\" and see if \"section\" can be part of it. \"Adjust\" is spelled A-D-J-U-S-T. \"Section\" is S-E-C-T-I-O-N. There's an 'S' and 'T' in \"adjust\", which are in \"section\", but the letters don't align. \n",
      "\n",
      "Alternatively, maybe \"section of Gatt\" is \"GATT\" and when you adjust (modify) it somehow to get \"adjust\". Let me check the letters. \"GATT\" is G, A, T, T. To make \"adjust\" (A, D, J, U, S, T), the letters would need to come from \"GATT\" and some other letters. But \"GATT\" doesn't have D, J, U, S. So this approach might not work. \n",
      "\n",
      "Alternatively, maybe \"unethically\" is a synonym for \"unethically\" and is part of the wordplay. Let's think of words that are synonyms for \"unethically\", like \"unethically\" itself or \"unethic\" + \"ally\". But that's stretching. Maybe \"unethically\" is a word that when combined with another word gives the answer. For example, if the answer is \"adjust\", but I need to check the clue again.\n",
      "\n",
      "The clue is \"Adjust section of Gatt unethically\". Let's split it: \"Adjust\" (6 letters) plus \"section of Gatt\" plus \"unethically\" (but the answer is 6 letters). Maybe \"section of Gatt\" is a part of the word that is adjusted. Let's see: If I take \"GATT\" and adjust it to make \"adjust\". Let me think: GATT → adjust. Maybe replacing T with D, J, U, S. But that requires more letters. Alternatively, maybe \"section\" here is the part that's adjusted. If \"GATT\" is the section, then adjust it. But adjusting GATT (four letters) to 6 letters... Maybe adding letters? But the clue doesn't mention adding letters, just adjusting. \n",
      "\n",
      "Alternatively, maybe \"section of Gatt\" is \"GATT\", and \"unethically\" is a word that modifies it. For example, if \"unethically\" is \"unethic\" + \"ally\", maybe \"unethic\" and \"ally\" are parts of the answer, but the answer needs to be 6 letters. Let's see: \"unethic\" is 7 letters, \"ally\" is 4. That's too long. \n",
      "\n",
      "Alternatively, maybe \"section\" is a synonym for \"part\" and the word \"adjust\" is the answer. But \"adjust\" is the answer regardless. The clue might be a trick, where \"section of Gatt unethically\" is a way of saying that the answer is \"adjust\" because it matches the 6 letters, and \"section of Gatt\" is irrelevant. Maybe the \"section of Gatt\" is a red herring. So the answer is simply the 6-letter word \"adjust\".\n",
      "\n",
      "Alternatively, let's think of \"adjust\" as the answer and see if \"section of Gatt\" can fit into it. The word \"adjust\" has letters A, D, J, U, S, T. If \"section\" refers to a part of this, like a substring. Let me check if \"section\" is a substring. The word \"section\" is S-E-C-T-I-O-N. Not in \"adjust\". Maybe \"section of Gatt\" is part of \"adjust\" or another word. \n",
      "\n",
      "Wait, maybe \"GATT\" is a part of \"adjust\"? No, \"adjust\" doesn't contain \"GATT\". Maybe \"unethically\" is a part of \"adjust\"? Not really. Alternatively, \"unethically\" could be \"unethic\" + \"ally\". If \"unethic\" is a part of \"adjust\", but \"unethic\" is 7 letters. Hmm. \n",
      "\n",
      "Alternatively, maybe \"unethically\" is a typo, and it's supposed to be \"ethically\" or another word. But the clue is as written. Let's consider that \"adjust\" is the answer, and the clue is just a play on words or a riddle where the \"section of Gatt\" isn't directly part of the answer but is part of the wordplay. \n",
      "\n",
      "Therefore, after going through various possibilities, I think the answer is \"Adjust\", as it's the 6-letter word that fits the clue. The \"section of Gatt unethically\" might be a red herring or part of a wordplay that I'm not seeing, but given that the answer needs to be 6 letters, and \"adjust\" fits, I'll go with that.\n",
      "</think>\n",
      "reasoning process here\n",
      "To solve the crossword clue \"Adjust section of Gatt unethically\" (6 letters), we start by identifying the main word to be solved, which is \"Adjust\" (6 letters). The clue provides additional components: \"section of Gatt unethically.\" \n",
      "\n",
      "Breaking down the clue:  \n",
      "1. \"Adjust\" is the core answer, as it fits the letter count.  \n",
      "2. \"Section of Gatt\" refers to parts of the word \"GATT.\"  \n",
      "3. \"Unethically\" might be a wordplay element (an anagram, hidden word, etc.), but \"unethically\" is a 9-letter word, which doesn't fit.  \n",
      "\n",
      "Considering the letters in \"Adjust\" (A, D, J, U, S, T) and \"GATT\" (G, A, T, T), the overlap is limited. While \"GATT\" contains A and T, other letters (D, J, U, S) are not present, making it challenging to form \"Adjust\" directly from \"GATT.\"  \n",
      "\n",
      "Given the clue's structure and the requirement for a 6-letter answer, the most plausible solution is \"Adjust,\" despite the apparent \"section of Gatt\" part. The extra words may serve as a distractor or part of a more complex wordplay not immediately apparent.  \n",
      "\n",
      "answer here  \n",
      "\\boxed{Adjust}<|im_end|>\n",
      "--------------------------------------------------------------------------------\n",
      "Token count: 3037\n"
     ]
    }
   ],
   "source": [
    "show_trace(DB_FILE_MERGED, 448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd552834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fffd_trace_indices(db_file: str, table: str = 'dataset') -> list[int]:\n",
    "    \"\"\"\n",
    "    Identify indices of rows in the 'trace' column that contain the Unicode replacement character � (U+FFFD).\n",
    "\n",
    "    Args:\n",
    "        db_file: Path to the SQLite database file.\n",
    "        table: Name of the table (default: 'dataset').\n",
    "\n",
    "    Returns:\n",
    "        List of row indices with traces containing � (U+FFFD).\n",
    "    \"\"\"\n",
    "    import sqlite3\n",
    "    import pandas as pd\n",
    "\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT trace FROM {table}\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    fffd_indices = [\n",
    "        idx for idx, trace in enumerate(df['trace']) \n",
    "        if trace is not None and '�' in str(trace)\n",
    "    ]\n",
    "\n",
    "    return fffd_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7a75aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "[2, 18, 36, 50, 59, 61, 71, 72, 76, 77, 78, 81, 91, 100, 102, 113, 120, 121, 131, 140, 144, 145, 159, 181, 185, 188, 194, 195, 197, 204, 211, 212, 273, 280, 305, 321, 325, 337, 359, 361, 369, 372, 379, 385, 394, 402, 418, 423, 428, 429, 430, 467, 474, 480, 484, 496, 502, 506, 511, 514, 522, 527, 531, 534, 536, 560, 562, 572, 578, 582, 585, 589, 598, 602, 607, 615, 618, 622, 627, 631, 636, 669, 673, 684, 686, 696, 720, 722, 723, 731, 735, 736, 739, 750, 752, 776, 778, 785, 793, 806, 811, 821, 824, 825, 830, 841, 855, 867, 875, 891, 926, 932]\n"
     ]
    }
   ],
   "source": [
    "DB_FILE = 'dataset_4qwen3_250622a_merged.db'\n",
    "\n",
    "contaminated_trace_indices = find_fffd_trace_indices(DB_FILE)\n",
    "\n",
    "print(len(contaminated_trace_indices))\n",
    "print(contaminated_trace_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90485acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "DB_FILE = 'dataset_4qwen3_250624a_merged.db'\n",
    "\n",
    "contaminated_trace_indices = find_fffd_trace_indices(DB_FILE)\n",
    "\n",
    "print(len(contaminated_trace_indices))\n",
    "print(contaminated_trace_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def print_index_groups(indices: list[int], n: int) -> None:\n",
    "    \"\"\"\n",
    "    Partition `indices` into `n` groups (last group may be shorter)\n",
    "    and print each group as a comma-separated string with no spaces.\n",
    "    \"\"\"\n",
    "    m = len(indices)                          # total indices\n",
    "    s = math.ceil(m / n)                      # group size  s = ⌈ m ⁄ n ⌉\n",
    "    \n",
    "    for k in range(n):\n",
    "        start, end = k * s, (k + 1) * s       # slice bounds\n",
    "        group = indices[start:end]\n",
    "        if group:                             # ignore empty tail groups\n",
    "            print(\",\".join(map(str, group)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e82980f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,18,36,50,59,61,71,72,76,77,78,81,91,100,102,113,120,121,131,140,144,145,159,181,185,188,194,195\n",
      "197,204,211,212,273,280,305,321,325,337,359,361,369,372,379,385,394,402,418,423,428,429,430,467,474,480,484,496\n",
      "502,506,511,514,522,527,531,534,536,560,562,572,578,582,585,589,598,602,607,615,618,622,627,631,636,669,673,684\n",
      "686,696,720,722,723,731,735,736,750,752,776,778,785,793,806,811,821,824,825,830,841,855,867,875,891,926,932\n"
     ]
    }
   ],
   "source": [
    "print_index_groups(contaminated_trace_indices, n=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def clear_fffd_traces(db_file: str,\n",
    "                      row_indices: list[int],\n",
    "                      table: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Set trace = '' for all rows in the table whose pandas index is in `row_indices`.\n",
    "    Assumes the table’s implicit ROWID order matches the DataFrame index.\n",
    "    \"\"\"\n",
    "    if not row_indices:\n",
    "        return\n",
    "\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    for idx in row_indices:\n",
    "        cur.execute(f\"UPDATE {table} SET trace = '' WHERE rowid = ?\", (idx + 1,))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18498e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"dataset_4qwen3.db\"\n",
    "\n",
    "clear_fffd_traces(DB_FILE, contaminated_trace_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d46cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from typing import Sequence\n",
    "\n",
    "def clear_trace_and_teacher(db_path: str,\n",
    "                            row_indices: Sequence[int],\n",
    "                            table: str = \"dataset\") -> None:\n",
    "    \"\"\"\n",
    "    Blank out both the `trace` **and** `teacher` columns for every row whose\n",
    "    *DataFrame* index appears in `row_indices`.\n",
    "\n",
    "    Assumption  \n",
    "    \\( \\text{ROWID}_{\\text{SQLite}} = \\text{pandas index} + 1 \\)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db_path : str\n",
    "        Path to the SQLite database file.\n",
    "    row_indices : Sequence[int]\n",
    "        Zero-based pandas indices to wipe.\n",
    "    table : str, optional\n",
    "        Target table name (default ``'dataset'``).\n",
    "    \"\"\"\n",
    "    if not row_indices:            # ∅ → no work\n",
    "        return\n",
    "\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Convert pandas indices → SQLite rowid (1-based)\n",
    "        params = [(idx + 1,) for idx in row_indices]\n",
    "\n",
    "        # Execute once per row via executemany:\n",
    "        cur.executemany(\n",
    "            f\"UPDATE {table} SET trace = '', teacher = '' WHERE rowid = ?\",\n",
    "            params\n",
    "        )\n",
    "        conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78947f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"dataset_4qwen3.db\"\n",
    "\n",
    "clear_trace_and_teacher(DB_FILE, [35,70,103,137,365,807])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3b5e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def add_teacher_column(db_file: str, table_name: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Add a 'teacher' column to the database. For rows where 'trace' is not empty,\n",
    "    set 'teacher' to 's1.1-7B'. For empty trace rows, set 'teacher' to empty string.\n",
    "    \"\"\"\n",
    "    # Load the full table\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Create teacher column if it doesn't already exist\n",
    "    if 'teacher' not in df.columns:\n",
    "        df['teacher'] = ''\n",
    "    \n",
    "    # Check if trace is not empty/whitespace\n",
    "    non_empty_mask = df['trace'].str.strip() != ''\n",
    "    \n",
    "    # Set 'teacher' to 's1.1-7B' where trace is not empty\n",
    "    df.loc[non_empty_mask, 'teacher'] = 's1.1-7B'\n",
    "    \n",
    "    # Write back to database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df.to_sql(table_name, conn, index=False, if_exists='replace')\n",
    "    conn.close()\n",
    "    \n",
    "    # Report results\n",
    "    total_rows = len(df)\n",
    "    teacher_assigned = non_empty_mask.sum()\n",
    "    \n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Rows with teacher assigned: {teacher_assigned}\")\n",
    "    print(f\"Rows with empty teacher: {total_rows - teacher_assigned}\")\n",
    "\n",
    "def analyze_db_with_teacher(db_file: str, table_name: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of the database including trace and teacher columns.\n",
    "    Shows statistics for all teacher types and their distribution.\n",
    "    \"\"\"\n",
    "    # Load table\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Check if teacher column exists\n",
    "    if 'teacher' not in df.columns:\n",
    "        print(\"No 'teacher' column found in the database.\")\n",
    "        return\n",
    "    \n",
    "    # Trace metrics - no entries are null, only empty or non-empty\n",
    "    trace_empty_rows = (df['trace'].str.strip() == '').sum()\n",
    "    trace_non_empty = df['trace'].str.strip() != ''\n",
    "    \n",
    "    # Teacher metrics - no entries are null, only empty or non-empty\n",
    "    teacher_empty_rows = (df['teacher'].str.strip() == '').sum()\n",
    "    teacher_non_empty = df['teacher'].str.strip() != ''\n",
    "    \n",
    "    # Get unique teacher types (excluding empty)\n",
    "    unique_teachers = df[teacher_non_empty]['teacher'].unique()\n",
    "    teacher_counts = df['teacher'].value_counts(dropna=False)\n",
    "    \n",
    "    print(\"Trace column:\")\n",
    "    print(f\"    Empty rows: {trace_empty_rows}\")\n",
    "    print(f\"    Non-empty rows: {trace_non_empty.sum()}\")\n",
    "    \n",
    "    print(f\"\\nTeacher column:\")\n",
    "    print(f\"    Empty rows: {teacher_empty_rows}\")\n",
    "    print(f\"    Non-empty rows: {teacher_non_empty.sum()}\")\n",
    "    print(f\"    Unique teacher types: {len(unique_teachers)}\")\n",
    "    \n",
    "    # Show counts for each teacher type\n",
    "    for teacher_type in sorted(unique_teachers):\n",
    "        count = teacher_counts.get(teacher_type, 0)\n",
    "        print(f\"    '{teacher_type}': {count}\")\n",
    "    \n",
    "    # Consistency checks\n",
    "    trace_teacher_mismatch = trace_non_empty & ~teacher_non_empty\n",
    "    teacher_trace_mismatch = teacher_non_empty & ~trace_non_empty\n",
    "    \n",
    "    print(f\"\\nConsistency checks:\")\n",
    "    print(f\"    Rows with non-empty trace but empty teacher: {trace_teacher_mismatch.sum()}\")\n",
    "    print(f\"    Rows with non-empty teacher but empty trace: {teacher_trace_mismatch.sum()}\")\n",
    "\n",
    "def update_teacher_column(db_file: str, teacher_model: str, table_name: str = 'dataset') -> None:\n",
    "    \"\"\"\n",
    "    Update the 'teacher' column for rows where trace is non-empty but teacher is empty.\n",
    "    Sets teacher to the specified teacher_model for these rows.\n",
    "    \n",
    "    Args:\n",
    "        db_file: Path to the SQLite database file\n",
    "        teacher_model: The teacher model string to assign (e.g., 's1.1-14B')\n",
    "        table_name: Name of the table in the database\n",
    "    \"\"\"\n",
    "    # Load the full table\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Check if teacher column exists\n",
    "    if 'teacher' not in df.columns:\n",
    "        print(\"Error: 'teacher' column not found in the database.\")\n",
    "        print(\"Please run add_teacher_column() first.\")\n",
    "        return\n",
    "    \n",
    "    # Find rows where trace is non-empty but teacher is empty\n",
    "    trace_non_empty = df['trace'].str.strip() != ''\n",
    "    teacher_empty = df['teacher'].str.strip() == ''\n",
    "    \n",
    "    # Mask for rows to update: trace non-empty AND teacher empty\n",
    "    update_mask = trace_non_empty & teacher_empty\n",
    "    \n",
    "    # Count rows before update for reporting\n",
    "    rows_to_update = update_mask.sum()\n",
    "    \n",
    "    if rows_to_update == 0:\n",
    "        print(\"No rows found where trace is non-empty and teacher is empty.\")\n",
    "        return\n",
    "    \n",
    "    # Update teacher column for matching rows\n",
    "    df.loc[update_mask, 'teacher'] = teacher_model\n",
    "    \n",
    "    # Write back to database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df.to_sql(table_name, conn, index=False, if_exists='replace')\n",
    "    conn.close()\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"Updated {rows_to_update} rows with teacher model: '{teacher_model}'\")\n",
    "    \n",
    "    # Show indices of updated rows\n",
    "    updated_indices = df[update_mask].index.tolist()\n",
    "    print(f\"Updated row indices: {updated_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc753cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = 'dataset_4qwen3_250624a_merged.db'\n",
    "\n",
    "# Add teacher column to your database\n",
    "add_teacher_column(DB_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e56fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = 'dataset_4qwen3_250624a_merged.db'\n",
    "\n",
    "# Update teacher column for rows with non-empty trace but empty teacher\n",
    "update_teacher_column(DB_FILE, 's1.1-7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1997d820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1817\n",
      "Rows with teacher assigned: 181\n",
      "Rows with empty teacher: 1636\n",
      "Trace column:\n",
      "    Empty rows: 1636\n",
      "    Non-empty rows: 181\n",
      "\n",
      "Teacher column:\n",
      "    Empty rows: 1636\n",
      "    Non-empty rows: 181\n",
      "    Unique teacher types: 1\n",
      "    's1.1-7B': 181\n",
      "\n",
      "Consistency checks:\n",
      "    Rows with non-empty trace but empty teacher: 0\n",
      "    Rows with non-empty teacher but empty trace: 0\n"
     ]
    }
   ],
   "source": [
    "DB_FILE = 'dataset_4qwen3_250624a_merged.db'\n",
    "\n",
    "# Analyze the results\n",
    "analyze_db_with_teacher(DB_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import vllm\n",
    "print(vllm.__version__)\n",
    "\n",
    "# 2.6.0+cu124\n",
    "# 0.8.5.post1\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b27137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "DB_FILE = 'dataset_4qwen3.db'\n",
    "\n",
    "# Read the SQLite database back into a DataFrame\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "df = pd.read_sql('SELECT * FROM dataset', conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "null_count = df['trace'].isnull().sum()\n",
    "empty_string_count = (df['trace'] == '').sum()\n",
    "whitespace_only_count = df['trace'].str.strip().eq('').sum() if df['trace'].dtype == 'object' else 0\n",
    "total_empty = df['trace'].isnull().sum() + (df['trace'].str.strip() == '').sum()\n",
    "\n",
    "print(f\"Null/NaN values: {null_count}\")\n",
    "print(f\"Empty strings: {empty_string_count}\")\n",
    "print(f\"Whitespace-only strings: {whitespace_only_count}\")\n",
    "print(f\"Total empty rows: {total_empty}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Probability Analyzer - Check reasoning trace token probabilities with small model\n",
    "#\n",
    "# Analyzes an existing reasoning trace to check the probability of each token\n",
    "# according to a small model. Marks tokens with probability below threshold with asterisk.\n",
    "\n",
    "# --------------------------- imports ---------------------------------------\n",
    "import os, html, uuid, asyncio, contextlib, nest_asyncio, logging\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "from vllm import TokensPrompt\n",
    "from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "from vllm.sampling_params import SamplingParams, RequestOutputKind\n",
    "\n",
    "nest_asyncio.apply()\n",
    "torch.set_grad_enabled(False)\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "# --------------------------- configuration ---------------------------------\n",
    "SMALL_GPU_INDEX = \"0\"\n",
    "SMALL_MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "SMALL_TEMPERATURE = 0.7\n",
    "MAX_SEQ_LEN = 8192\n",
    "PROB_THRESHOLD = 0.01\n",
    "\n",
    "# ---------------- utility: temporarily set visible GPUs --------------------\n",
    "@contextlib.contextmanager\n",
    "def visible_gpus(devices: str):\n",
    "    original = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "    print(f\"\\nCUDA_VISIBLE_DEVICES = {devices}\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = original\n",
    "\n",
    "# --------------------------- engine setup ----------------------------------\n",
    "async def setup_small_engine():\n",
    "    global small_engine, small_tokenizer, small_vocab_size\n",
    "    \n",
    "    small_checkpoint = snapshot_download(SMALL_MODEL_NAME)\n",
    "\n",
    "    with visible_gpus(SMALL_GPU_INDEX):\n",
    "        print(\"torch sees\", torch.cuda.device_count(), \"GPU(s)\")              \n",
    "        small_engine = AsyncLLMEngine.from_engine_args(\n",
    "            AsyncEngineArgs(model=small_checkpoint, \n",
    "                            tensor_parallel_size=1,\n",
    "                            max_model_len=MAX_SEQ_LEN, \n",
    "                            gpu_memory_utilization=0.20,\n",
    "                            dtype=\"bfloat16\"),\n",
    "            start_engine_loop=True)\n",
    "        \n",
    "        small_tokenizer = await small_engine.get_tokenizer()\n",
    "\n",
    "    # Get model config using async method\n",
    "    small_model_config = await small_engine.get_model_config()\n",
    "    small_vocab_size = small_model_config.get_vocab_size()\n",
    "    \n",
    "    print(f\"Small vocab size: {small_vocab_size}\")\n",
    "\n",
    "# --------------------------- sampling params -------------------------------\n",
    "small_sampling_params = SamplingParams(\n",
    "    max_tokens=1,\n",
    "    temperature=SMALL_TEMPERATURE,\n",
    "    top_p=0.95, \n",
    "    logprobs=20,  # vLLM's max allowed logprob size\n",
    "    output_kind=RequestOutputKind.DELTA,\n",
    ")\n",
    "\n",
    "# -------------------------- helper functions -------------------------------\n",
    "def html_heatmap(token_ids, probabilities, tokenizer):\n",
    "    \"\"\"Create heatmap visualization of token probabilities\"\"\"\n",
    "    \n",
    "    def colour(probability):\n",
    "        if probability < PROB_THRESHOLD:\n",
    "            return \"rgb(255,0,0)\"  # Red for below threshold\n",
    "        else:\n",
    "            return \"rgb(0,0,0)\"    # Black for above threshold\n",
    "    \n",
    "    spans = []\n",
    "    \n",
    "    # Find token groups that form complete characters\n",
    "    token_groups = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(token_ids):\n",
    "        # Start with current token\n",
    "        group_start = i\n",
    "        group_end = i + 1\n",
    "        \n",
    "        # Expand the group until we have a valid UTF-8 sequence\n",
    "        while group_end <= len(token_ids):\n",
    "            # Try decoding the current group\n",
    "            group_text = tokenizer.decode(token_ids[group_start:group_end])\n",
    "            \n",
    "            if '\\ufffd' not in group_text:\n",
    "                # Valid decode, but check if we should include more tokens\n",
    "                if group_end < len(token_ids):\n",
    "                    # Check if adding the next token changes the decode\n",
    "                    extended_text = tokenizer.decode(token_ids[group_start:group_end+1])\n",
    "                    current_plus_next = group_text + tokenizer.decode([token_ids[group_end]])\n",
    "                    \n",
    "                    if extended_text != current_plus_next or '\\ufffd' in current_plus_next:\n",
    "                        # Next token is part of this character, continue\n",
    "                        group_end += 1\n",
    "                        continue\n",
    "                \n",
    "                # We have a complete group\n",
    "                break\n",
    "            else:\n",
    "                # Invalid decode, need more tokens\n",
    "                group_end += 1\n",
    "                if group_end > len(token_ids):\n",
    "                    # Reached end with incomplete sequence\n",
    "                    group_end = len(token_ids)\n",
    "                    break\n",
    "        \n",
    "        # Store the group\n",
    "        token_groups.append((group_start, group_end))\n",
    "        i = group_end\n",
    "    \n",
    "    # Now render each group\n",
    "    for group_start, group_end in token_groups:\n",
    "        # Decode the group\n",
    "        text = tokenizer.decode(token_ids[group_start:group_end])\n",
    "        \n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        escaped = html.escape(text).replace(\" \", \"&nbsp;\")\n",
    "        \n",
    "        # Check if any token in this group has low probability\n",
    "        any_low_prob = False\n",
    "        min_prob = 1.0\n",
    "        \n",
    "        for token_idx in range(group_start, group_end):\n",
    "            if token_idx < len(probabilities):\n",
    "                prob = probabilities[token_idx]\n",
    "                min_prob = min(min_prob, prob)\n",
    "                any_low_prob = any_low_prob or (prob < PROB_THRESHOLD)\n",
    "        \n",
    "        style = f\"color:{colour(min_prob)};\"\n",
    "        if any_low_prob:\n",
    "            style += \" text-decoration:underline;\"\n",
    "        spans.append(f\"<span style='{style}'>{escaped}</span>\")\n",
    "    \n",
    "    return HTML(\"<pre style='white-space:pre-wrap; line-height:1.45; \"\n",
    "                \"font-family:inherit; background:#fff; padding:8px; \"\n",
    "                \"border:1px solid #ddd;'>\" + \"\".join(spans) + \"</pre>\")\n",
    "\n",
    "# ------------------------- core analysis loop ------------------------------\n",
    "async def one_step_analyze(context_ids):\n",
    "    \"\"\"Get probability distribution for next token given context\"\"\"\n",
    "    tokens_prompt = TokensPrompt(prompt_token_ids=context_ids)\n",
    "    generator = small_engine.generate(tokens_prompt, small_sampling_params, request_id=str(uuid.uuid4()))\n",
    "    return (await anext(generator)).outputs[0]\n",
    "\n",
    "async def analyze_trace(prompt_part: str, trace_part: str):\n",
    "    \"\"\"Analyze each token in the trace part for its probability\"\"\"\n",
    "    \n",
    "    # Tokenize the prompt and trace separately\n",
    "    prompt_token_ids = small_tokenizer.encode(prompt_part)\n",
    "    trace_token_ids = small_tokenizer.encode(trace_part)\n",
    "    \n",
    "    print(f\"Prompt tokens: {len(prompt_token_ids)}\")\n",
    "    print(f\"Trace tokens: {len(trace_token_ids)}\")\n",
    "    print(f\"Analyzing {len(trace_token_ids)} trace tokens...\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Step\\tProb\\tTok_ID\\tTok_Txt\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    probabilities = []\n",
    "    records = []\n",
    "    \n",
    "    # For each token in the trace, check its probability\n",
    "    for step_index in range(len(trace_token_ids)):\n",
    "        # Context is: prompt + trace tokens up to this position\n",
    "        context_ids = prompt_token_ids + trace_token_ids[:step_index]\n",
    "        \n",
    "        # The token we're analyzing\n",
    "        actual_token_id = trace_token_ids[step_index]\n",
    "        actual_token_text = small_tokenizer.decode([actual_token_id])\n",
    "        \n",
    "        # Get model's probability distribution for next token\n",
    "        output = await one_step_analyze(context_ids)\n",
    "        \n",
    "        # Extract probabilities from model output\n",
    "        logprobs_dict = output.logprobs[0]\n",
    "        \n",
    "        # Get probability of the actual token that was used\n",
    "        if actual_token_id in logprobs_dict:\n",
    "            actual_prob = torch.exp(torch.tensor(logprobs_dict[actual_token_id].logprob)).item()\n",
    "        else:\n",
    "            actual_prob = 0.0  # Token not in top predictions\n",
    "        \n",
    "        probabilities.append(actual_prob)\n",
    "        \n",
    "        # Check if probability is below threshold\n",
    "        low_prob = actual_prob < PROB_THRESHOLD\n",
    "        \n",
    "        record = {\n",
    "            'step': step_index + 1,\n",
    "            'token_id': actual_token_id,\n",
    "            'token_text': actual_token_text,\n",
    "            'probability': actual_prob,\n",
    "            'low_prob': low_prob\n",
    "        }\n",
    "        records.append(record)\n",
    "        \n",
    "        print(f\"{step_index + 1:4d}{'*' if low_prob else ' '}\\t\"\n",
    "              f\"{actual_prob:.4f}\\t\"\n",
    "              f\"{actual_token_id}\\t'{actual_token_text}'\",\n",
    "              flush=True)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Display the heatmap for trace tokens only\n",
    "    display(html_heatmap(trace_token_ids, probabilities, small_tokenizer))\n",
    "    \n",
    "    # Print only tokens below probability threshold\n",
    "    low_prob_records = [record for record in records if record['low_prob']]\n",
    "    if low_prob_records:\n",
    "        print(f\"\\nTokens below {PROB_THRESHOLD} probability threshold:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Step\\tProb\\tTok_ID\\tTok_Txt\")\n",
    "        print(\"-\" * 80)\n",
    "        for record in low_prob_records:\n",
    "            print(f\"{record['step']:4d}*\\t\"\n",
    "                  f\"{record['probability']:.4f}\\t\"\n",
    "                  f\"{record['token_id']}\\t'{record['token_text']}'\")\n",
    "        print(\"-\" * 80)\n",
    "    else:\n",
    "        print(f\"\\nNo tokens below {PROB_THRESHOLD} probability threshold found.\")\n",
    "    \n",
    "    # Statistics\n",
    "    low_prob_count = sum(record['low_prob'] for record in records)\n",
    "    print(f\"\\nLow probability tokens (< {PROB_THRESHOLD}): {low_prob_count}/{len(records)} \"\n",
    "          f\"({low_prob_count/len(records)*100:.2f}%)\")\n",
    "    \n",
    "    return records\n",
    "\n",
    "# ---------------------- high-level convenience -----------------------------\n",
    "async def run_analysis(prompt_part: str, trace_part: str):\n",
    "    \"\"\"Main function to run the analysis\"\"\"\n",
    "    return await analyze_trace(prompt_part, trace_part)\n",
    "\n",
    "# ------------------------ fire up the engine ------------------------------\n",
    "await setup_small_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- example usage ---------------------------------\n",
    "idx = 0  # Set the dataframe index you want to analyze\n",
    "\n",
    "# Get question and trace from dataframe\n",
    "question = df['question'].iloc[idx]\n",
    "trace = df['trace'].iloc[idx]\n",
    "\n",
    "# Create the prompt part (ends with \"<think>\")\n",
    "prompt_part = f\"\"\"A conversation between User and Assistant. The User asks a question, and the Assistant responds in two clearly defined sections: 1. Reasoning Process - A step-by-step, logical exploration and analysis of the problem, enclosed within <think> and </think> tags. 2. Answer - A direct and concise response based on the reasoning process, with the final answer enclosed within \\\\boxed{{}}. For example, \n",
    "<think>\n",
    "reasoning process here\n",
    "</think>\n",
    "answer here\n",
    "\\\\boxed{{final answer here}}\n",
    "\n",
    "Now, continue the actual conversation below.\n",
    "User: {question}\n",
    "Assistant:\n",
    "<think>\"\"\"\n",
    "\n",
    "# The trace part is just the trace\n",
    "trace_part = trace\n",
    "\n",
    "# Run the analysis\n",
    "records = await run_analysis(prompt_part, trace_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6daf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def non_empty_trace_indices(db_path: str, table: str = \"dataset\", col: str = \"trace\") -> list[int]:\n",
    "    \"\"\"\n",
    "    Return a list of row indices (SQLite `rowid`s) whose **trace** column\n",
    "    is neither NULL nor an empty / whitespace-only string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    db_path : str\n",
    "        Path to the SQLite database file.\n",
    "    table   : str, optional\n",
    "        Table name holding the data (default ``\"dataset\"``).\n",
    "    col     : str, optional\n",
    "        Column holding the trace text (default ``\"trace\"``).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[int]\n",
    "        SQLite rowids that satisfy the non-empty condition.\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        # Pull only rowid + trace to keep it light\n",
    "        df = pd.read_sql(f\"SELECT rowid AS idx, {col} FROM {table}\", conn)\n",
    "\n",
    "    mask = df[col].notna() & df[col].astype(str).str.strip().ne(\"\")\n",
    "    idx = df.loc[mask, \"idx\"]\n",
    "\n",
    "    return (idx - 1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ffca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FILE = \"dataset_4qwen3.db\"\n",
    "\n",
    "indices = non_empty_trace_indices(DB_FILE)\n",
    "print(len(indices))\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af700086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- batch analysis wrapper -----------------------\n",
    "async def analyze_batch_low_prob_only(indices: list):\n",
    "    \"\"\"Analyze multiple traces and only print tokens below probability threshold\"\"\"\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        print(f\"Entry {idx}\")\n",
    "        \n",
    "        # Get question and trace from dataframe\n",
    "        question = df['question'].iloc[idx]\n",
    "        trace = df['trace'].iloc[idx]\n",
    "        \n",
    "        # Create the prompt part\n",
    "        prompt_part = f\"\"\"A conversation between User and Assistant. The User asks a question, and the Assistant responds in two clearly defined sections: 1. Reasoning Process - A step-by-step, logical exploration and analysis of the problem, enclosed within <think> and </think> tags. 2. Answer - A direct and concise response based on the reasoning process, with the final answer enclosed within \\\\boxed{{}}. For example, \n",
    "<think>\n",
    "reasoning process here\n",
    "</think>\n",
    "answer here\n",
    "\\\\boxed{{final answer here}}\n",
    "\n",
    "Now, continue the actual conversation below.\n",
    "User: {question}\n",
    "Assistant:\n",
    "<think>\"\"\"\n",
    "        \n",
    "        # The trace part is just the trace\n",
    "        trace_part = trace\n",
    "        \n",
    "        # Tokenize the prompt and trace separately\n",
    "        prompt_token_ids = small_tokenizer.encode(prompt_part)\n",
    "        trace_token_ids = small_tokenizer.encode(trace_part)\n",
    "        \n",
    "        probabilities = []\n",
    "        low_prob_records = []\n",
    "        \n",
    "        # For each token in the trace, check its probability\n",
    "        for step_index in range(len(trace_token_ids)):\n",
    "            # Context is: prompt + trace tokens up to this position\n",
    "            context_ids = prompt_token_ids + trace_token_ids[:step_index]\n",
    "            \n",
    "            # The token we're analyzing\n",
    "            actual_token_id = trace_token_ids[step_index]\n",
    "            actual_token_text = small_tokenizer.decode([actual_token_id])\n",
    "            \n",
    "            # Get model's probability distribution for next token\n",
    "            output = await one_step_analyze(context_ids)\n",
    "            \n",
    "            # Extract probabilities from model output\n",
    "            logprobs_dict = output.logprobs[0]\n",
    "            \n",
    "            # Get probability of the actual token that was used\n",
    "            if actual_token_id in logprobs_dict:\n",
    "                actual_prob = torch.exp(torch.tensor(logprobs_dict[actual_token_id].logprob)).item()\n",
    "            else:\n",
    "                actual_prob = 0.0  # Token not in top predictions\n",
    "            \n",
    "            probabilities.append(actual_prob)\n",
    "            \n",
    "            # Check if probability is below threshold\n",
    "            low_prob = actual_prob < PROB_THRESHOLD\n",
    "            \n",
    "            if low_prob:\n",
    "                record = {\n",
    "                    'step': step_index + 1,\n",
    "                    'token_id': actual_token_id,\n",
    "                    'token_text': actual_token_text,\n",
    "                    'probability': actual_prob,\n",
    "                    'low_prob': low_prob\n",
    "                }\n",
    "                low_prob_records.append(record)\n",
    "        \n",
    "        # Print only tokens below probability threshold\n",
    "        if low_prob_records:\n",
    "            print(\"\\nStep\\tProb\\tTok_ID\\tTok_Txt\")\n",
    "            for record in low_prob_records:\n",
    "                print(f\"{record['step']:4d}*\\t\"\n",
    "                      f\"{record['probability']:.4f}\\t\"\n",
    "                      f\"{record['token_id']}\\t'{record['token_text']}'\")\n",
    "            # Statistics\n",
    "            total_tokens = len(trace_token_ids)\n",
    "            low_prob_count = len(low_prob_records)\n",
    "            print(f\"\\nLow probability tokens (< {PROB_THRESHOLD}): {low_prob_count}/{total_tokens} \"\n",
    "                f\"({low_prob_count/total_tokens*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"\\nNo tokens below {PROB_THRESHOLD} probability threshold found.\")\n",
    "        \n",
    "# --------------------------- batch usage example --------------------------\n",
    "await analyze_batch_low_prob_only(indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
