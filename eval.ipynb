{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220d06e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-30 11:21:36 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "\n",
    "# !pip install -q torch==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip install -q vllm==0.8.5.post1\n",
    "# !pip install --upgrade fsspec datasets\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random, os, json, re, torch\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# vLLM imports\n",
    "import html, uuid, asyncio, contextlib, nest_asyncio, logging\n",
    "from IPython.display import HTML, display\n",
    "from huggingface_hub import snapshot_download\n",
    "from vllm import TokensPrompt\n",
    "from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "from vllm.sampling_params import SamplingParams, RequestOutputKind\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "torch.set_grad_enabled(False)\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "NUM_RUNS = 1\n",
    "# NUM_RUNS = 16\n",
    "BASE_SEED = 42\n",
    "\n",
    "MODEL_REPO_NAME = \"Qwen/Qwen3-0.6B\" \n",
    "# MODEL_REPO_NAME = \"jaeh8nkim/s1-slth-qwen3-0.6b\"\n",
    "\n",
    "MODEL_LOCAL_PATH = \"vanilla-qwen3-0.6b-local\"\n",
    "# MODEL_LOCAL_PATH = \"s1-slth-qwen3-0.6b-local\"\n",
    "\n",
    "SMALL_GPU_INDEX = \"3\"\n",
    "\n",
    "SMALL_TEMPERATURE = 0.7\n",
    "# MAX_SEQ_LEN = 8192\n",
    "MAX_SEQ_LEN = 16384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model Download\n",
    "\n",
    "def download_model_locally(repo_name, local_path):\n",
    "    \"\"\"Download model from HuggingFace and save locally\"\"\"\n",
    "    print(f\"üì• Downloading model from {repo_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if model already exists locally\n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"‚úÖ Model already exists at {local_path}\")\n",
    "            return local_path\n",
    "        \n",
    "        # Download model using snapshot_download (same as vLLM uses)\n",
    "        checkpoint_path = snapshot_download(repo_name)\n",
    "        \n",
    "        # Create local directory\n",
    "        os.makedirs(local_path, exist_ok=True)\n",
    "        \n",
    "        # Copy all files from checkpoint to local path\n",
    "        import shutil\n",
    "        shutil.copytree(checkpoint_path, local_path, dirs_exist_ok=True)\n",
    "        \n",
    "        print(f\"‚úÖ Model downloaded and saved to {local_path}\")\n",
    "        return local_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download the model locally\n",
    "print(\"üöÄ Downloading model locally...\")\n",
    "model_path = download_model_locally(MODEL_REPO_NAME, MODEL_LOCAL_PATH)\n",
    "\n",
    "if model_path is None:\n",
    "    raise RuntimeError(\"Failed to download model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc0df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up engine with local model: vanilla-qwen3-0.6b-local\n",
      "\n",
      "CUDA_VISIBLE_DEVICES = 3\n",
      "torch sees 1 GPU(s)\n",
      "WARNING 06-30 11:21:54 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f83afe5b1d0>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa691503fca40c1b88676cc7b7c9284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 151936\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: vLLM Engine Setup\n",
    "\n",
    "# ---------------- utility: temporarily set visible GPUs --------------------\n",
    "@contextlib.contextmanager\n",
    "def visible_gpus(devices: str):\n",
    "    original = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "    print(f\"\\nCUDA_VISIBLE_DEVICES = {devices}\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = original\n",
    "\n",
    "# --------------------------- engine setup ----------------------------------\n",
    "async def setup_engine():\n",
    "    global engine, tokenizer, vocab_size\n",
    "    \n",
    "    # Use the locally downloaded model\n",
    "    print(f\"Setting up engine with local model: {MODEL_LOCAL_PATH}\")\n",
    "\n",
    "    with visible_gpus(SMALL_GPU_INDEX):\n",
    "        print(\"torch sees\", torch.cuda.device_count(), \"GPU(s)\")              \n",
    "        engine = AsyncLLMEngine.from_engine_args(\n",
    "            AsyncEngineArgs(model=MODEL_LOCAL_PATH,  # Use local path instead of checkpoint\n",
    "                            tensor_parallel_size=1,\n",
    "                            max_model_len=MAX_SEQ_LEN, \n",
    "                            gpu_memory_utilization=0.90,\n",
    "                            dtype=\"bfloat16\"),\n",
    "            start_engine_loop=True)\n",
    "        \n",
    "        tokenizer = await engine.get_tokenizer()\n",
    "\n",
    "    # Get model config using async method\n",
    "    model_config = await engine.get_model_config()\n",
    "    vocab_size = model_config.get_vocab_size()\n",
    "    \n",
    "    print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# --------------------------- sampling params -------------------------------\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=MAX_SEQ_LEN,\n",
    "    temperature=SMALL_TEMPERATURE,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# Initialize the engine\n",
    "await setup_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11cea43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Answer extraction and grading functions\n",
    "\n",
    "def extract_boxed_answer(records, tokenizer):\n",
    "    \"\"\"Extract the last \\\\boxed{} answer between tokens 151668 and 151645\"\"\"\n",
    "    token_ids = [record['token_id'] for record in records]\n",
    "    \n",
    "    # Find positions of the tokens\n",
    "    pos_151668 = [i for i, tid in enumerate(token_ids) if tid == 151668]\n",
    "    pos_151645 = [i for i, tid in enumerate(token_ids) if tid == 151645]\n",
    "    \n",
    "    if len(pos_151668) != 1 or len(pos_151645) == 0:\n",
    "        return None\n",
    "\n",
    "    start_pos = pos_151668[0]\n",
    "    end_pos = pos_151645[0]  # Take the first occurrence of 151645\n",
    "    \n",
    "    if start_pos >= end_pos:\n",
    "        return None\n",
    "\n",
    "    # Extract token IDs between the markers (including the end marker)\n",
    "    between_token_ids = token_ids[start_pos:end_pos+1]\n",
    "    \n",
    "    # Decode the entire sequence at once to avoid U+FFFD issues\n",
    "    between_text = tokenizer.decode(between_token_ids)\n",
    "    \n",
    "    # Find all \\\\boxed{} patterns with proper brace matching\n",
    "    matches = []\n",
    "    i = 0\n",
    "    while i < len(between_text):\n",
    "        boxed_start = between_text.find('\\\\boxed{', i)\n",
    "        if boxed_start == -1:\n",
    "            break\n",
    "        \n",
    "        j = boxed_start + 7  # Start after '\\\\boxed{'\n",
    "        brace_count = 1\n",
    "        while j < len(between_text) and brace_count > 0:\n",
    "            if between_text[j] == '{':\n",
    "                brace_count += 1\n",
    "            elif between_text[j] == '}':\n",
    "                brace_count -= 1\n",
    "            j += 1\n",
    "        \n",
    "        if brace_count == 0:\n",
    "            matches.append(between_text[boxed_start + 7:j-1])\n",
    "        \n",
    "        i = boxed_start + 1\n",
    "    \n",
    "    return matches[-1] if matches else None\n",
    "\n",
    "def llm_grader(expected_answer, boxed_answer, openai_client, model_name=\"gpt-4o-mini\"):\n",
    "\n",
    "    def grader_prompt(expected_answer, boxed_answer):\n",
    "        \"\"\"Creates the system and user prompts for grading.\"\"\"\n",
    "        system_prompt = (\n",
    "            f\"You are an expert grader tasked with evaluating the correctness of an answer.\\n\"\n",
    "            f\"You will be provided with two pieces of text: the expected answer and the generated answer.\\n\"\n",
    "            f\"Your task is to determine if the generated answer is semantically equivalent to the expected answer.\\n\"\n",
    "            f\"Ignore minor formatting differences, extra whitespace, or trivial variations. For numerical answers, consider equivalent representations as correct (e.g., '1/2' and '0.5').\\n\"\n",
    "            f\"Respond with exactly one word: either 'true' (if correct) or 'false' (if incorrect). Do not include quotation marks, explanations, or any other text.\\n\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"Expected answer:\\n\"\n",
    "            f\"{expected_answer}\\n\"\n",
    "            f\"Generated answer:\\n\"\n",
    "            f\"{boxed_answer}\\n\"\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        return messages\n",
    "    \n",
    "    def grader(grading_messages, openai_client, model_name):\n",
    "        api_response = openai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=grading_messages\n",
    "        ).choices[0].message.content\n",
    "        \n",
    "        grade = api_response.strip().lower()\n",
    "        return grade\n",
    "    \n",
    "    grading_messages = grader_prompt(expected_answer, boxed_answer)\n",
    "    grade = grader(grading_messages, openai_client, model_name)\n",
    "    \n",
    "    # Ensure the grade is exactly 'true' or 'false'\n",
    "    if grade in ['true', 'false']:\n",
    "        return grade\n",
    "    else:\n",
    "        # Fallback in case the API returns something unexpected\n",
    "        return 'false'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3700de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Evaluation functions\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "async def graded_is_correct(gold, pred, tokenizer):\n",
    "    # Convert generated text into token-records so extract_boxed_answer works\n",
    "    ids = tokenizer.encode(pred)\n",
    "    records = [{\"token_id\": t} for t in ids]\n",
    "\n",
    "    boxed = extract_boxed_answer(records, tokenizer)\n",
    "    extracted = boxed if boxed else pred\n",
    "\n",
    "    return llm_grader(gold, extracted, client) == \"true\"\n",
    "\n",
    "def print_dataset_info(dataset, task_name):\n",
    "    \"\"\"Print dataset count and first 5 examples\"\"\"\n",
    "    print(f\"\\n--- {task_name.upper()} DATASET INFO ---\")\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    print(f\"Dataset type: {type(dataset)}\")\n",
    "    \n",
    "    # Check the first item to understand the structure\n",
    "    if len(dataset) > 0:\n",
    "        first_item = dataset[0]\n",
    "        print(f\"First item type: {type(first_item)}\")\n",
    "        print(f\"First item keys: {list(first_item.keys()) if hasattr(first_item, 'keys') else 'No keys'}\")\n",
    "    \n",
    "    print(f\"First 5 examples:\")\n",
    "    \n",
    "    for i in range(min(5, len(dataset))):\n",
    "        item = dataset[i]  # Access by index instead of iteration\n",
    "        print(f\"\\n{i+1}. \", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            if \"math\" in task_name:\n",
    "                question = item.get(\"problem\", item.get(\"question\", \"\"))\n",
    "                answer = item.get(\"solution\", \"\")\n",
    "                print(f\"Question: {question[:200]}...\")\n",
    "                print(f\"   Answer: {answer[:200]}...\")\n",
    "            elif \"gpqa\" in task_name:\n",
    "                # Handle both original GPQA format and alternative formats\n",
    "                question = item.get(\"Question\", item.get(\"question\", item.get(\"Problem\", \"\")))\n",
    "                answer = item.get(\"Correct Answer\", item.get(\"correct_answer\", item.get(\"Answer\", \"\")))\n",
    "                print(f\"Question: {question[:200]}...\")\n",
    "                print(f\"   Answer: {answer}\")\n",
    "            else:  # AIME\n",
    "                question = item.get(\"problem\", item.get(\"Problem\", item.get(\"question\", \"\")))\n",
    "                answer = str(item.get(\"answer\", item.get(\"Answer\", \"\")))\n",
    "                print(f\"Question: {question[:200]}...\")\n",
    "                print(f\"   Answer: {answer}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying item: {e}\")\n",
    "            print(f\"Item keys: {list(item.keys()) if hasattr(item, 'keys') else 'Not a dict'}\")\n",
    "            print(f\"Item type: {type(item)}\")\n",
    "            print(f\"Raw item: {str(item)[:200]}...\")\n",
    "            break\n",
    "\n",
    "async def evaluate_problem_multiple_times(item, task_name, num_runs):\n",
    "    \"\"\"\n",
    "    Evaluate a single problem multiple times and return accuracy for that problem.\n",
    "    \"\"\"\n",
    "    global engine, tokenizer\n",
    "    correct = 0\n",
    "    \n",
    "    # Extract question and gold answer based on task type\n",
    "    try:\n",
    "        if \"math\" in task_name:\n",
    "            question = item.get(\"problem\", item.get(\"question\", \"\"))\n",
    "            gold = item.get(\"solution\", \"\")\n",
    "        elif \"gpqa\" in task_name:\n",
    "            # Handle both original GPQA format and alternative formats\n",
    "            question = item.get(\"Question\", item.get(\"question\", item.get(\"Problem\", \"\")))\n",
    "            \n",
    "            # Try different field names for correct answer\n",
    "            gold = item.get(\"Correct Answer\", item.get(\"correct_answer\", item.get(\"Answer\", \"\")))\n",
    "            \n",
    "            # Handle choices if they exist\n",
    "            choices = []\n",
    "            if \"Incorrect Answer 1\" in item:\n",
    "                # Original format\n",
    "                choices = [\n",
    "                    item[\"Incorrect Answer 1\"],\n",
    "                    item[\"Incorrect Answer 2\"],\n",
    "                    item[\"Incorrect Answer 3\"],\n",
    "                    item[\"Correct Answer\"],\n",
    "                ]\n",
    "                random.shuffle(choices)\n",
    "                gold = chr(65 + choices.index(item[\"Correct Answer\"]))\n",
    "                question += \"\\n\\nChoices:\\n\" + \"\\n\".join(\n",
    "                    f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)\n",
    "                )\n",
    "            elif any(f\"choice_{i}\" in item for i in ['A', 'B', 'C', 'D']):\n",
    "                # Alternative choice format\n",
    "                choices = [item.get(f\"choice_{i}\", \"\") for i in ['A', 'B', 'C', 'D']]\n",
    "                question += \"\\n\\nChoices:\\n\" + \"\\n\".join(\n",
    "                    f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices) if c\n",
    "                )\n",
    "                # Find which choice is correct\n",
    "                for i, choice in enumerate(['A', 'B', 'C', 'D']):\n",
    "                    if item.get(f\"choice_{choice}\", \"\") == gold:\n",
    "                        gold = choice\n",
    "                        break\n",
    "            \n",
    "        else:  # AIME\n",
    "            question = item.get(\"problem\", item.get(\"Problem\", item.get(\"question\", \"\")))\n",
    "            gold = str(item.get(\"answer\", item.get(\"Answer\", \"\")))\n",
    "\n",
    "        system_prompt = (\n",
    "            f\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\n\"\n",
    "            f\"You must respond to every query in the following manner:\\n\"\n",
    "            f\"First, provide a step-by-step logical exploration of the problem.\\n\"\n",
    "            f\"Then, provide a clear and direct response based on your reasoning, with the final answer enclosed in \\\\boxed{{}}.\"\n",
    "        )\n",
    "\n",
    "        input = (\n",
    "            f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{question}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n<think>\"\n",
    "        )\n",
    "        \n",
    "        # Run the problem multiple times\n",
    "        for run in range(num_runs):\n",
    "            random.seed(BASE_SEED + run)\n",
    "            torch.manual_seed(BASE_SEED + run)\n",
    "            \n",
    "            # Generate with vLLM (using working pattern)\n",
    "            request_id = str(uuid.uuid4())\n",
    "            generator = engine.generate(input, sampling_params, request_id)\n",
    "            \n",
    "            # Get the result using working pattern\n",
    "            final_output = None\n",
    "            async for request_output in generator:\n",
    "                final_output = request_output\n",
    "            \n",
    "            if final_output and final_output.outputs:\n",
    "                predicted = final_output.outputs[0].text.strip()\n",
    "                if await graded_is_correct(gold, predicted, tokenizer):\n",
    "                    correct += 1\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing problem: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    return correct / num_runs\n",
    "\n",
    "async def evaluate_model_average(num_runs=NUM_RUNS):\n",
    "    \"\"\"\n",
    "    Evaluate a model on multiple datasets with multiple runs.\n",
    "    \"\"\"\n",
    "    print(f\"Using vLLM engine with {MODEL_REPO_NAME}...\")\n",
    "\n",
    "    # Reordered datasets as requested: aime24, aime25, gpqa diamond, math500\n",
    "    dataset_configs = {\n",
    "        \"aime_2024\": (\"HuggingFaceH4/aime_2024\", \"train\", None),\n",
    "        \"aime_2025\": (\"opencompass/AIME2025\", \"test\", \"AIME2025-I\"),  # Fixed split to \"test\"\n",
    "        \"gpqa_diamond\": (\"spawn99/GPQA-diamond-ClaudeR1\", \"train\", None),\n",
    "        \"math_500\": (\"HuggingFaceH4/MATH-500\", \"test\", None),\n",
    "    }\n",
    "\n",
    "    # First, load all datasets and print their info\n",
    "    datasets = {}\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" LOADING ALL DATASETS \")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for task, (repo, split, config) in dataset_configs.items():\n",
    "        try:\n",
    "            print(f\"Loading {task} dataset from {repo}...\")\n",
    "            if config:\n",
    "                ds = load_dataset(repo, config, split=split, trust_remote_code=True)\n",
    "            else:\n",
    "                ds = load_dataset(repo, split=split, trust_remote_code=True)\n",
    "            \n",
    "            datasets[task] = ds\n",
    "            print(f\"‚úÖ {task} loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {task}: {e}\")\n",
    "            print(f\"Skipping {task} dataset...\")\n",
    "            continue\n",
    "    \n",
    "    # Print info for all loaded datasets\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" DATASET INFORMATION \")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for task, ds in datasets.items():\n",
    "        print_dataset_info(ds, task)\n",
    "    \n",
    "    # Now run evaluations\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" STARTING EVALUATIONS \")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for task, ds in datasets.items():\n",
    "        print(f\"\\nüîÑ Evaluating {task.upper()}...\")\n",
    "        \n",
    "        problem_accuracies = []\n",
    "        \n",
    "        # Iterate through each problem\n",
    "        for i in tqdm(range(len(ds)), desc=f\"{task} problems\"):\n",
    "            item = ds[i]\n",
    "            problem_accuracy = await evaluate_problem_multiple_times(item, task, num_runs)\n",
    "            problem_accuracies.append(problem_accuracy)\n",
    "            \n",
    "            # Determine status\n",
    "            correct_runs = int(problem_accuracy * num_runs)  # Convert back to count\n",
    "            if correct_runs > 0:\n",
    "                status_emoji = \"‚úÖ\"\n",
    "            else:\n",
    "                status_emoji = \"‚ùå\"\n",
    "            \n",
    "            # Show individual problem results\n",
    "            print(f\"{status_emoji} Problem {i+1:02d}/{len(ds)} ‚Äî {task}: {correct_runs}/{num_runs}\")\n",
    "\n",
    "        average_accuracy = sum(problem_accuracies) / len(problem_accuracies)\n",
    "        all_results[task] = {\"average_accuracy\": average_accuracy, \"problem_accuracies\": problem_accuracies}\n",
    "        \n",
    "        # Final summary for this dataset\n",
    "        total_runs = len(ds) * num_runs\n",
    "        total_correct_runs = sum(int(acc * num_runs) for acc in problem_accuracies)\n",
    "        print(f\"‚úÖ {task} complete - {total_correct_runs}/{total_runs} ({average_accuracy:.2%} accuracy)\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261aab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vLLM engine with Qwen/Qwen3-0.6B...\n",
      "\n",
      "============================================================\n",
      " LOADING ALL DATASETS \n",
      "============================================================\n",
      "Loading aime_2024 dataset from HuggingFaceH4/aime_2024...\n",
      "‚úÖ aime_2024 loaded successfully\n",
      "Loading aime_2025 dataset from opencompass/AIME2025...\n",
      "‚úÖ aime_2025 loaded successfully\n",
      "Loading gpqa_diamond dataset from spawn99/GPQA-diamond-ClaudeR1...\n",
      "‚úÖ gpqa_diamond loaded successfully\n",
      "Loading math_500 dataset from HuggingFaceH4/MATH-500...\n",
      "‚úÖ math_500 loaded successfully\n",
      "\n",
      "============================================================\n",
      " DATASET INFORMATION \n",
      "============================================================\n",
      "\n",
      "--- AIME_2024 DATASET INFO ---\n",
      "Total samples: 30\n",
      "Dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "First item type: <class 'dict'>\n",
      "First item keys: ['id', 'problem', 'solution', 'answer', 'url', 'year']\n",
      "First 5 examples:\n",
      "\n",
      "1. Question: Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ min...\n",
      "   Answer: 204\n",
      "\n",
      "2. Question: Let $ABC$ be a triangle inscribed in circle $\\omega$. Let the tangents to $\\omega$ at $B$ and $C$ intersect at point $D$, and let $\\overline{AD}$ intersect $\\omega$ at $P$. If $AB=5$, $BC=9$, and $AC=...\n",
      "   Answer: 113\n",
      "\n",
      "3. Question: Each vertex of a regular octagon is independently colored either red or blue with equal probability. The probability that the octagon can then be rotated so that all of the blue vertices end up at pos...\n",
      "   Answer: 371\n",
      "\n",
      "4. Question: Define $f(x)=|| x|-\\tfrac{1}{2}|$ and $g(x)=|| x|-\\tfrac{1}{4}|$. Find the number of intersections of the graphs of \\[y=4 g(f(\\sin (2 \\pi x))) \\quad\\text{ and }\\quad x=4 g(f(\\cos (3 \\pi y))).\\]...\n",
      "   Answer: 385\n",
      "\n",
      "5. Question: Let $p$ be the least prime number for which there exists a positive integer $n$ such that $n^{4}+1$ is divisible by $p^{2}$. Find the least positive integer $m$ such that $m^{4}+1$ is divisible by $p^...\n",
      "   Answer: 110\n",
      "\n",
      "--- AIME_2025 DATASET INFO ---\n",
      "Total samples: 15\n",
      "Dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "First item type: <class 'dict'>\n",
      "First item keys: ['question', 'answer']\n",
      "First 5 examples:\n",
      "\n",
      "1. Question: Find the sum of all integer bases $b>9$ for which $17_{b}$ is a divisor of $97_{b}$....\n",
      "   Answer: 70\n",
      "\n",
      "2. Question: On $\\triangle ABC$ points $A,D,E$, and $B$ lie that order on side $\\overline{AB}$ with $AD=4, DE=16$, and $EB=8$. Points $A,F,G$, and $C$ lie in that order on side $\\overline{AC}$ with $AF=13, FG=52$,...\n",
      "   Answer: 588\n",
      "\n",
      "3. Question: The 9 members of a baseball team went to an ice cream parlor after their game. Each player had a singlescoop cone of chocolate, vanilla, or strawberry ice cream. At least one player chose each flavor,...\n",
      "   Answer: 16\n",
      "\n",
      "4. Question: Find the number of ordered pairs $(x,y)$, where both $x$ and $y$ are integers between $-100$ and $100$, inclusive, such that $12x^{2}-xy-6y^{2}=0$....\n",
      "   Answer: 117\n",
      "\n",
      "5. Question: There are $8!=40320$ eight-digit positive integers that use each of the digits $1,2,3,4,5,6,7,8$ exactly once. Let $N$ be the number of these integers that are divisible by 22. Find the difference bet...\n",
      "   Answer: 279\n",
      "\n",
      "--- GPQA_DIAMOND DATASET INFO ---\n",
      "Total samples: 198\n",
      "Dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "First item type: <class 'dict'>\n",
      "First item keys: ['record_id', 'question', 'correct_answer', 'correct_explanation', 'deepseek_reasoner_answer', 'deepseek_reasoner_grade', 'deepseek_reasoner_eval', 'claude_sonnet_standalone_answer', 'claude_sonnet_standalone_grade', 'claude_sonnet_standalone_eval', 'claude_sonnet_with_reasoning_answer', 'claude_sonnet_with_reasoning_grade', 'claude_sonnet_with_reasoning_eval', 'metadata', 'token_usage', 'costs', 'model_responses']\n",
      "First 5 examples:\n",
      "\n",
      "1. Question: Two quantum states with energies E1 and E2 have a lifetime of 10^-9 sec and 10^-8 sec, respectively. We want to clearly distinguish these two energy levels. Which one of the following options could be...\n",
      "   Answer: 10^-4 eV\n",
      "\n",
      "2. Question: trans-cinnamaldehyde was treated with methylmagnesium bromide, forming product 1.\n",
      "\n",
      "1 was treated with pyridinium chlorochromate, forming product 2.\n",
      "\n",
      "3 was treated with (dimethyl(oxo)-l6-sulfaneylidene...\n",
      "   Answer: 11\n",
      "\n",
      "3. Question: A spin-half particle is in a linear superposition 0.5|\\uparrow\\rangle+sqrt(3)/2|\\downarrow\\rangle of its spin-up and spin-down states. If |\\uparrow\\rangle and |\\downarrow\\rangle are the eigenstates of...\n",
      "   Answer: -0.7\n",
      "\n",
      "4. Question: In a parallel universe where a magnet can have an isolated North or South pole, Maxwell‚Äôs equations look different. But, specifically, which of those equations are different?...\n",
      "   Answer: The ones related to the circulation of the electric field and the divergence of the magnetic field.  \n",
      "\n",
      "5. Question: Calculate the eigenvector of a quantum mechanical operator $\\vec{P}$ for a muon along an arbitrary direction $\\vec{n}$ lying in the x-z plane corresponding to the eigenvalue $+\\hbar/2$. Given the $X-$...\n",
      "   Answer: (\\cos(\\theta/2), \\sin (\\theta/2))\n",
      "\n",
      "\n",
      "--- MATH_500 DATASET INFO ---\n",
      "Total samples: 500\n",
      "Dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "First item type: <class 'dict'>\n",
      "First item keys: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id']\n",
      "First 5 examples:\n",
      "\n",
      "1. Question: Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$...\n",
      "   Answer: We have that $r = \\sqrt{0^2 + 3^2} = 3.$  Also, if we draw the line connecting the origin and $(0,3),$ this line makes an angle of $\\frac{\\pi}{2}$ with the positive $x$-axis.\n",
      "\n",
      "[asy]\n",
      "unitsize(0.8 cm);\n",
      "...\n",
      "\n",
      "2. Question: Define\n",
      "\\[p = \\sum_{k = 1}^\\infty \\frac{1}{k^2} \\quad \\text{and} \\quad q = \\sum_{k = 1}^\\infty \\frac{1}{k^3}.\\]Find a way to write\n",
      "\\[\\sum_{j = 1}^\\infty \\sum_{k = 1}^\\infty \\frac{1}{(j + k)^3}\\]in term...\n",
      "   Answer: We count the number of times $\\frac{1}{n^3}$ appears in the sum\n",
      "\\[\\sum_{j = 1}^\\infty \\sum_{k = 1}^\\infty \\frac{1}{(j + k)^3},\\]where $n$ is a fixed positive integer.  (In other words, we are conditio...\n",
      "\n",
      "3. Question: If $f(x) = \\frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a common fraction....\n",
      "   Answer: $f(-2)+f(-1)+f(0)=\\frac{3(-2)-2}{-2-2}+\\frac{3(-1)-2}{-1-2}+\\frac{3(0)-2}{0-2}=\\frac{-8}{-4}+\\frac{-5}{-3}+\\frac{-2}{-2}=2+\\frac{5}{3}+1=\\boxed{\\frac{14}{3}}$...\n",
      "\n",
      "4. Question: How many positive whole-number divisors does 196 have?...\n",
      "   Answer: First prime factorize $196=2^2\\cdot7^2$.  The prime factorization of any divisor of 196 cannot include any primes other than 2 and 7.  We are free to choose either 0, 1, or 2 as the exponent of 2 in t...\n",
      "\n",
      "5. Question: The results of a cross-country team's training run are graphed below. Which student has the greatest average speed? [asy]\n",
      "for ( int i = 1; i <= 7; ++i )\n",
      "{\n",
      "\n",
      "draw((i,0)--(i,6));\n",
      "}\n",
      "\n",
      "for ( int i = 1; i <=...\n",
      "   Answer: Evelyn covered more distance in less time than Briana, Debra and Angela, so her average speed is greater than any of their average speeds. Evelyn went almost as far as Carla in less than half the time...\n",
      "\n",
      "============================================================\n",
      " STARTING EVALUATIONS \n",
      "============================================================\n",
      "\n",
      "üîÑ Evaluating AIME_2024...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:   3%|‚ñé         | 1/30 [01:26<41:41, 86.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 01/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:   7%|‚ñã         | 2/30 [02:52<40:14, 86.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 02/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  10%|‚ñà         | 3/30 [04:18<38:46, 86.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 03/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  13%|‚ñà‚ñé        | 4/30 [04:58<29:22, 67.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 04/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  17%|‚ñà‚ñã        | 5/30 [06:17<29:56, 71.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 05/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  20%|‚ñà‚ñà        | 6/30 [07:43<30:44, 76.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 06/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  23%|‚ñà‚ñà‚ñé       | 7/30 [09:00<29:27, 76.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 07/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  27%|‚ñà‚ñà‚ñã       | 8/30 [10:25<29:07, 79.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 08/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  30%|‚ñà‚ñà‚ñà       | 9/30 [10:45<21:20, 60.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 09/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  33%|‚ñà‚ñà‚ñà‚ñé      | 10/30 [11:42<19:53, 59.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 10/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  37%|‚ñà‚ñà‚ñà‚ñã      | 11/30 [13:12<21:48, 68.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 11/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [14:38<22:16, 74.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 12/30 ‚Äî aime_2024: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 13/30 [14:52<15:51, 55.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 13/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 14/30 [15:44<14:33, 54.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 14/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [17:09<15:59, 63.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 15/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 16/30 [18:03<14:11, 60.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 16/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [19:30<14:52, 68.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 17/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [20:24<12:50, 64.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 18/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 19/30 [21:52<13:05, 71.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 19/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [23:19<12:39, 75.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 20/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 21/30 [24:09<10:15, 68.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 21/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 22/30 [24:45<07:48, 58.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 22/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 23/30 [26:05<07:35, 65.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 23/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/30 [27:04<06:18, 63.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 24/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 25/30 [27:21<04:06, 49.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 25/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 26/30 [28:47<04:01, 60.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 26/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 27/30 [29:40<02:54, 58.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 27/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 28/30 [31:06<02:13, 66.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 28/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [32:16<01:07, 67.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 29/30 ‚Äî aime_2024: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2024 problems: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [32:31<00:00, 65.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 30/30 ‚Äî aime_2024: 0/1\n",
      "‚úÖ aime_2024 complete - 1/30 (3.33% accuracy)\n",
      "\n",
      "üîÑ Evaluating AIME_2025...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:   7%|‚ñã         | 1/15 [00:17<04:08, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 01/15 ‚Äî aime_2025: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:  13%|‚ñà‚ñé        | 2/15 [01:24<10:04, 46.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 02/15 ‚Äî aime_2025: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:  20%|‚ñà‚ñà        | 3/15 [01:59<08:13, 41.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 03/15 ‚Äî aime_2025: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:  27%|‚ñà‚ñà‚ñã       | 4/15 [02:26<06:30, 35.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 04/15 ‚Äî aime_2025: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:  33%|‚ñà‚ñà‚ñà‚ñé      | 5/15 [03:04<06:07, 36.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 05/15 ‚Äî aime_2025: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:  40%|‚ñà‚ñà‚ñà‚ñà      | 6/15 [03:46<05:46, 38.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 06/15 ‚Äî aime_2025: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 7/15 [04:46<06:02, 45.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 07/15 ‚Äî aime_2025: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 8/15 [05:36<05:28, 46.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 08/15 ‚Äî aime_2025: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 9/15 [06:30<04:54, 49.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 09/15 ‚Äî aime_2025: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 10/15 [07:27<04:17, 51.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 10/15 ‚Äî aime_2025: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aime_2025 problems:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 11/15 [08:30<03:40, 55.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 11/15 ‚Äî aime_2025: 0/1\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main execution\n",
    "\n",
    "results = await evaluate_model_average(NUM_RUNS)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" FINAL AVERAGED ACCURACIES \")\n",
    "print(\"=\"*50)\n",
    "for task, result in results.items():\n",
    "    print(f\"{task.upper():<15}: {result['average_accuracy']:.2%}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2957ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
