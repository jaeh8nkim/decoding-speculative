{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d8f1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jaeh8nkim/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch: 2.6.0+cu124\n",
      "✅ Transformers: 4.53.0\n",
      "✅ TRL: 0.19.0\n",
      "✅ CUDA: True\n",
      "✅ GPUs detected: 1\n",
      "  GPU 0: NVIDIA RTX A6000 (47.5GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjaeh8nkim\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaeh8nkim\n",
      "{\n",
      "  \"name\": \"jaeh8nkim\",\n",
      "  \"auth\": {\n",
      "    \"type\": \"access_token\",\n",
      "    \"accessToken\": {\n",
      "      \"displayName\": \"notebooks\",\n",
      "      \"role\": \"write\",\n",
      "      \"createdAt\": \"2025-06-29T15:24:25.718Z\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "🔄 Please restart kernel and continue with configuration\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Installation and Setup\n",
    "# Step 1: Install PyTorch and ML libraries\n",
    "# !pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q --upgrade transformers accelerate datasets trl bitsandbytes scipy deepspeed wandb\n",
    "\n",
    "# Step 2: Verification and imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "import logging\n",
    "import warnings\n",
    "import wandb\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import HfApi, create_repo, upload_folder, login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "load_dotenv() \n",
    "\n",
    "# Configure logging\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Print versions and GPU info\n",
    "print(f\"✅ PyTorch: {torch.__version__}\")\n",
    "print(f\"✅ Transformers: {transformers.__version__}\")\n",
    "print(f\"✅ TRL: {trl.__version__}\")\n",
    "print(f\"✅ CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"✅ GPUs detected: {gpu_count}\")\n",
    "    for i in range(gpu_count):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f}GB)\")\n",
    "\n",
    "# Step 3: Login to services\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"), relogin=True)\n",
    "wb_api  = wandb.Api()        \n",
    "wb_user = wb_api.viewer\n",
    "print(wb_user.username)\n",
    "\n",
    "login(token=os.getenv(\"HF_WRITE_TOKEN\"))\n",
    "hf_info = HfApi().whoami(token=os.getenv(\"HF_WRITE_TOKEN\"))\n",
    "print(json.dumps({k: hf_info[k] for k in (\"name\", \"auth\") if k in hf_info}, indent=2))\n",
    "\n",
    "print(\"🔄 Please restart kernel and continue with configuration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5445e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Base model (loading from): Qwen/Qwen3-0.6B\n",
      "  Output model (saving to): s1-4q36-qwen3-0.6b\n",
      "  Block size: 8192\n",
      "  Dataset: jaeh8nkim/s1K_for_Qwen3-0.6B\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Training Configuration\n",
    "\n",
    "# Canonical IDs for both models\n",
    "BASE_MODEL_NAME = \"Qwen3-0.6B\"\n",
    "BASE_MODEL_REMOTE_PATH = \"Qwen/\" + BASE_MODEL_NAME\n",
    "BASE_MODEL_LOCAL_PATH = BASE_MODEL_NAME + \"-local\"\n",
    "\n",
    "# SFTD_MODEL_NAME = \"s1K-Distill-Qwen3-0.6B\" + \"-250710\"\n",
    "# SFTD_MODEL_REMOTE_PATH = \"jaeh8nkim/\" + SFTD_MODEL_NAME\n",
    "# SFTD_MODEL_LOCAL_PATH = SFTD_MODEL_NAME + \"-local\"\n",
    "\n",
    "SFTD_MODEL_NAME = \"s1K4Q3p6B-Distill-Qwen3-0.6B\" + \"-250710\"\n",
    "SFTD_MODEL_REMOTE_PATH = \"jaeh8nkim/\" + SFTD_MODEL_NAME\n",
    "SFTD_MODEL_LOCAL_PATH = SFTD_MODEL_NAME + \"-local\"\n",
    "\n",
    "# DATASET_REMOTE_PATH = \"simplescaling/s1K-1.1_tokenized\"\n",
    "DATASET_REMOTE_PATH = \"jaeh8nkim/s1K-for-Qwen3-0.6B\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"S1-faithful training configuration\"\"\"\n",
    "    base_model_name:   str = field(default=BASE_MODEL_REMOTE_PATH)   # load from here\n",
    "    output_model_name: str = field(default=SFTD_MODEL_NAME)   # save to here\n",
    "    block_size: int = 8192\n",
    "    wandb_project: Optional[str] = SFTD_MODEL_NAME\n",
    "    wandb_entity:  Optional[str] = \"jaeh8nkim\"\n",
    "    train_file_path: Optional[str] = DATASET_REMOTE_PATH\n",
    "    dagger: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.wandb_project:\n",
    "            os.environ[\"WANDB_PROJECT\"] = self.wandb_project\n",
    "        if self.wandb_entity:\n",
    "            os.environ[\"WANDB_ENTITY\"]  = self.wandb_entity\n",
    "\n",
    "# Initialise\n",
    "config = TrainingConfig()\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Base model (loading from): {config.base_model_name}\")\n",
    "print(f\"  Output model (saving to): {config.output_model_name}\")\n",
    "print(f\"  Block size: {config.block_size}\")\n",
    "print(f\"  Dataset: {config.train_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007e0686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 07:37:13,961 - INFO - Loading model: Qwen/Qwen3-0.6B\n",
      "2025-07-01 07:37:16,220 - INFO - Loading dataset: jaeh8nkim/s1K_for_Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded: 596,049,920 total, 596,049,920 trainable parameters\n",
      "✅ Dataset loaded: 157 train samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af27c7677d34e8588fa70264abf731c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Applied Qwen3 token style conversion\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Model and Dataset Loading (FIXED)\n",
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Load Qwen3-0.6B model and tokenizer\"\"\"\n",
    "    logging.info(f\"Loading model: {config.base_model_name}\")  # Changed this line\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model_name,  # Changed this line\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.base_model_name,  # Changed this line\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Template definitions\n",
    "    instruction_template = \"<|im_start|>user\"\n",
    "    response_template = \"<|im_start|>assistant\\n\"\n",
    "    \n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"✅ Model loaded: {total:,} total, {train:,} trainable parameters\")\n",
    "    \n",
    "    return model, tokenizer, instruction_template, response_template\n",
    "\n",
    "def load_s1_dataset(qwen3_style=True):\n",
    "    \"\"\"Load and preprocess S1 dataset\"\"\"\n",
    "    logging.info(f\"Loading dataset: {config.train_file_path}\")\n",
    "    \n",
    "    dataset = load_dataset(config.train_file_path)\n",
    "    print(f\"✅ Dataset loaded: {len(dataset['train'])} train samples\")\n",
    "    \n",
    "    # Filter to text column only\n",
    "    train_dataset = dataset['train'].select_columns(['text'])\n",
    "    test_dataset = dataset['test'].select_columns(['text']) if 'test' in dataset else train_dataset\n",
    "    filtered_dataset = DatasetDict(train=train_dataset, test=test_dataset)\n",
    "    \n",
    "    # Apply Qwen3 token style conversion\n",
    "    if qwen3_style:\n",
    "        def swap_tokens(example):\n",
    "            txt = example['text']\n",
    "            txt = txt.replace('<|im_start|>think', '<think>')\n",
    "            txt = txt.replace('<|im_start|>answer', '</think>')\n",
    "            example['text'] = txt\n",
    "            return example\n",
    "        \n",
    "        filtered_dataset = filtered_dataset.map(swap_tokens)\n",
    "        print(\"✅ Applied Qwen3 token style conversion\")\n",
    "    \n",
    "    return filtered_dataset\n",
    "\n",
    "# Load model and data\n",
    "model, tokenizer, instruction_template, response_template = load_model_and_tokenizer()\n",
    "dataset = load_s1_dataset(qwen3_style=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c486af95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Detected dataset size: 157\n",
      "Training setup: 16 batch size, 45 steps, 2 warmup\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71e31317dc641d9b87cfddeb025cce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11abafc501a94dfb9d903b8196b5152d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5385cab2a0e496fb5f2fda41d56691e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 07:37:35,908] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 07:37:36,022 - INFO - /home/jaeh8nkim/equigranular/.conda/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/jaeh8nkim/equigranular/.conda/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/jaeh8nkim/equigranular/.conda/include -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/jaeh8nkim/equigranular/.conda/include -c /tmp/tmpftlcyqz_/test.c -o /tmp/tmpftlcyqz_/test.o\n",
      "2025-07-01 07:37:36,065 - INFO - /home/jaeh8nkim/equigranular/.conda/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/jaeh8nkim/equigranular/.conda/lib -Wl,-rpath-link,/home/jaeh8nkim/equigranular/.conda/lib -L/home/jaeh8nkim/equigranular/.conda/lib /tmp/tmpftlcyqz_/test.o -laio -o /tmp/tmpftlcyqz_/a.out\n",
      "/home/jaeh8nkim/equigranular/.conda/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "2025-07-01 07:37:36,622 - INFO - /home/jaeh8nkim/equigranular/.conda/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/jaeh8nkim/equigranular/.conda/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/jaeh8nkim/equigranular/.conda/include -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/jaeh8nkim/equigranular/.conda/include -c /tmp/tmpykh3zr_k/test.c -o /tmp/tmpykh3zr_k/test.o\n",
      "2025-07-01 07:37:36,665 - INFO - /home/jaeh8nkim/equigranular/.conda/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/jaeh8nkim/equigranular/.conda/lib -Wl,-rpath-link,/home/jaeh8nkim/equigranular/.conda/lib -L/home/jaeh8nkim/equigranular/.conda/lib /tmp/tmpykh3zr_k/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpykh3zr_k/a.out\n",
      "/home/jaeh8nkim/equigranular/.conda/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `memcpy@GLIBC_2.14'\n",
      "/home/jaeh8nkim/equigranular/.conda/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/jaeh8nkim/equigranular/.conda/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/jaeh8nkim/equigranular/.conda/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /home/jaeh8nkim/equigranular/.conda/lib/libstdc++.so.6: undefined reference to `aligned_alloc@GLIBC_2.16'\n",
      "/home/jaeh8nkim/equigranular/.conda/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/jaeh8nkim/equigranular/.conda/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/home/jaeh8nkim/equigranular/.conda/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `clock_gettime@GLIBC_2.17'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 07:37:37,424] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "GPU memory before training: 1.1GB / 47.5GB\n",
      "🚀 Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/decoding-speculative/wandb/run-20250701_073737-t0rp8ycg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jaeh8nkim/s1-4q36-qwen3-0.6b/runs/t0rp8ycg' target=\"_blank\">s1-4q36-qwen3-0.6b</a></strong> to <a href='https://wandb.ai/jaeh8nkim/s1-4q36-qwen3-0.6b' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jaeh8nkim/s1-4q36-qwen3-0.6b' target=\"_blank\">https://wandb.ai/jaeh8nkim/s1-4q36-qwen3-0.6b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jaeh8nkim/s1-4q36-qwen3-0.6b/runs/t0rp8ycg' target=\"_blank\">https://wandb.ai/jaeh8nkim/s1-4q36-qwen3-0.6b/runs/t0rp8ycg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 08:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.533100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.492800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.472600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.399300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.454500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.464700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.474800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.456900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.426300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.432200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.484700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.445100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.432200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.426900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.448500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.450800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.475800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 07:39:14,145 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:14,245 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:14,246 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:14,678 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:14,781 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:14,782 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:15,218 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:15,280 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:15,281 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:15,567 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:15,650 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:15,651 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:16,045 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:16,172 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:16,173 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:16,733 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:16,854 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:16,855 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:17,402 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:17,537 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:17,538 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:18,134 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:18,263 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:18,264 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:18,819 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:18,899 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:18,900 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:19,266 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:19,363 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:19,364 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:19,819 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:19,975 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:19,977 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:20,645 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:20,704 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:20,705 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:20,972 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:21,051 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:39:21,052 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:55,459 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:55,598 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:55,599 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:56,225 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:56,396 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:56,397 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:57,157 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:57,287 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:57,288 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:57,843 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:57,918 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:57,919 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:58,276 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:58,395 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:58,396 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:58,932 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:59,064 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:59,065 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:59,632 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:59,717 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:40:59,718 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:00,093 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:00,154 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:00,155 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:00,473 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:00,630 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:00,632 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:01,317 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:01,398 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:01,399 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:01,783 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:01,892 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:01,893 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:02,404 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:02,556 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:02,557 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:03,215 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:03,289 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:41:03,290 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:37,542 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:37,730 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:37,731 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:38,535 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:38,610 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:38,611 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:38,955 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:39,047 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:39,048 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:39,488 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:39,638 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:39,639 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:40,284 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:40,346 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:40,348 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:40,649 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:40,778 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:40,779 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:41,351 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:41,481 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:41,482 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:42,087 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:42,275 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:42,276 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:43,076 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:43,129 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:43,130 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:43,382 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:43,454 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:43,455 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:43,802 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:43,927 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:43,928 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:44,515 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:44,702 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:44,703 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:45,512 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:45,596 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:42:45,598 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:18,884 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:18,965 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:18,966 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:19,349 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:19,475 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:19,476 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:20,035 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:20,160 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:20,161 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:20,755 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:20,963 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:20,964 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:21,886 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:21,995 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:21,996 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:22,492 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:22,618 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:22,619 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:23,191 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:23,340 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:23,341 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:24,012 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:24,147 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:24,148 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:24,771 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:24,953 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:24,955 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:25,753 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:25,859 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:25,860 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:26,335 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:26,460 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:26,462 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:27,007 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:27,108 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:27,109 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:27,565 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:27,703 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:44:27,705 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:01,135 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:01,265 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:01,266 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:01,865 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:02,051 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:02,052 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:02,864 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:02,962 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:02,963 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:03,398 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:03,503 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:03,504 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:03,975 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:04,078 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:04,079 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:04,539 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:04,650 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:04,651 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:05,173 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:05,329 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:05,330 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:06,032 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:06,161 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:06,162 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:06,732 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:06,840 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:06,841 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:07,336 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:07,451 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:07,452 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:08,000 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:08,181 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:08,182 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:08,981 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:09,118 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:09,119 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:09,736 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:09,910 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "2025-07-01 07:46:09,911 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training completed! Model saved to: ./s1-4q36-qwen3-0.6b\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Training Configuration and Execution\n",
    "def create_sft_config():\n",
    "    \"\"\"Create SFTConfig with S1-faithful hyperparameters\"\"\"\n",
    "    \n",
    "    # Load the dataset to get its size\n",
    "    current_dataset = load_dataset(config.train_file_path)\n",
    "    dataset_size = len(current_dataset['train'])\n",
    "\n",
    "    # S1 hyperparameters\n",
    "    lr = 1e-5\n",
    "    epochs = 5\n",
    "    weight_decay = 1e-4\n",
    "    micro_batch_size = 1\n",
    "    gradient_accumulation_steps = 16\n",
    "    \n",
    "    # Calculate training steps using actual dataset size\n",
    "    effective_batch_size = micro_batch_size * gradient_accumulation_steps\n",
    "    steps_per_epoch = dataset_size // effective_batch_size\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    warmup_steps = int(total_steps * 0.05)\n",
    "    \n",
    "    print(f\"🎯 Detected dataset size: {dataset_size}\")\n",
    "    print(f\"Training setup: {effective_batch_size} batch size, {total_steps} steps, {warmup_steps} warmup\")\n",
    "    \n",
    "    return trl.SFTConfig(\n",
    "        # Model and data\n",
    "        max_seq_length=config.block_size,\n",
    "        dataset_text_field='text',\n",
    "        \n",
    "        # Training schedule\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        per_device_eval_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        \n",
    "        # Optimizer\n",
    "        learning_rate=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.95,\n",
    "        warmup_ratio=0.05,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        \n",
    "        # Precision and memory\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=0,\n",
    "        \n",
    "        # Evaluation and logging\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=1,\n",
    "        \n",
    "        # Output\n",
    "        output_dir=f\"./{SFTD_MODEL_NAME}\",\n",
    "        logging_dir=f\"./{SFTD_MODEL_NAME}/logs\",\n",
    "        save_only_model=True,\n",
    "        \n",
    "        # Data handling\n",
    "        remove_unused_columns=True,\n",
    "        report_to=[\"wandb\"] if config.wandb_project else [],\n",
    "        run_name=SFTD_MODEL_NAME,\n",
    "        dataloader_drop_last=True,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Execute S1-faithful training\"\"\"\n",
    "    sft_args = create_sft_config()\n",
    "    \n",
    "    # Create data collator\n",
    "    collator = trl.DataCollatorForCompletionOnlyLM(\n",
    "        instruction_template=instruction_template,\n",
    "        response_template=response_template,\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = trl.SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'] if 'test' in dataset else dataset['train'],\n",
    "        args=sft_args,\n",
    "        data_collator=collator\n",
    "    )\n",
    "    \n",
    "    # Monitor memory\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU memory before training: {allocated:.1f}GB / {total:.1f}GB\")\n",
    "    \n",
    "    # Train\n",
    "    print(\"🚀 Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model\n",
    "    trainer.save_model(output_dir=sft_args.output_dir)\n",
    "    tokenizer.save_pretrained(sft_args.output_dir)\n",
    "    \n",
    "    print(f\"✅ Training completed! Model saved to: {sft_args.output_dir}\")\n",
    "    return trainer, sft_args\n",
    "\n",
    "# Execute training\n",
    "trainer, sft_args = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b6ea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Step 1: Setting up repository and uploading SFT-ed model...\n",
      "✅ Repository ready: https://huggingface.co/jaeh8nkim/s1-4q36-qwen3-0.6b\n",
      "📤 Uploading SFT-ed model to jaeh8nkim/s1-4q36-qwen3-0.6b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43a0c5b77854f4eabf43ea989b42739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da39fbda5cfb4a0eab526c86f006e1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a09e2caa5e54848aaab2926afcc4f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc10e3d32534129acdf04aba2b11e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SFT-ed model uploaded successfully!\n",
      "\n",
      "🚀 Step 2: Downloading both models for local inference...\n",
      "📥 Downloading Official base Qwen3-0.6B from Qwen/Qwen3-0.6B...\n",
      "✅ Official base Qwen3-0.6B ready at ./vanilla-qwen3-0.6b-local\n",
      "📥 Downloading SFT-ed model from HuggingFace from jaeh8nkim/s1-4q36-qwen3-0.6b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124ab677c3b74b00a0e4144af5a32075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2900e0d9fa7446a86523d66162e7537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cbafe8fea44032976b681c72c270e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff3cd26104a473a904f938ba63a0e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc8954c7be247c2a91832aa8a2a9155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3d4f8a0d054456a243bf88629816d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1226420b0a3e4f8ca825a651c5801070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d13fe44c504ecb8857e44231e44b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e382bfb173424c438a6089e192605e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25d97d57685427db71c4e6c56f69718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SFT-ed model from HuggingFace ready at ./s1-4q36-qwen3-0.6b-local\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Model Upload and Download\n",
    "\n",
    "def setup_sft_repository():\n",
    "    \"\"\"Setup HuggingFace repository for SFT-ed model\"\"\"\n",
    "\n",
    "    repo_id  = SFTD_MODEL_REMOTE_PATH\n",
    "\n",
    "    try:\n",
    "        create_repo(repo_id=repo_id, repo_type=\"model\", private=False, exist_ok=True)\n",
    "        print(f\"✅ Repository ready: https://huggingface.co/{repo_id}\")\n",
    "        return repo_id\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Repository setup issue: {e}\")\n",
    "        return None\n",
    "\n",
    "def upload_sft_model(repo_id):\n",
    "    \"\"\"Upload SFT-ed model\"\"\"\n",
    "    if repo_id:\n",
    "        try:\n",
    "            print(f\"📤 Uploading SFT-ed model to {repo_id}...\")\n",
    "            upload_folder(\n",
    "                folder_path=sft_args.output_dir,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=\"Upload S1-faithful fine-tuned Qwen3-0.6B\"\n",
    "            )\n",
    "            print(f\"✅ SFT-ed model uploaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Upload failed: {e}\")\n",
    "\n",
    "def download_model(repo_name, local_folder_name, description):\n",
    "    \"\"\"Download a single model and save locally\"\"\"\n",
    "    print(f\"📥 Downloading {description} from {repo_name}...\")\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            repo_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            repo_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Save locally\n",
    "        local_path = f\"./{local_folder_name}\"\n",
    "        model.save_pretrained(local_path)\n",
    "        tokenizer.save_pretrained(local_path)\n",
    "\n",
    "        print(f\"✅ {description} ready at {local_path}\")\n",
    "\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"path\": local_path\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Download failed for {description}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Execute model management\n",
    "print(\"🚀 Step 1: Setting up repository and uploading SFT-ed model...\")\n",
    "sft_repo_id = setup_sft_repository()\n",
    "upload_sft_model(sft_repo_id)\n",
    "\n",
    "print(\"\\n🚀 Step 2: Downloading both models for local inference...\")\n",
    "\n",
    "# Download SFT model\n",
    "base_model = download_model(\n",
    "    repo_name=BASE_MODEL_REMOTE_PATH,\n",
    "    local_folder_name=BASE_MODEL_LOCAL_PATH,\n",
    "    description=\"Official base Qwen3-0.6B\",\n",
    ")\n",
    "\n",
    "sftd_model = download_model(\n",
    "    repo_name=SFTD_MODEL_REMOTE_PATH,\n",
    "    local_folder_name=SFTD_MODEL_LOCAL_PATH,\n",
    "    description=\"SFT-ed model from HuggingFace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d1241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
