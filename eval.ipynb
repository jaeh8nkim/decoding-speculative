{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d06e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-01 08:33:45 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "\n",
    "# !pip install -q torch==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip install -q vllm==0.8.5.post1\n",
    "# !pip install --upgrade fsspec datasets\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random, os, json, re, torch\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# vLLM imports\n",
    "import html, uuid, asyncio, contextlib, nest_asyncio, logging\n",
    "from IPython.display import HTML, display\n",
    "from huggingface_hub import snapshot_download\n",
    "from vllm import TokensPrompt\n",
    "from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "from vllm.sampling_params import SamplingParams, RequestOutputKind\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "torch.set_grad_enabled(False)\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "BASE_SEED = 42\n",
    "SMALL_TEMPERATURE = 0.7\n",
    "\n",
    "# MODEL_NAME = \"Qwen3-0.6B\"\n",
    "# MODEL_REMOTE_PATH = \"Qwen/\" + MODEL_NAME\n",
    "# MODEL_LOCAL_PATH = MODEL_NAME + \"-local\"\n",
    "\n",
    "# MODEL_NAME = \"s1K-Distill-Qwen3-0.6B\" + \"-250710\"\n",
    "# MODEL_REMOTE_PATH = \"jaeh8nkim/\" + MODEL_NAME\n",
    "# MODEL_LOCAL_PATH = MODEL_NAME + \"-local\"\n",
    "\n",
    "MODEL_NAME = \"s1K4Q3p6B-Distill-Qwen3-0.6B\" + \"-250710\"\n",
    "MODEL_REMOTE_PATH = \"jaeh8nkim/\" + MODEL_NAME\n",
    "MODEL_LOCAL_PATH = MODEL_NAME + \"-local\"\n",
    "\n",
    "SMALL_GPU_INDEX = \"2\"\n",
    "\n",
    "NUM_RUNS = 1\n",
    "# NUM_RUNS = 16\n",
    "\n",
    "# MAX_SEQ_LEN = 8192\n",
    "MAX_SEQ_LEN = 16384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf17ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Downloading model locally...\n",
      "üì• Downloading model from jaeh8nkim/s1-4q36-qwen3-0.6b...\n",
      "‚úÖ Model already exists at s1-4q36-qwen3-0.6b-local\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Model Download\n",
    "\n",
    "def download_model_locally(repo_name, local_path):\n",
    "    \"\"\"Download model from HuggingFace and save locally\"\"\"\n",
    "    print(f\"üì• Downloading model from {repo_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if model already exists locally\n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"‚úÖ Model already exists at {local_path}\")\n",
    "            return local_path\n",
    "        \n",
    "        # Download model using snapshot_download (same as vLLM uses)\n",
    "        checkpoint_path = snapshot_download(repo_name)\n",
    "        \n",
    "        # Create local directory\n",
    "        os.makedirs(local_path, exist_ok=True)\n",
    "        \n",
    "        # Copy all files from checkpoint to local path\n",
    "        import shutil\n",
    "        shutil.copytree(checkpoint_path, local_path, dirs_exist_ok=True)\n",
    "        \n",
    "        print(f\"‚úÖ Model downloaded and saved to {local_path}\")\n",
    "        return local_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download the model locally\n",
    "print(\"üöÄ Downloading model locally...\")\n",
    "model_path = download_model_locally(MODEL_REMOTE_PATH, MODEL_LOCAL_PATH)\n",
    "\n",
    "if model_path is None:\n",
    "    raise RuntimeError(\"Failed to download model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc0df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up engine with local model: s1-4q36-qwen3-0.6b-local\n",
      "\n",
      "CUDA_VISIBLE_DEVICES = 2\n",
      "torch sees 1 GPU(s)\n",
      "WARNING 07-01 08:33:56 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f8c1199a910>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ba8edaa11d45e79c123ec5b85acbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 151936\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: vLLM Engine Setup\n",
    "\n",
    "# ---------------- utility: temporarily set visible GPUs --------------------\n",
    "@contextlib.contextmanager\n",
    "def visible_gpus(devices: str):\n",
    "    original = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "    print(f\"\\nCUDA_VISIBLE_DEVICES = {devices}\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = original\n",
    "\n",
    "# --------------------------- engine setup ----------------------------------\n",
    "async def setup_engine():\n",
    "    global engine, tokenizer, vocab_size\n",
    "    \n",
    "    # Use the locally downloaded model\n",
    "    print(f\"Setting up engine with local model: {MODEL_LOCAL_PATH}\")\n",
    "\n",
    "    with visible_gpus(SMALL_GPU_INDEX):\n",
    "        print(\"torch sees\", torch.cuda.device_count(), \"GPU(s)\")              \n",
    "        engine = AsyncLLMEngine.from_engine_args(\n",
    "            AsyncEngineArgs(model=MODEL_LOCAL_PATH,  # Use local path instead of checkpoint\n",
    "                            tensor_parallel_size=1,\n",
    "                            max_model_len=MAX_SEQ_LEN, \n",
    "                            gpu_memory_utilization=0.90,\n",
    "                            dtype=\"bfloat16\"),\n",
    "            start_engine_loop=True)\n",
    "        \n",
    "        tokenizer = await engine.get_tokenizer()\n",
    "\n",
    "    # Get model config using async method\n",
    "    model_config = await engine.get_model_config()\n",
    "    vocab_size = model_config.get_vocab_size()\n",
    "    \n",
    "    print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# --------------------------- sampling params -------------------------------\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=MAX_SEQ_LEN,\n",
    "    temperature=SMALL_TEMPERATURE,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# Initialize the engine\n",
    "await setup_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cea43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Answer extraction and grading functions\n",
    "\n",
    "def extract_boxed_answer(records, tokenizer):\n",
    "    \"\"\"Extract the last \\\\boxed{} answer between tokens 151668 and 151645\"\"\"\n",
    "    token_ids = [record['token_id'] for record in records]\n",
    "    \n",
    "    # Find positions of the tokens\n",
    "    pos_151668 = [i for i, tid in enumerate(token_ids) if tid == 151668]\n",
    "    pos_151645 = [i for i, tid in enumerate(token_ids) if tid == 151645]\n",
    "    \n",
    "    if len(pos_151668) != 1 or len(pos_151645) == 0:\n",
    "        return None\n",
    "\n",
    "    start_pos = pos_151668[0]\n",
    "    end_pos = pos_151645[0]  # Take the first occurrence of 151645\n",
    "    \n",
    "    if start_pos >= end_pos:\n",
    "        return None\n",
    "\n",
    "    # Extract token IDs between the markers (including the end marker)\n",
    "    between_token_ids = token_ids[start_pos:end_pos+1]\n",
    "    \n",
    "    # Decode the entire sequence at once to avoid U+FFFD issues\n",
    "    between_text = tokenizer.decode(between_token_ids)\n",
    "    \n",
    "    # Find all \\\\boxed{} patterns with proper brace matching\n",
    "    matches = []\n",
    "    i = 0\n",
    "    while i < len(between_text):\n",
    "        boxed_start = between_text.find('\\\\boxed{', i)\n",
    "        if boxed_start == -1:\n",
    "            break\n",
    "        \n",
    "        j = boxed_start + 7  # Start after '\\\\boxed{'\n",
    "        brace_count = 1\n",
    "        while j < len(between_text) and brace_count > 0:\n",
    "            if between_text[j] == '{':\n",
    "                brace_count += 1\n",
    "            elif between_text[j] == '}':\n",
    "                brace_count -= 1\n",
    "            j += 1\n",
    "        \n",
    "        if brace_count == 0:\n",
    "            matches.append(between_text[boxed_start + 7:j-1])\n",
    "        \n",
    "        i = boxed_start + 1\n",
    "    \n",
    "    return matches[-1] if matches else None\n",
    "\n",
    "def llm_grader(expected_answer, boxed_answer, openai_client, model_name=\"gpt-4o-mini\"):\n",
    "\n",
    "    def grader_prompt(expected_answer, boxed_answer):\n",
    "        \"\"\"Creates the system and user prompts for grading.\"\"\"\n",
    "        system_prompt = (\n",
    "            f\"You are an expert grader tasked with evaluating the correctness of an answer.\\n\"\n",
    "            f\"You will be provided with two pieces of text: the expected answer and the generated answer.\\n\"\n",
    "            f\"Your task is to determine if the generated answer is semantically equivalent to the expected answer.\\n\"\n",
    "            f\"Ignore minor formatting differences, extra whitespace, or trivial variations. For numerical answers, consider equivalent representations as correct (e.g., '1/2' and '0.5').\\n\"\n",
    "            f\"Respond with exactly one word: either 'true' (if correct) or 'false' (if incorrect). Do not include quotation marks, explanations, or any other text.\\n\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"Expected answer:\\n\"\n",
    "            f\"{expected_answer}\\n\"\n",
    "            f\"Generated answer:\\n\"\n",
    "            f\"{boxed_answer}\\n\"\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        return messages\n",
    "    \n",
    "    def grader(grading_messages, openai_client, model_name):\n",
    "        api_response = openai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=grading_messages\n",
    "        ).choices[0].message.content\n",
    "        \n",
    "        grade = api_response.strip().lower()\n",
    "        return grade\n",
    "    \n",
    "    grading_messages = grader_prompt(expected_answer, boxed_answer)\n",
    "    grade = grader(grading_messages, openai_client, model_name)\n",
    "    \n",
    "    # Ensure the grade is exactly 'true' or 'false'\n",
    "    if grade in ['true', 'false']:\n",
    "        return grade\n",
    "    else:\n",
    "        # Fallback in case the API returns something unexpected\n",
    "        return 'false'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3700de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Evaluation functions\n",
    "\n",
    "# Configuration: datasets to evaluate and their order\n",
    "EVAL_DATASETS = \"math_500\"\n",
    "# EVAL_DATASETS = \"aime_2024,aime_2025,gpqa_diamond,math_500\"\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "async def graded_is_correct(gold, pred, tokenizer):\n",
    "    # Convert generated text into token-records so extract_boxed_answer works\n",
    "    ids = tokenizer.encode(pred)\n",
    "    records = [{\"token_id\": t} for t in ids]\n",
    "\n",
    "    boxed = extract_boxed_answer(records, tokenizer)\n",
    "    extracted = boxed if boxed else pred\n",
    "\n",
    "    return llm_grader(gold, extracted, client) == \"true\"\n",
    "\n",
    "def print_dataset_info(dataset, task_name):\n",
    "    \"\"\"Print dataset count and first 5 examples\"\"\"\n",
    "    print(f\"\\n--- {task_name.upper()} DATASET INFO ---\")\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    print(f\"Dataset type: {type(dataset)}\")\n",
    "    \n",
    "    # Check the first item to understand the structure\n",
    "    if len(dataset) > 0:\n",
    "        first_item = dataset[0]\n",
    "        print(f\"First item type: {type(first_item)}\")\n",
    "        print(f\"First item keys: {list(first_item.keys()) if hasattr(first_item, 'keys') else 'No keys'}\")\n",
    "    \n",
    "    print(f\"First 5 examples:\")\n",
    "    \n",
    "    for i in range(min(5, len(dataset))):\n",
    "        item = dataset[i]  # Access by index instead of iteration\n",
    "        print(f\"\\n{i+1}. \", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            if \"math\" in task_name:\n",
    "                question = item.get(\"problem\", item.get(\"question\", \"\"))\n",
    "                answer = item.get(\"solution\", \"\")\n",
    "                print(f\"Question: {question[:200]}...\")\n",
    "                print(f\"   Answer: {answer[:200]}...\")\n",
    "            elif \"gpqa\" in task_name:\n",
    "                # Handle both original GPQA format and alternative formats\n",
    "                question = item.get(\"Question\", item.get(\"question\", item.get(\"Problem\", \"\")))\n",
    "                answer = item.get(\"Correct Answer\", item.get(\"correct_answer\", item.get(\"Answer\", \"\")))\n",
    "                print(f\"Question: {question[:200]}...\")\n",
    "                print(f\"   Answer: {answer}\")\n",
    "            else:  # AIME\n",
    "                question = item.get(\"problem\", item.get(\"Problem\", item.get(\"question\", \"\")))\n",
    "                answer = str(item.get(\"answer\", item.get(\"Answer\", \"\")))\n",
    "                print(f\"Question: {question[:200]}...\")\n",
    "                print(f\"   Answer: {answer}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying item: {e}\")\n",
    "            print(f\"Item keys: {list(item.keys()) if hasattr(item, 'keys') else 'Not a dict'}\")\n",
    "            print(f\"Item type: {type(item)}\")\n",
    "            print(f\"Raw item: {str(item)[:200]}...\")\n",
    "            break\n",
    "\n",
    "async def evaluate_problem_multiple_times(item, task_name, num_runs):\n",
    "    \"\"\"\n",
    "    Evaluate a single problem multiple times and return accuracy for that problem.\n",
    "    \"\"\"\n",
    "    global engine, tokenizer\n",
    "    correct = 0\n",
    "    \n",
    "    # Extract question and gold answer based on task type\n",
    "    try:\n",
    "        if \"math\" in task_name:\n",
    "            question = item.get(\"problem\", item.get(\"question\", \"\"))\n",
    "            gold = item.get(\"solution\", \"\")\n",
    "        elif \"gpqa\" in task_name:\n",
    "            # Handle both original GPQA format and alternative formats\n",
    "            question = item.get(\"Question\", item.get(\"question\", item.get(\"Problem\", \"\")))\n",
    "            \n",
    "            # Try different field names for correct answer\n",
    "            gold = item.get(\"Correct Answer\", item.get(\"correct_answer\", item.get(\"Answer\", \"\")))\n",
    "            \n",
    "            # Handle choices if they exist\n",
    "            choices = []\n",
    "            if \"Incorrect Answer 1\" in item:\n",
    "                # Original format\n",
    "                choices = [\n",
    "                    item[\"Incorrect Answer 1\"],\n",
    "                    item[\"Incorrect Answer 2\"],\n",
    "                    item[\"Incorrect Answer 3\"],\n",
    "                    item[\"Correct Answer\"],\n",
    "                ]\n",
    "                random.shuffle(choices)\n",
    "                gold = chr(65 + choices.index(item[\"Correct Answer\"]))\n",
    "                question += \"\\n\\nChoices:\\n\" + \"\\n\".join(\n",
    "                    f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)\n",
    "                )\n",
    "            elif any(f\"choice_{i}\" in item for i in ['A', 'B', 'C', 'D']):\n",
    "                # Alternative choice format\n",
    "                choices = [item.get(f\"choice_{i}\", \"\") for i in ['A', 'B', 'C', 'D']]\n",
    "                question += \"\\n\\nChoices:\\n\" + \"\\n\".join(\n",
    "                    f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices) if c\n",
    "                )\n",
    "                # Find which choice is correct\n",
    "                for i, choice in enumerate(['A', 'B', 'C', 'D']):\n",
    "                    if item.get(f\"choice_{choice}\", \"\") == gold:\n",
    "                        gold = choice\n",
    "                        break\n",
    "            \n",
    "        else:  # AIME\n",
    "            question = item.get(\"problem\", item.get(\"Problem\", item.get(\"question\", \"\")))\n",
    "            gold = str(item.get(\"answer\", item.get(\"Answer\", \"\")))\n",
    "\n",
    "        system_prompt = (\n",
    "            f\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\n\"\n",
    "            f\"You must respond to every query in the following manner:\\n\"\n",
    "            f\"First, provide a step-by-step logical exploration of the problem.\\n\"\n",
    "            f\"Then, provide a clear and direct response based on your reasoning, with the final answer enclosed in \\\\boxed{{}}.\"\n",
    "        )\n",
    "\n",
    "        input = (\n",
    "            f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{question}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n<think>\"\n",
    "        )\n",
    "        \n",
    "        # Run the problem multiple times\n",
    "        for run in range(num_runs):\n",
    "            random.seed(BASE_SEED + run)\n",
    "            torch.manual_seed(BASE_SEED + run)\n",
    "            \n",
    "            # Generate with vLLM (using working pattern)\n",
    "            request_id = str(uuid.uuid4())\n",
    "            generator = engine.generate(input, sampling_params, request_id)\n",
    "            \n",
    "            # Get the result using working pattern\n",
    "            final_output = None\n",
    "            async for request_output in generator:\n",
    "                final_output = request_output\n",
    "            \n",
    "            if final_output and final_output.outputs:\n",
    "                predicted = final_output.outputs[0].text.strip()\n",
    "                if await graded_is_correct(gold, predicted, tokenizer):\n",
    "                    correct += 1\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing problem: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    return correct / num_runs\n",
    "\n",
    "def load_combined_aime_2025():\n",
    "    \"\"\"Load and combine both AIME2025-I and AIME2025-II datasets\"\"\"\n",
    "    try:\n",
    "        # Load both AIME2025 datasets silently\n",
    "        aime_i = load_dataset(\"opencompass/AIME2025\", \"AIME2025-I\", split=\"test\", trust_remote_code=True)\n",
    "        aime_ii = load_dataset(\"opencompass/AIME2025\", \"AIME2025-II\", split=\"test\", trust_remote_code=True)\n",
    "        \n",
    "        # Combine the datasets\n",
    "        from datasets import concatenate_datasets\n",
    "        combined_aime = concatenate_datasets([aime_i, aime_ii])\n",
    "        \n",
    "        return combined_aime\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading AIME2025 datasets: {e}\")\n",
    "        return None\n",
    "\n",
    "async def evaluate_model_average(num_runs=NUM_RUNS):\n",
    "    \"\"\"\n",
    "    Evaluate a model on multiple datasets with multiple runs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse evaluation datasets from config string\n",
    "    evaluation_order = [dataset.strip() for dataset in EVAL_DATASETS.split(\",\")]\n",
    "    print(f\"üìã Will evaluate datasets: {', '.join(evaluation_order)}\")\n",
    "\n",
    "    # First, load all datasets and print their info\n",
    "    datasets = {}\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" LOADING ALL DATASETS \")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for dataset_name in evaluation_order:\n",
    "        if dataset_name == \"aime_2024\":\n",
    "            try:\n",
    "                print(f\"Loading aime_2024 dataset...\")\n",
    "                ds = load_dataset(\"HuggingFaceH4/aime_2024\", split=\"train\", trust_remote_code=True)\n",
    "                datasets[\"aime_2024\"] = ds\n",
    "                print(f\"‚úÖ aime_2024 loaded: {len(ds)} problems\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading aime_2024: {e}\")\n",
    "        \n",
    "        elif dataset_name == \"aime_2025\":\n",
    "            try:\n",
    "                print(f\"Loading aime_2025 dataset...\")\n",
    "                aime_2025_combined = load_combined_aime_2025()\n",
    "                if aime_2025_combined is not None:\n",
    "                    datasets[\"aime_2025\"] = aime_2025_combined\n",
    "                    print(f\"‚úÖ aime_2025 loaded: {len(aime_2025_combined)} problems\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading aime_2025: {e}\")\n",
    "        \n",
    "        elif dataset_name == \"gpqa_diamond\":\n",
    "            try:\n",
    "                print(f\"Loading gpqa_diamond dataset...\")\n",
    "                ds = load_dataset(\"spawn99/GPQA-diamond-ClaudeR1\", split=\"train\", trust_remote_code=True)\n",
    "                datasets[\"gpqa_diamond\"] = ds\n",
    "                print(f\"‚úÖ gpqa_diamond loaded: {len(ds)} problems\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading gpqa_diamond: {e}\")\n",
    "        \n",
    "        elif dataset_name == \"math_500\":\n",
    "            try:\n",
    "                print(f\"Loading math_500 dataset...\")\n",
    "                ds = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\", trust_remote_code=True)\n",
    "                datasets[\"math_500\"] = ds\n",
    "                print(f\"‚úÖ math_500 loaded: {len(ds)} problems\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading math_500: {e}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    # Print info for all loaded datasets in the desired order\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" DATASET INFORMATION \")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for task in evaluation_order:\n",
    "        if task in datasets:\n",
    "            print_dataset_info(datasets[task], task)\n",
    "    \n",
    "    # Now run evaluations in the specified order\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" STARTING EVALUATIONS \")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for task in evaluation_order:\n",
    "        if task not in datasets:\n",
    "            print(f\"‚ö†Ô∏è  Skipping {task} - dataset not loaded\")\n",
    "            continue\n",
    "            \n",
    "        ds = datasets[task]\n",
    "        print(f\"\\nüîÑ Evaluating {task.upper()}...\")\n",
    "        \n",
    "        problem_accuracies = []\n",
    "        \n",
    "        # Iterate through each problem\n",
    "        for i in tqdm(range(len(ds)), desc=f\"{task} problems\"):\n",
    "            item = ds[i]\n",
    "            problem_accuracy = await evaluate_problem_multiple_times(item, task, num_runs)\n",
    "            problem_accuracies.append(problem_accuracy)\n",
    "            \n",
    "            # Determine status\n",
    "            correct_runs = int(problem_accuracy * num_runs)  # Convert back to count\n",
    "            if correct_runs > 0:\n",
    "                status_emoji = \"‚úÖ\"\n",
    "            else:\n",
    "                status_emoji = \"‚ùå\"\n",
    "            \n",
    "            # Show individual problem results with additional info for AIME 2025\n",
    "            if task == \"aime_2025\":\n",
    "                # Determine if this is from AIME I or II based on position\n",
    "                if i < 15:  # First 15 are from AIME I\n",
    "                    contest_info = f\"(AIME I, #{i+1})\"\n",
    "                else:  # Last 15 are from AIME II\n",
    "                    contest_info = f\"(AIME II, #{i-14})\"\n",
    "                print(f\"{status_emoji} Problem {i+1:02d}/{len(ds)} {contest_info} ‚Äî {task}: {correct_runs}/{num_runs}\")\n",
    "            else:\n",
    "                print(f\"{status_emoji} Problem {i+1:02d}/{len(ds)} ‚Äî {task}: {correct_runs}/{num_runs}\")\n",
    "\n",
    "        average_accuracy = sum(problem_accuracies) / len(problem_accuracies)\n",
    "        all_results[task] = {\"average_accuracy\": average_accuracy, \"problem_accuracies\": problem_accuracies}\n",
    "        \n",
    "        # Final summary for this dataset\n",
    "        total_runs = len(ds) * num_runs\n",
    "        total_correct_runs = sum(int(acc * num_runs) for acc in problem_accuracies)\n",
    "        print(f\"‚úÖ {task} complete - {total_correct_runs}/{total_runs} ({average_accuracy:.2%} accuracy)\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261aab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Will evaluate datasets: math_500\n",
      "\n",
      "============================================================\n",
      " LOADING ALL DATASETS \n",
      "============================================================\n",
      "Loading math_500 dataset...\n",
      "‚úÖ math_500 loaded: 500 problems\n",
      "\n",
      "============================================================\n",
      " DATASET INFORMATION \n",
      "============================================================\n",
      "\n",
      "--- MATH_500 DATASET INFO ---\n",
      "Total samples: 500\n",
      "Dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "First item type: <class 'dict'>\n",
      "First item keys: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id']\n",
      "First 5 examples:\n",
      "\n",
      "1. Question: Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$...\n",
      "   Answer: We have that $r = \\sqrt{0^2 + 3^2} = 3.$  Also, if we draw the line connecting the origin and $(0,3),$ this line makes an angle of $\\frac{\\pi}{2}$ with the positive $x$-axis.\n",
      "\n",
      "[asy]\n",
      "unitsize(0.8 cm);\n",
      "...\n",
      "\n",
      "2. Question: Define\n",
      "\\[p = \\sum_{k = 1}^\\infty \\frac{1}{k^2} \\quad \\text{and} \\quad q = \\sum_{k = 1}^\\infty \\frac{1}{k^3}.\\]Find a way to write\n",
      "\\[\\sum_{j = 1}^\\infty \\sum_{k = 1}^\\infty \\frac{1}{(j + k)^3}\\]in term...\n",
      "   Answer: We count the number of times $\\frac{1}{n^3}$ appears in the sum\n",
      "\\[\\sum_{j = 1}^\\infty \\sum_{k = 1}^\\infty \\frac{1}{(j + k)^3},\\]where $n$ is a fixed positive integer.  (In other words, we are conditio...\n",
      "\n",
      "3. Question: If $f(x) = \\frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a common fraction....\n",
      "   Answer: $f(-2)+f(-1)+f(0)=\\frac{3(-2)-2}{-2-2}+\\frac{3(-1)-2}{-1-2}+\\frac{3(0)-2}{0-2}=\\frac{-8}{-4}+\\frac{-5}{-3}+\\frac{-2}{-2}=2+\\frac{5}{3}+1=\\boxed{\\frac{14}{3}}$...\n",
      "\n",
      "4. Question: How many positive whole-number divisors does 196 have?...\n",
      "   Answer: First prime factorize $196=2^2\\cdot7^2$.  The prime factorization of any divisor of 196 cannot include any primes other than 2 and 7.  We are free to choose either 0, 1, or 2 as the exponent of 2 in t...\n",
      "\n",
      "5. Question: The results of a cross-country team's training run are graphed below. Which student has the greatest average speed? [asy]\n",
      "for ( int i = 1; i <= 7; ++i )\n",
      "{\n",
      "\n",
      "draw((i,0)--(i,6));\n",
      "}\n",
      "\n",
      "for ( int i = 1; i <=...\n",
      "   Answer: Evelyn covered more distance in less time than Briana, Debra and Angela, so her average speed is greater than any of their average speeds. Evelyn went almost as far as Carla in less than half the time...\n",
      "\n",
      "============================================================\n",
      " STARTING EVALUATIONS \n",
      "============================================================\n",
      "\n",
      "üîÑ Evaluating MATH_500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   0%|          | 1/500 [00:04<34:10,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 01/500 ‚Äî math_500: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   0%|          | 2/500 [00:12<54:56,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 02/500 ‚Äî math_500: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   1%|          | 3/500 [00:17<48:27,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 03/500 ‚Äî math_500: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   1%|          | 4/500 [00:22<45:52,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 04/500 ‚Äî math_500: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   1%|          | 5/500 [01:30<3:51:44, 28.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 05/500 ‚Äî math_500: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   1%|          | 6/500 [01:34<2:43:59, 19.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 06/500 ‚Äî math_500: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   1%|‚ñè         | 7/500 [01:43<2:14:24, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 07/500 ‚Äî math_500: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   2%|‚ñè         | 8/500 [01:53<1:58:10, 14.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 08/500 ‚Äî math_500: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   2%|‚ñè         | 9/500 [01:58<1:31:57, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 09/500 ‚Äî math_500: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   2%|‚ñè         | 10/500 [02:42<2:56:19, 21.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Problem 10/500 ‚Äî math_500: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "math_500 problems:   2%|‚ñè         | 11/500 [03:02<2:51:24, 21.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Problem 11/500 ‚Äî math_500: 1/1\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main execution\n",
    "\n",
    "results = await evaluate_model_average(NUM_RUNS)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" FINAL AVERAGED ACCURACIES \")\n",
    "print(\"=\"*50)\n",
    "for task, result in results.items():\n",
    "    print(f\"{task.upper():<15}: {result['average_accuracy']:.2%}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2957ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
